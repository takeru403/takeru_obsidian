JLAB-THY-25-4413


**Toward an event-level analysis of hadron structure using differential programming**


[Kevin Braga,](https://orcid.org/0009-0008-8752-8292) [1] [Markus Diefenthaler,](https://orcid.org/0000-0002-4717-4484) [2] [Steven Goldenberg,](https://orcid.org/0000-0002-5264-6298) [2] Daniel Lersch, [2,] _[ ∗]_ [Yaohang Li,](https://orcid.org/0000-0003-0178-1876) [3]

Jian-Wei Qiu, [2] Kishansingh Rajput, [2, 4] [Felix Ringer,](https://orcid.org/0000-0002-5939-3510) [5,] _[ †]_ [Nobuo Sato,](https://orcid.org/0000-0002-1535-6208) [2,] _[ ‡]_ and Malachi Schram 2

1 _Department of Physics, William & Mary, Williamsburg, Virginia 23185, USA_
2 _Thomas Jefferson National Accelerator Facility, Newport News, VA 23606, USA_
3 _Department of Computer Science, Old Dominion University, Norfolk, Virginia 23529, USA_
4 _Department of Computer Science, University of Houston, Houston, TX 77204, USA_
5 _Department of Physics and Astronomy, Stony Brook University, Stony Brook, NY 11794, USA_
(Dated: July 22, 2025)


Reconstructing the internal properties of hadrons in terms of fundamental quark and gluon degrees of freedom is a central goal in nuclear and particle physics. This effort lies at the core of major
experimental programs, such as the Jefferson Lab 12 GeV program and the upcoming Electron-Ion
Collider. A primary challenge is the inherent inverse problem: converting large-scale observational
data from collision events into the fundamental quantum correlation functions (QCFs) that characterize the microscopic structure of hadronic systems within the theory of QCD. Recent advances in
scientific computing and machine learning have opened new avenues for addressing this challenge
using deep learning techniques. A particularly promising direction is the integration of theoretical
calculations and experimental simulations into a unified framework capable of reconstructing QCFs
directly from event-level information. In this work, we introduce a differential sampling method
called the local orthogonal inverse transform sampling (LOITS) algorithm. We validate its performance through a closure test, demonstrating the accurate reconstruction of a test distribution from
sampled events using Generative Adversarial Networks. The LOITS algorithm provides a central
building block for addressing inverse problems involving QCFs and enables end-to-end inference
pipelines within the framework of differential programming.



**CONTENTS**


I. Introduction 1


II. Physics of Quantum Correlation Functions 3


III. Differentiable sampling algorithm 4


IV. Inference using differential sampling and GANs 7


V. Conclusions 10


Acknowledgments 12


A. Double half-moon distribution 12


References 12


**I.** **INTRODUCTION**


The next generation of experiments in nuclear particle physics, including the ongoing Jefferson Lab 12 GeV
program and the future Electron-Ion Collider (EIC), is
expected to deliver an unprecedented volume of data for
studying the internal properties of hadrons and nuclei.
One of the primary scientific endeavors is to reconstruct


_∗_ [dlersch@jlab.org](mailto:dlersch@jlab.org)

_†_ [felix.ringer@stonybrook.edu](mailto:felix.ringer@stonybrook.edu)

_‡_ [nsato@jlab.org](mailto:nsato@jlab.org)



the internal three-dimensional structure of nucleons and
nuclei in terms of their quark and gluon degrees of freedom. These structures are encoded in quantum correlation functions (QCFs), such as parton distribution
functions (PDFs), transverse momentum distributions
(TMDs), and generalized parton distribution functions
(GPDs), all of which are defined within the fundamental theory of strong interactions. While PDFs describe
the longitudinal momentum distribution of partons inside hadrons, TMDs and GPDs account for the transverse momentum and spatial distribution, respectively.
Reconstructing such quantities presents an inverse problem, as they cannot be directly measured in experiments.
Instead, they need to be inferred from observational data
that consist of large collections of collision events, each
composed of data structures for the final-state particles.


Traditionally, QCFs are inferred from differential cross
sections that are derived from event-averaged, binned
quantities, also referred to as summary statistics. These
cross sections are extracted from measured data using an
unfolding procedure based on simulations. The unfolding accounts for model uncertainties, detector effects, and
background contributions. A commonly used technique
for this purpose is Iterative Bayesian Unfolding [1]. As a
second step, QCFs are extracted from the unfolded data
using QCD factorization theorems [2], which separate the
cross section into perturbatively calculable components
and the nonperturbative QCFs of interest. Constraints
from lattice QCD can also be incorporated [3–9]. See
Refs. [9–24] for examples of this approach.


Looking ahead, to reduce the uncertainties on QCFs
and sharpen our understanding of nucleon and nuclear


structure, full end-to-end simulations may become feasible given recent advances in AI and machine learning [25].
These could make use of event-level data without loss of
information. This approach is expected to be particularly valuable for extracting the three-dimensional structure of nucleons and nuclei, which needs to be inferred
from multi-differential cross sections. In general, such
an event-level analysis involves the following steps: i)
Proposing a set of trial QCFs based on a well-motivated
modeling strategy; ii) Converting the QCFs into differential cross sections via QCD factorization theorems; iii)
Generating phase-space or event-level samples from the
resulting cross sections; iv) Simulating realistic experimental data, including detector effects and backgrounds,
using the generated events from iii); and v) Optimizing
the QCFs by minimizing a distance metric between the
simulated and observed event samples. To implement
this approach in practice, a fully differential simulation
and analysis pipeline is required. In the following, we
discuss various components of this strategy in more detail.

**Event-level data.** In recent years, several efforts
have been made to address inference problems in nuclear and particle physics using unbinned data. See,
in particular, the proposal to publish unbinned data by
experimental collaborations and the discussion of potential applications in Ref. [26]. In general, using unbinned data helps preserve the full information content
of the dataset, avoiding information loss due to binning.
For example, Ref. [27] demonstrated advantages of unbinned maximum likelihood methods applied to simulated Higgs data, showing improved performance compared to traditional binned approaches. While for lowdimensional observables an optimal, inference-aware binning can be constructed [28–31], this becomes significantly more challenging for multi-differential observables
and/or low-statistics regimes, which are particularly relevant in studies of 3D hadron structure.

**Event-level theory.** To carry out unbinned inference
tasks at the event level, both experimental data and theoretical predictions need to be available at the event level.
For inference tasks involving parton showers [32–37] or
specific fixed-order calculations [38–40], event-level theoretical results are readily available. However, much of the
current QCD phenomenology targeting the extraction of
QCFs relies on analytical perturbative QCD calculations
at fixed order matched to e.g. TMD resummation. In addition, theoretical calculations of exclusive processes sensitive to GPDs exist only as analytical expressions. For
processes such as Deeply Virtual Compton Scattering,
these results can be converted into event-level samples
using algorithms like the one developed in Ref. [41, 42].
See also Refs. [43–46]. Since theoretical simulations are
generally not differentiable, surrogate models need to be
introduced to incorporate them into end-to-end inference
pipelines [47–54]. In this work, we propose a complementary method to convert analytical cross section results
into differentiable event samples, thereby avoiding the



2


need for surrogate models. Specifically, we introduce a
Local Orthogonal Inverse Transform Sampling (LOITS)
algorithm that enables us to build a differentiable manifold connecting phase space samples of events with the
underlying QCF model parameters.
**Detector effects.** As noted above, comparing experimental data with theoretical predictions requires a
careful treatment of detector effects and backgrounds.
This can be achieved either by unfolding the unbinned
experimental measurements from detector level to particle level or by forward-folding the theoretical predictions
through a detector simulation such as GEANT4 [55]. In
recent years, event-level unfolding techniques have been
developed to go beyond traditional methods that rely
on binned data. For instance, density-based approaches
have been introduced in Refs. [47, 56–65], while classifierbased methods for reweighting events have been proposed
in Refs. [66–70]. Some of these techniques have already
been adopted by experimental collaborations, as seen in
Refs. [71–73]. For certain low-energy scattering cross sections, such as those measured at Jefferson Lab, unfolding
can be more challenging due to the lack of sufficiently
accurate theoretical models for generating phase space
samples, in contrast to the high-energy collider environment. When forward folding of theoretical predictions
is used within the inference pipeline, a differentiable detector simulation is required that can be achieved using
surrogate models. See, Ref. [74] and references therein.
Instead, when using unfolded event samples, correlations
between events need to be accounted for [75].
**Parametrization** **of** **QCFs.** The modeling of
QCFs introduces an additional computational challenge.
Strictly speaking, QCFs are continuous functions, and in
general, it is not possible to constrain them directly without preconditioning the problem, typically by truncating
their local analytic behavior. In practice, a variety of
parameterizations are routinely employed. These range
from simple functional forms involving _O_ (10 _−_ 100) parameters [7] to more flexible representations using neural networks [20, 76], which typically involve a significantly larger number of parameters. As the complexity of QCFs increases, for example, for multidimensional
structures such as GPDs, more flexible parameterizations
are needed to mitigate modeling bias and reliably estimate uncertainties. Multidimensional parton densities
are well-suited to be modeled as pixelated images, allowing for a direct quantification of the resolution at which
QCF images can be reconstructed from data. As an illustrative example, in this work, we consider Generative
Adversarial Networks (GANs), which can be trained on
event-level data to generate pixelated QCF images that
are consistent with observational constraints. While this
example application is based on the application of generative AI, we note that the LOITs sampling algorithm is
independent of this particular approach.

**Uncertainties.** Unlike fundamental physical parameters such as the _W_ -boson mass, electroweak couplings,
or Standard Model Effective Field Theory parameters,


model parameters used in QCF modeling are subject to
priors that regularize the inverse problem [77]. Uncertainty quantification for QCFs has a long history, beginning with the pioneering work of the CTEQ collaboration [78]. Subsequently, the NNPDF collaboration
introduced neural network–based parametrizations and
employed ensemble-based approaches to address the uncertainty quantification for PDFs. Since then, various
groups have adopted ensemble methods for a variety
of QCFs, including helicity-dependent PDFs, fragmentation functions, TMDs, and GPDs [19, 79–81]. Generally, these approaches fundamentally rely on observational data presented as summary statistics (e.g., differential cross sections, asymmetries), and to our knowledge, a fully realized framework for reconstructing QCFs
with uncertainty quantification carried out directly at
the event level has not yet been achieved. Nevertheless,
ensemble-based methodologies can, in principle, be extended to event-level QCF inference through bootstrap
resampling of events combined with the propagation of
uncertainties arising from unfolding or folding procedures. While a comprehensive treatment of the uncertainty quantification for QCF reconstruction at the event
level is beyond the scope of this work, our focus in this
document is to provide a differentiable solution for realizing end-to-end, simulation-based inference that can
leverage generative AI techniques. Since we are particularly interested in multidimensional QCFs represented
as images, we will also discuss a specific aspect of UQ
related to the resolution of the images that our trained
models can infer.

The remainder of this document is organized as follows:
In Section II, we briefly review the underlying physics of
QCFs, their connection to observational data, and relevant aspects of detector simulations. In Section III, we
introduce the differentiable sampling algorithm LOITS.
Section IV presents a test case demonstrating the performance of our approach using GANs. Finally, we conclude
in Section V.


**II.** **PHYSICS OF QUANTUM CORRELATION**
**FUNCTIONS**


QCFs are nonperturbative objects defined within QCD
that characterize the internal structure of hadrons and
nuclei in terms of their quark and gluon degrees of freedom. They are typically defined as matrix elements of
QCD field operators evaluated between hadronic or nuclear states. A classical example of QCFs are collinear
PDFs, which have a long history in high-energy and nuclear particle physics. These objects represent the number densities of partons carrying a fraction of the large
light-cone momentum of their parent hadron. Using
QCD factorization theorems, one can compute differential cross sections for a given process as a convolution of
QCFs with process-dependent parton-level cross sections
that are calculable in perturbative QCD. This framework



3


enables the description of a wide range of processes at
different collider experiments.
While PDFs are one-dimensional QCFs, there exist extensions that encode the three-dimensional structure of

hadrons. For instance, transverse momentum-dependent
(TMD) PDFs are three-dimensional QCFs that include
both the longitudinal momentum fraction and the intrinsic nonperturbative transverse momentum distribution of partons inside hadrons, offering a momentumspace tomography. Complementary to this are Generalized Parton Distributions (GPDs), which are threedimensional QCFs encoding the spatial distributions of
partons, thereby enabling position-space tomography of
hadrons or nuclei.

To access the spatial distribution of partons inside a
proton, we can study for example the following exclusive
scattering process _ep →_ _e_ _[′]_ _p_ _[′]_ _γ_, which allows for the reconstruction of so-called Compton Form Factors (CFFs) in
deeply virtual Compton scattering (DVCS) [82]. These
CFFs play a role analogous to that of structure functions
in Deep Inelastic Scattering (DIS) and can be factorized
in terms of GPDs and short-distance partonic coefficient
functions:



(1)


Here, _F_ _N_ _[a]_ [denotes a CFF, expressed in terms of the mo-]
mentum transfer to the target nucleon _t_, the longitudinal momentum transfer _ξ_, and the virtuality of the exchanged photon _Q_ [2] . The index _a_ distinguishes different types of CFFs associated with the polarization of
the initial-state nucleon, as well as the possible spinconserving and spin-flip transitions of the diffracted final state [82, 83]. For spin- 2 [1] [particles such as nucleons,]

there are four independent CFFs, each corresponding to
one of the four associated GPDs, denoted _F_ _[a]_
_i/N_ [, where]
the additional index _i_ labels the parton flavor. Similar to
PDFs, GPDs depend additionally on the parton’s lightcone momentum fraction _x_ and the factorization scale _µ_ [2]

governed by GPD evolution equations [82, 84–86]. The
CFFs are then obtained by integrating the GPDs against
perturbatively calculable coefficient functions _C_ _i_ _[a]_ [.]
In contrast to structure functions in DIS, the formulation of CFFs in DVCS requires integration over the full
range of the parton momentum fraction, _−_ 1 _< x <_ 1.
This presents significant challenges for reconstructing the
_x_ -dependence of GPDs [87, 88], as the reconstruction
is effectively limited to the kinematic ridge at _x_ = _ξ_ .
However, complementary observables that provide constraints away from the ridge do exist, such as exclusive
photoproduction of back-to-back _π, γ_ system [89].
The multidimensional nature of GPDs demands careful modeling strategies in inference tasks. For example,
the Fourier transform of GPDs with respect to _t_ provides
access to the coordinate-space density of partons relative
to the hadron’s center, offering a spatial image of par


1
_F_ _N_ _[a]_ [(] _[ξ, t, Q]_ [2] [) =]
� _−_



_C_ _i_ _[a]_ [(] _[x, ξ, Q]_ [2] [;] _[ µ]_ [2] [)] _[F]_ _[ a]_ _i/N_ [(] _[x, ξ, t]_ [;] _[ µ]_ [2] [)] _[ .]_

_i_



_dx_ �
_−_ 1


FIG. 1. Schematic illustration of the simulation-based inference pipeline and the backpropagation chain for optimizing
QCF model parameters. Phase space samples _x_ _θ_ are drawn
from a theoretical density _p_ ( _x|θ_ ), conditioned on the QCF
model parameters _θ_ . These samples are passed through a detector simulation _D_ to produce _x_ [sim] _θ_, which are compared to
experimental data _x_ [exp] using a loss function that quantifies
their discrepancy. The LOITS algorithm introduced in this
work enables a fully differentiable sampling procedure, creating a continuous and differentiable path between _θ_ and the
loss. The red box highlights the differentiable sampling stage,
enabling backpropagation through the entire pipeline.


ton distributions. In this context, quantifying the resolution at which experimental data can constrain these
coordinate-space GPD densities becomes particularly important. Similar considerations apply not only to GPDs,
but also to TMDs and even collinear PDFs. From this
perspective, machine learning models for QCFs offer a
promising strategy for addressing inverse problems, particularly given their flexibility in optimizing large parameter spaces. While the numerical techniques developed in
the following sections are broadly applicable to inverse
problems in nuclear and particle physics, our primary
area of focus is nuclear imaging involving multidimensional QCFs, such as GPDs or TMDs.


**III.** **DIFFERENTIABLE SAMPLING**

**ALGORITHM**


As discussed above, this work aims to provide a solution for QCF model optimization using event-level data.
Our goal is to construct a differentiable map between
phase space samples and QCF model parameters. The
proposed strategy is rather general and can be applied
to simple parametric forms of QCFs, neural network
parametrizations, as well as generative models where
QCFs are represented as images.
Let us denote by _p_ ( _x|θ_ ) a theoretical description of
the phase space density, conditioned on a set of tunable
QCF model parameters _θ_, where _x_ denotes the phase



4


space features. This quantity can be constructed using
differential cross sections derived from QCD factorization theorems, as discussed in section II. For example, in
exclusive processes involving the DVCS subprocess, one
has _p_ ( _ξ, t, Q_ [2] _|θ_ ) _∝_ _dσ/dξ dt dQ_ [2] where _θ_ denotes the input model parameters of the GPDs. With this in mind,
the simulation pipeline can be schematically written as


_θ →_ _p_ ( _x|θ_ ) _→_ _x_ _θ_ _→_ _D_ ( _x_ _θ_ ) _→_ _x_ [sim] _θ_ _._ (2)


Here, the samples _x_ _θ_ are drawn from the theoretical
phase space density, _x_ _θ_ _∼_ _p_ ( _x|θ_ ). The subscript of _x_ _θ_
indicates that gradients _∇_ _θ_ _x_ _θ_ need to be tractable, following the differential programming paradigm, to optimize model parameters. The generated samples can then
be passed through a detector simulation _D_, which transforms _x_ _θ_ into realistic samples _x_ [sim] _θ_ that can be compared
with experimental data at the event level. Typically, detector effects require simulators such as [90]. Since detector simulation is generally non-differentiable, a machine
learning surrogate is often needed to enable differentiability. However, in this work, we do not address this aspect
and focus only on the differentiability of the phase space
samples prior to detector effects. A schematic representation of the simulation pipeline is shown in Fig. 1. The diagram illustrates the computational path connecting the
QCF parameters _θ_ to a generic loss function that quantifies the discrepancy between simulated event samples
and experimental data, along with the chain of differentiability required for parameter optimization. The goal
of the LOITS algorithm introduced here is to enable the
computation of exact gradients of the loss function with
respect to _θ_ by constructing a differentiable map between
_x_ _θ_ and _θ_, as highlighted by the red dashed box in Fig. 1.
Without the differential sampling algorithm, the chain
rule originating from the loss function is broken, and the
gradients needed to optimize the model parameters cannot be computed. While gradient estimation via finite
differences is a possible alternative, our proposed solution leverages the automatic differentiation capabilities
of modern machine learning libraries to compute gradients both accurately and efficiently.
We now proceed to discuss our proposed LOITS algorithm, which enables us to build a differentiable map
between the phase space samples _x_ _θ_ and the parameters
_θ_ . The focus of LOITS is the sampling procedure, indicated by the red dashed box in Fig. 1. The key idea is
that the algorithmic procedure for drawing samples from
the density must retain information about the QCF parameters _θ_ . For example, when using standard Markov
Chain Monte Carlo (MCMC) methods to generate samples from a target density with Gaussian proposals, the
gradients vanish _∇_ _θ_ _x_ _θ_ = 0. This is because the samples
are obtained from a Gaussian proposal function that has
no analytical dependence on the target density’s model
parameters _θ_ . Instead, a differentiable sampling algorithm can be obtained using the well-known method of
_inverse transform sampling_ (ITS). To illustrate the procedure, let us first consider a simple example where the


phase space _x_ is one-dimensional and restricted to the
interval 0 _< x <_ 1 [1] . In this case, we can calculate the
cumulative density function (CDF) as


_x_
CDF( _x, θ_ ) = _dz p_ ( _z|θ_ ) _._ (3)
� 0


By construction, CDF( _x, θ_ ) is analytic in _θ_ provided that
_p_ ( _x|θ_ ) is itself an analytic function of _θ_, which is typically
the case for applications to QCF. Therefore, the gradient
_∇_ _θ_ CDF is tractable. Applying the chain rule, we obtain



_∂_ CDF( _x, θ_ )



_x_

_dz_ _[∂]_ [CDF][(] _[x][,][ θ]_ [)]
0 _∂p_ ( _z|θ_ )



_,_ (4)
_∂θ_ _i_



_x_

( _x, θ_ ) =

_∂θ_ _i_ � 0




[CDF][(] _[x][,][ θ]_ [)] _∂p_ ( _z|θ_ )

_∂p_ ( _z|θ_ ) _∂θ_ _i_



5


FIG. 2. Illustration of the 2D phase space segmentation for
implementing the LOITS algorithm. The blue lines indicate
the orthogonal sampling directions used by LOITS to define
local one-dimensional densities. For simplicity, the explicit
dependence of the density on the model parameters _θ_ is omitted.


values and _Y_ ( _θ_ ) the corresponding array of CDF values,
i.e., _Y_ ( _θ_ ) = _{_ CDF( _x, θ_ ) for _x_ in _X}_ . Then, samples can
be approximately generated as


_x_ _θ_ = CDF _[−]_ [1] ( _u, θ_ ) _≈_ LI( _u_ ; _X, Y_ ( _θ_ )) _._ (7)


Provided that the interpolation function LI is analytic in
_Y_, and hence in _θ_, the samples _x_ _θ_ generated via Eq. (7)
are differentiable with respect to _θ_ . In our numerical
studies presented below, we use a simple local linear interpolation function, which satisfies the required analytic
condition.
At this stage, we have demonstrated how ITS can be
used to generate differentiable samples for a simplified
one-dimensional setting. Unfortunately, ITS cannot be
directly extended beyond one dimension because CDFs
are not uniquely defined in higher dimensions. However,
given that the density _p_ ( _x|θ_ ) does not vanish within the
physical phase space boundaries, as is the case for the applications considered in this work, we will show that it is
still possible to generalize this method to higher dimensions by introducing a local ITS algorithm. To achieve
this, we begin by extending the 1D ITS method to a local
sampling procedure, which can then be generalized to a
higher-dimensional phase space as well. In the 1D case,
we partition the phase space into _K_ segments and apply
ITS within each segment independently. Let us illustrate
the procedure for the 1D case introduced earlier. The
steps are as follows:


1. Divide the domain 0 _< x <_ 1 into _K_ contiguous
segments:


_{x_ _k_ _, x_ _k_ +1 ; _k_ = 1 _, . . ., K} ._ (8)



where the subscript _i_ refers to the _i_ [th] model parameter. To perform ITS, we need to access the inverse CDF.
Samples from _p_ ( _x|θ_ ) can then be generated as


_x_ _θ_ ( _u_ ) = CDF _[−]_ [1] ( _u, θ_ ) _,_ (5)


where _u ∼U_ [0 _,_ 1] is drawn from a uniform distribution
over 0 _< u <_ 1. The resulting samples _x_ _θ_ are then
distributed according to the target density, i.e., _x_ _θ_ _∼_
_p_ ( _x|θ_ ). Importantly, for any value of _u_, the sample _x_ _θ_ is
an analytic function of _θ_ . Thus, using the chain rule, we
can compute its gradient as



_∂x_ _θ_

= _[∂]_ [CDF] _[−]_ [1] [(] _[u][,][ θ]_ [)]
_∂θ_ _i_ _∂θ_ _i_



_∂x_ _θ_



_∂θ_ _i_



= _[∂]_ [CDF] _[−]_ [1] [(] _[u][,][ θ]_ [)]



_,_

_._ (6)
_∂θ_ _i_




[(] _[u]_ _[,][ θ]_ [)]

[CDF] _[−]_ _∂_ CDF( _x_ _θ_ _, θ_ )

_∂_ CDF( _x_ _θ_ _, θ_ ) _∂θ_ _i_



It is important to note that, by construction, the endpoints of the CDF are fixed for all values of _θ_, i.e.,
CDF(0 _, θ_ ) = CDF(1 _, θ_ ) = 1. As a result, the gradients in
Eq. (6) vanish at the endpoints _x_ _θ_ = 0 and _x_ _θ_ = 1. This
implies that samples near the boundaries lose sensitivity
to the model parameters _θ_ . We will discuss this aspect
in more in section IV.
In general, it is not possible to obtain an analytical expression for CDF _[−]_ [1] ( _u, θ_ ) for an arbitrary density
_p_ ( _x|θ_ ). However, provided that the density does not vanish within the phase space boundaries, which is the case
for the applications considered in this work, the CDF is
a strictly monotonic function. As a result, its inverse is
uniquely defined. Using this feature, we can evaluate the
CDF on a discrete set of values 0 _< x <_ 1 and construct
an approximate inverse CDF _[−]_ [1] using a local interpolation (LI) function [2] . Specifically, let _X_ be the grid of _x_


1 The range of support can be extended by modifying the integration limits of the CDF.
2 We define a local interpolation as


_y_ = LI( _x_ ; _X, Y_ )


where _X, Y_ are pairwise connected arrays to be interpolated, and
_y_ is the interpolated value at the coordinate _x_ .


6



_**y**_


0 _._ 8


0 _._ 6


0 _._ 4


0 _._ 2



**LOITS** **(4** _**×**_ **4)**


0 _._ 2 0 _._ 4 0 _._ 6 0 _._ 8 _**x**_



_**y**_


0 _._ 8


0 _._ 6


0 _._ 4


0 _._ 2



**LOITS** **(4** _**×**_ **4)** **+ MH**


0 _._ 2 0 _._ 4 0 _._ 6 0 _._ 8 _**x**_



_**y**_


0 _._ 8


0 _._ 6


0 _._ 4


0 _._ 2



**LOITS** **(50** _**×**_ **50)**


0 _._ 2 0 _._ 4 0 _._ 6 0 _._ 8 _**x**_



FIG. 3. Sampling of the half moon distribution. **Left:** Reconstructed distribution using the LOITS algorithm with a 4 _×_ 4
phase space segmentation. **Middle:** Same as the left panel, but using the MH improved LOITS algorithm. **Right:** Same as
the left panel, but with a 50 _×_ 50 phase space segmentation.



For now, let us assume an equidistant segmentation.


2. Compute segment probabilities and local densities:


_x_ _k_ +1
_m_ _k_ = _dx p_ ( _x|θ_ ) _,_ (9)
� _x_ _k_


1
_p_ _k_ ( _x|θ_ ) = _p_ ( _x|θ_ ) Θ( _x_ _k_ _< x < x_ _k_ +1 ) _._ (10)
_m_ _k_


3. Compute the local CDFs for a subgrid _x_ _k_ _< x_ _l_ _<_
_x_ _k_ +1 with _l_ = 1 _, . . ., L_ for all the _k_ segments, i.e.


_x_ _l_
CDF _k_ ( _x_ _i_ _, θ_ ) = _dx p_ _k_ ( _x|θ_ ) _._ (11)
� _x_ _k_


4. To generate a total of _N_ samples, estimate the number of samples that need to be generated for each
segment via


_n_ _k_ = int( _m_ _k_ _· N_ ) _,_ (12)


where int( _·_ ) denotes the integer rounding operation.


5. For each segment, draw _n_ _k_ uniform samples _u ∈_

[0 _,_ 1] and map them to phase space values _x_ _θ_ using
a local interpolation of the CDF, i.e.


_x_ _θ_ = LI( _u_ ; _X_ _k_ _, Y_ _k_ ( _θ_ )) _._ (13)


Here, _X_ _k_ _, Y_ _k_ ( _θ_ ) represent the subgrid and corresponding local CDF values.


6. Aggregate the generated samples from all the _K_
segments.


By construction, the collection of samples from all segments is differentiable with respect to _θ_ . Similar to the



case without local sampling, the samples at the boundaries of each segment have vanishing gradients, since their
corresponding local CDF values are independent of _θ_ . We
revisit this aspect in detail in section IV.

It is important to note that the local ITS algorithm
offers no advantage in the 1D case, as differentiable
sampling can be accomplished without segmenting the
phase space. However, the method becomes crucial when
extending the algorithm to multidimensional densities.
To illustrate this, consider a two-dimensional density
_p_ ( _x, y|θ_ ) supported on the domain 0 _< x, y <_ 1, from
which we wish to draw samples ( _x, y_ ) _∼_ _p_ ( _x, y|θ_ ) such
that the gradients _∇x_ _θ_ and _∇y_ _θ_ are tractable. We
extend the local 1D ITS approach by segmenting the
two-dimensional phase space and approximating the target density as a product of one-dimensional densities:
_p_ ( _x, y|θ_ ) _≈_ _p_ ( _x, y_ 0 _|θ_ ) _p_ ( _x_ 0 _, y|θ_ ). Here, _x_ 0 _, y_ 0 are fixed values as illustrated in Fig. 2. We select one coordinate to
define orthogonal sampling segments, each of which defines a local one-dimensional density. Samples are then
generated via ITS along each direction and concatenated
to form the desired two-dimensional samples. This procedure naturally generalizes to arbitrary dimensions. Due
to its local and orthogonal structure in phase space, we
refer to this method as the LOITS algorithm.
While LOITS is a differentiable sampling algorithm, it
is important to note that the phase space sampling is only
approximate. In particular, if the target density varies
rapidly, the distribution of samples generated by LOITS
may not accurately reflect the true density. This issue
can be mitigated by increasing the number of segments
used in the local interpolation. However, this becomes
less practical as the dimensionality of the phase space increases. To address this potential shortcoming, we can incorporate the Metropolis–Hastings (MH) algorithm into
the sampling procedure, which leads to asymptotically
exact results. We follow a strategy similar to those used


for improving sampling from multidimensional distributions using normalizing flows and diffusion models [91–
93]. Specifically, we construct a Markov Chain Monte
Carlo (MCMC) sampler using an independent proposal
function based on the LOITS algorithm. Unlike traditional MCMC with Gaussian proposals, in this case the
proposed states are drawn from the LOITS-generated
distribution, which retains differentiability. The LOITS
samples are then subjected to an additional accept–reject
step using the MH criterion, which improves the sampling
fidelity.
As an example, we again consider a two-dimensional
phase space. A LOITS-generated state ( _x, y_ ) is associated with an approximate density _p_ _[∗]_ ( _x, y|θ_ ) =
_p_ ( _x, y_ 0 _|θ_ ) _p_ ( _x_ 0 _, y|θ_ ), where ( _x_ 0 _, y_ 0 ) is the coordinate at the
origin of the local orthogonal segment associated with the
generated state, see Fig. 2. The MH acceptance probability for a newly proposed state ( _x_ _i_ +1 _, y_ _i_ +1 ) is then given
by



_A_ = min _u,_ _[p]_ [(] _[x]_ _[i]_ [+1] _[,]_ _[y]_ _[i]_ [+1] _[|][θ]_ [)] _·_ _p_ _[∗]_ ( _x_ _i_ _,_ _y_ _i_ _|θ_ )
� _p_ ( _x_ _i_ _, y_ _i_ _|θ_ ) _p_ _[∗]_ ( _x_ _i_ +1 _, y_ _i_ +1 _|θ_ )



_,_ (14)
�



7


tance rate remains high, resulting in an efficient sampling
procedure with minimal computational overhead. Importantly, when a proposed sample is rejected, the current
state is retained in the Markov chain. This step is crucial as it ensures that the sampling trajectory remains
continuous and, more importantly, preserves the differentiability of the full sampling process with respect to
the underlying model parameters.
Having established a differentiable sampling procedure, the natural question is whether the resulting gradients are informative. That is, given a loss function
defined at the sample level, do the gradients with respect
to the model parameters provide meaningful directions
for optimization? In the next section, we will address
this question through a controlled example designed to
test the performance of LOITS in a parameter inference
task.


**IV.** **INFERENCE USING DIFFERENTIAL**

**SAMPLING AND GANS**


To demonstrate the use of the LOITS algorithm for inference tasks, we consider the following two-dimensional
density as the ground truth that we aim to reconstruct


_p_ ( _x, y|ϕ_ ) = _N_ _x_ _[ϕ]_ [0] (1 _−_ _x_ ) _[ϕ]_ [1] _y_ _[ϕ]_ [2] (1 _−_ _y_ ) _[ϕ]_ [3] (1 + _ϕ_ 4 _xy_ ) _._
(15)


Here, _N_ is a normalization constant and _ϕ_ denotes a
fixed set of model parameters used to define the ground
truth. The goal of this closure test is to generate training
samples ( _x, y_ ) _∼_ _p_ ( _x, y|ϕ_ ) and reconstruct the underlying
density from a given set of _N_ samples,


_D_ _T_ _≡{_ ( _x_ _i_ _, y_ _i_ ) _}_ _[N]_ _i_ =1 _[∼]_ _[p]_ [(] _[x, y][|][ϕ]_ [)] _[ .]_ (16)


Here, the subscript “ _T_ ” indicates the set of training samples. One option for reconstructing the density is to fit
the parametric form in Eq. (15) using unbinned maximum likelihood methods, thereby reducing the task to
regressing the parameters _ϕ_ . Alternatively, the density
can be represented as a discretized image, where individual _pixels_ of the image must be inferred. In this work,
we focus on the latter approach, as it aligns more closely
with our intended applications in nuclear imaging. This
formulation also allows us to introduce the concept of resolution in the reconstructed image. We emphasize that
the LOITS algorithm is applicable to both approaches.
To infer the pixel values of the density image, we use
GANs [94], as illustrated in Fig. 4. The generator network _G_ maps samples from a latent space to a pixelated
representation of the density. Mathematically, this can
be expressed as


_p_ _[G]_ _ij_ [= NN] _[G]_ [(] _[z, θ]_ _[G]_ [)] _[,]_ (17)


where NN _G_ is a neural network with trainable parameters
_θ_ _G_, and _z_ denotes latent variables sampled from a fixed



where _u ∼U_ [0 _,_ 1], and _p_ ( _x, y_ ) denotes the true target
density. In Fig. 3, we illustrate the application of LOITS
to a two-dimensional “half-moon” distribution. See Appendix A for a definition of the test distribution used
here. In the left panel, we show the reconstructed histogram from LOITS samples with a 4 _×_ 4 phase space
segmentation. As expected, the result exhibits noticeable
distortions, although the global features of the distribution roughly capture the two half-moon structures. Using
the same setup, the middle panel shows the reconstructed
histogram when LOITS is augmented with the MH step.
The MH step corrects the sampling procedure, and the
distribution of the two half-moons is more accurately reconstructed, thanks to the accept–reject correction of the
LOITS proposals. Lastly, in the right panel, we show the
results using LOITS with a finer 50 _×_ 50 phase space
segmentation. As expected, the accuracy improves significantly, but at the expense of more segments and a
higher computational cost.
As mentioned earlier, we have assumed equidistant segmentation along each dimension. In principle, there is
nothing in the LOITS or LOITS+MH procedure that
prevents the use of alternative segmentation strategies.
For example, it is possible to define a phase space grid
with finer segments in regions where the density exhibits
rapid variations and coarser segments in regions where
the density is relatively flat. This adaptive segmentation can improve the accuracy of LOITS sampling and
increase the acceptance rate in the MH step.
In summary, we have demonstrated that differentiable
sampling of a density beyond 1D can be realized using the
LOITS algorithm and that its discretization artifacts can
be efficiently corrected using a MH accept–reject step.
Therefore, the LOITS algorithm provides a differentiable
MCMC sampler. Because LOITS-generated proposals already approximate the true target density, the MH accep

8






|LOITS|Col2|Col3|Col4|
|---|---|---|---|
|**LOITS**|**LOITS**|**LOITS**|**LOITS**|
|||||



FIG. 4. Schematic representation of the GAN setup using the LOITS algorithm.



prior. The indices _i, j_ label the discretized coordinates _x_
and _y_ of the pixelated density, respectively.
The role of LOITS is to take the discretized density _p_ _[G]_ _ij_
as input and generate a set of _N_ samples, denoted by


_D_ _G_ _≡{_ ( _x_ _[θ]_ _i_ _[G]_ _[, y]_ _i_ _[θ]_ _[G]_ [)] _[}]_ _i_ _[N]_ =1 _[∼]_ _[p]_ _[G]_ _ij_ _[.]_ (18)


Here, the subscript “ _G_ ” indicates that the samples _D_ _G_
are obtained using the density image from the generator network. The full pipeline for the generator can be
summarized as


_z →_ NN _G_ _→_ _p_ _[G]_ _ij_ _[→]_ [LOITS] _[ →D]_ _[G]_ _[.]_ (19)


To apply LOITS effectively, the pixel resolution of _p_ _[G]_ _ij_
needs to exceed the segmentation scale of the LOITS algorithm. For instance, within the blue region in Fig. 2, a
sufficient number of pixels along each orthogonal segment
is required to accurately compute the local CDF in each
dimension. By construction, the generated samples are
differentiable with respect to the pixel values. Moreover,
since the pixel values themselves are differentiable with
respect to the generator parameters _θ_ _G_, one can compute
the gradients of the generated samples with respect to _θ_ _G_
via the chain rule. For example, the gradient with respect
to the _k_ [th] parameter of the generator can be estimated

as



training dataset _D_ _T_ or from the generated set _D_ _G_, depending on the stage of adversarial training. The discriminator’s output _d_ serves as a classification score estimating whether a given sample is real or generated.
We follow the GAN training procedure using the binary cross-entropy (BCE) loss to train both networks.
In general, the BCE loss is defined as



_L_ BCE ( _w, {d}_ ) = [1]

_N_



� [ _w_ log _d_ _i_ + (1 _−_ _w_ ) log(1 _−_ _d_ _i_ )] _,_


_i_



(22)


where _{d}_ = _{d_ _i_ = NN _D_ ( _x_ _i_ _, y_ _i_ _, θ_ _D_ ) _|i_ = 1 _, . . ., N_ _}_ is the
set of discriminator outputs for a batch of samples, and
_w ∈_ 0 _,_ 1 indicates the target label, where 1 corresponds
to real samples and 0 to generated samples. The discriminator and generator networks are trained by minimizing
the following loss functions:


_L_ _D_ = _L_ BCE (1 _, {d}_ _T_ ) + _L_ BCE (0 _, {d}_ _G_ )
_L_ _G_ = _L_ BCE (1 _, {d}_ _G_ ) _._ (23)


Here, _{d}_ _T_ and _{d}_ _G_ denote the discriminator outputs
evaluated on the real and generated samples, respectively. The discriminator is trained to correctly classify
real vs. generated samples, while the generator is trained
to fool the discriminator into classifying generated samples as real.
It is important to emphasize that the generator loss
_L_ _G_ requires the generated samples _D_ _G_ to be differentiable with respect to the generator parameters _θ_ _G_ . This
requirement is precisely what the LOITS algorithm is
designed to address, enabling end-to-end optimization of
the generator via differentiable sample generation.
The advantage of using GANs lies in their ability to infer complex densities or images at the sample level without requiring an explicit definition of the likelihood function. This contrasts with traditional likelihood-based inference methods, such as maximum likelihood estimation
or Bayesian inference, which rely on a tractable and often
analytically known likelihood function.



_∂p_ _[G]_ _ij_
_,_ (20)
_∂θ_ _G_ _[k]_



_∂x_ _θ_ _G_



_θ_ _G_ =

_∂θ_ _G_ _[k]_ �



_∂x_ _θ_ _G_

_∂p_ _[G]_ _ij_



_ij_



and similarly for _y_ _θ_ _G_ .
To train the generator network _G_, we adopt the standard GAN setup by introducing a discriminator network
_D_ that is trained to distinguish generated and real (training) samples. The discriminator is a neural network that
we define as


_d_ = NN _D_ ( _x, y, θ_ _D_ ) _,_ (21)


which outputs a scalar classification score. Here, NN _D_
has trainable parameters _θ_ _D_, and ( _x, y_ ) represents an input sample. These samples are drawn either from the


9



0 _._ 8


0 _._ 6


0 _._ 4


0 _._ 2


0 _._ 8


0 _._ 6


0 _._ 4


0 _._ 2


0 _._ 8


0 _._ 6


0 _._ 4


0 _._ 2


0 _._ 8


0 _._ 6


0 _._ 4


0 _._ 2



**True** **Gen** _**.**_ **(epoch = 0)** **Gen** _**.**_ **(epoch = 10K)** **Gen** _**.**_ **(epoch = 100K)**


_**y**_
_**p**_ **(** _**x, y**_ **)**


_**x**_



0 _._ 2 0 _._ 4 0 _._ 6 0 _._ 8



0 _._ 2 0 _._ 4 0 _._ 6 0 _._ 8 0 _._ 2 0 _._ 4 0 _._ 6 0 _._ 8 0 _._ 2 0 _._ 4 0 _._ 6 0 _._ 8



FIG. 5. GAN-based image inference of the underlying density supervised on 10k phase space samples. The top row shows the
ground truth density image (leftmost panel) alongside GAN-generated images at different stages of training. The subsequent
rows display lower-resolution reconstructions obtained by histogramming the phase space samples. As the resolution decreases,
the agreement between the GAN-generated density and the ground truth improves, indicating that large-scale features are
more robustly captured by the generator.



With these considerations in mind, we now describe
the specific setup used for training the GAN:


  - The generator maps a latent noise vector to an
image with 50 _×_ 50 pixels, where each pixel corresponds to a discretized coordinate in the ( _x, y_ )
plane representing the associated density value.



The generator is implemented as a fully connected
neural network followed by transposed convolutional layers. Specifically, it begins with three linear layers, each containing 100 neurons, followed by
four two-dimensional transposed convolutional layers that progressively upsample the features. Each


convolutional layer employs 100 filters. The final output is refined using bilinear interpolation to
match the target resolution of 50 _×_ 50 pixels. All
layers use a leaky ReLU activation function with
a negative slope of 0.2, except for the final layer,
which uses a sigmoid activation to constrain the
output values between 0 and 1.


  - To simplify the GAN training, we average the generator’s output over a batch of 1,000 latent noise
samples before passing it to the LOITS algorithm.
This averaging produces a smoother density image
and improves the efficiency of the training process.


  - The discriminator is trained on batches of 100k
samples, drawn either from the real (training)
dataset or from the generated set produced via
LOITS. It is implemented as a fully connected neural network with four linear layers, each using 128
neurons and leaky ReLU activation. The output
layer uses sigmoid activation to produce a binary
classification score.


  - Both the generator and discriminator are trained
for 100k epochs using 8 NVIDIA T4 GPUs. We
employ a distributed training strategy as described
in Ref. [95].


In Fig. 5, we present the results of GAN training using 10k phase space samples. The upper leftmost panel
shows the ground truth density that the generator is
tasked with learning. The remaining three panels in the
top row illustrate how the generated image evolves during training. As the number of training epochs increases,
the generator progressively improves its output. After
approximately 100k epochs, training stabilizes, and the
generated image closely approximates the ground truth,
although some residual artifacts remain.
To evaluate how well the inferred density matches the
ground truth, we estimate the effective resolution of the
generator output. While several metrics could be used,
we adopt a straightforward approach: histogramming the
event-level samples at various binning resolutions. This
provides a practical way to assess the scale at which the
generated density best reproduces the true underlying
distribution. Rows 2–4 of Fig. 5, analogous to the top
row, show these comparisons using progressively coarser
binning. As evident from the figure, the agreement improves as the binning becomes coarser, indicating that
the generator more reliably captures large-scale structures than fine-grained details. In realistic applications,
the effective resolution could be calibrated using simulations as a function of the available event statistics.
The fact that the effective inferred resolution is coarser
than the original target image highlights a limitation
of the GAN training process. Several factors may contribute to this, including (i) the limited size of the training sample, (ii) the behavior of the LOITS gradients, iii)
the architecture and optimization hyperparameters of the



10


neural networks. In Fig. 6, we isolate the effect of training
sample size by repeating the analysis with datasets containing 10k, 100k, and 1000k samples. The results are
shown as pixel-wise ratios between the GAN-generated
and ground truth densities, allowing for a direct visual
assessment of how reconstruction fidelity improves with
increasing data.
The first column of Fig. 6 shows the pixel-wise ratio
between the GAN-generated density and the true density.
The increasing ratio values close to one across the phase
space indicate that, as the number of training samples
increases, the GAN more accurately infers the underlying density. Columns 2–4 show lower-resolution comparisons obtained by histogramming the samples, following
the same procedure as in Fig. 5. Across all resolutions,
we observe consistent improvements in both the fidelity
and effective resolution of the generated density as more
data becomes available. These results demonstrate that
the LOITS algorithm enables efficient gradient-based optimization for training GANs with up to _O_ (3M) parameters, even in the absence of an explicit likelihood. This
allows for direct event-level inference of complex densities
using differentiable sample generation.
Although the LOITS gradients vanish exactly at the
boundaries of the sampling grid, they remain nonzero
away from the edges, which is sufficient to drive the optimization of the GAN. Because samples are drawn from
a continuous uniform distribution, see Eq. (7), the probability of generating a point exactly on the grid edge
is negligible. While the gradients decay as samples approach the boundaries, their direction, not magnitude,
is the crucial quantity for optimization. Gradient-based
optimizers such as Adam [96] can reliably update model
parameters as long as the gradients are nonzero within
machine precision, regardless of their absolute scale.


**V.** **CONCLUSIONS**


In this work, we introduced an algorithm called
Longitudinal Orthogonal Inverse Transform Sampling
(LOITS), which constructs a differentiable map between
phase space samples and the corresponding multidimensional densities from which those samples are drawn.
This algorithm is a key building block enabling, in principle, an end-to-end simulation-based inference pipeline for
studying the nonperturbative structure of hadrons, which
is encapsulated in various correlation functions such as
PDFs, TMDs, and GPDs, directly from event-level data,
without relying on surrogate machine learning models.
The algorithm extends inverse transform sampling to
multi-dimensional problems and leverages modern autodifferentiation frameworks available in state-of-the-art
machine learning libraries to compute gradients efficiently. While the sampling produced by LOITS is approximate, we have demonstrated that its accuracy can
be systematically improved by incorporating a Metropolis–Hastings accept-reject step.


11



**Model (50x50)** **Histogram (20x20)** **Histogram (10x10)** **Histogram (3x3)**



0 _._ 2 0 _._ 4 0 _._ 6 0 _._ 8 0 _._ 2 0 _._ 4 0 _._ 6 0 _._ 8 0 _._ 2 0 _._ 4 0 _._ 6 0 _._ 8 _**x**_



_**y**_


0 _._ 8


0 _._ 6


0 _._ 4


0 _._ 2


0 _._ 8


0 _._ 6


0 _._ 4


0 _._ 2


0 _._ 8


0 _._ 6


0 _._ 4


0 _._ 2



0 _._ 2 0 _._ 4 0 _._ 6 0 _._ 8



FIG. 6. Pixel-wise ratios between GAN-generated and ground truth densities from different training runs, using datasets of
varying sizes: 10k, 100k, and 1000k samples.



To validate the effectiveness of the gradients produced
by the LOITS algorithm, we tested it on a simplified
two-dimensional problem. Specifically, we employed a
GAN as the generative model for density image reconstruction, closely mirroring its intended application to
three-dimensional hadron structure studies. The results
confirm that LOITS yields accurate and informative gradients for optimizing generative models in a physicsinformed, likelihood-free inference setting. While our
tests focused on GANs, the LOITS algorithm is broadly
applicable and can be integrated with other machine
learning approaches, such as normalizing flows, or even
applied to classical unbinned likelihood analyses.


In addition, the imaging-based approach has enabled
us to quantify the resolution that GAN models can
achieve when trained on event-level information. Our
results confirm that the fidelity of the reconstructed density image improves with increasing amounts of training
data, indicating a direct connection between sample size
and learnable resolution. This provides a valuable tool



for uncertainty quantification in the emerging era of femtoscale imaging, where precise control over resolution is
essential for extracting QCFs from observational data.

An end-to-end simulation-based inference framework
also requires the availability of differentiable detector
simulations. Currently, simulators such as Geant4 [90]
are not differentiable; however, surrogates based on generative modeling are becoming available [74] and can be
integrated into our proposed framework. Alternatively,
sample-level unfolding techniques provide another path
forward, and the LOITS algorithm can be incorporated
into inference pipelines that utilize unfolded data.

While the development of the LOITS algorithm was
motivated by challenges in hadron structure studies,
particularly inverse problems involving multidimensional
distributions such as PDFs, TMDs, and GPDs, we anticipate that its applicability extends well beyond this
domain. In particular, LOITS may offer significant value
in a broad class of optimization problems that require
a differentiable mapping between stochastic observables


and the underlying fundamental degrees of freedom.


**ACKNOWLEDGMENTS**


This work was supported by the DOE contract No.
DE-AC05-06OR23177, under which Jefferson Science Associates, LLC operates Jefferson Lab; the U.S. Department of Energy, Office of Science, Office of Nuclear
Physics, contract no. DE-AC02-06CH11357; the Scientific Discovery through Advanced Computing (SciDAC)
award Femtoscale Imaging of Nuclei using Exascale Platforms. FR was supported by the DOE Quark-Gluon Tomography (QGT) Topical Collaboration under contract
No. DE-SC0023646 and by the DOE, Office of Science,
Office of Nuclear Physics, Early Career Program under
contract No. DE-SC0024358 and DE-SC0025881. NS
was supported by the DOE, Office of Science, Office of
Nuclear Physics in the Early Career Program.


**Appendix A: Double half-moon distribution**


For completeness, we provide here the analytic expressions for the double half-moon (DHM) test distribution


[1] G. D’Agostini, “A multidimensional unfolding method
based on bayes’ theorem,” _[Nuclear Instruments and](http://dx.doi.org/https://doi.org/10.1016/0168-9002(95)00274-X)_
_[Methods in Physics Research Section A: Accelerators,](http://dx.doi.org/https://doi.org/10.1016/0168-9002(95)00274-X)_
_[Spectrometers, Detectors and Associated Equipment](http://dx.doi.org/https://doi.org/10.1016/0168-9002(95)00274-X)_ **362**
[no. 2, (1995) 487–498.](http://dx.doi.org/https://doi.org/10.1016/0168-9002(95)00274-X)

[2] J. C. Collins, D. E. Soper, and G. F. Sterman,
“Factorization of Hard Processes in QCD,” _[Adv. Ser.](http://dx.doi.org/10.1142/9789814503266_0001)_
_[Direct. High Energy Phys.](http://dx.doi.org/10.1142/9789814503266_0001)_ **5** (1989) 1–91,
`[arXiv:hep-ph/0409313](http://arxiv.org/abs/hep-ph/0409313)` .

[3] H.-W. Lin, W. Melnitchouk, A. Prokudin, N. Sato, and
H. Shows, “First Monte Carlo Global Analysis of
Nucleon Transversity with Lattice QCD Constraints,”
_Phys. Rev. Lett._ **120** [no. 15, (2018) 152502,](http://dx.doi.org/10.1103/PhysRevLett.120.152502)
`[arXiv:1710.09858 [hep-ph]](http://arxiv.org/abs/1710.09858)` .

[4] J. Bringewatt, N. Sato, W. Melnitchouk, J.-W. Qiu,
F. Steffens, and M. Constantinou, “Confronting lattice
parton distributions with global QCD analysis,” _[Phys.](http://dx.doi.org/10.1103/PhysRevD.103.016003)_
_Rev. D_ **103** [no. 1, (2021) 016003,](http://dx.doi.org/10.1103/PhysRevD.103.016003) `[arXiv:2010.00548](http://arxiv.org/abs/2010.00548)`

`[[hep-ph]](http://arxiv.org/abs/2010.00548)` .

[5] **Jefferson Lab Angular Momentum (JAM),**
**HadStruc** Collaboration, P. C. Barry _et al._,
“Complementarity of experimental and lattice QCD
data on pion parton distributions,” _[Phys. Rev. D](http://dx.doi.org/10.1103/PhysRevD.105.114051)_ **105**
[no. 11, (2022) 114051,](http://dx.doi.org/10.1103/PhysRevD.105.114051) `[arXiv:2204.00543 [hep-ph]](http://arxiv.org/abs/2204.00543)` .

[6] **JAM** Collaboration, C. Cocuzza, A. Metz,
D. Pitonyak, A. Prokudin, N. Sato, and R. Seidl,
“Transversity Distributions and Tensor Charges of the
Nucleon: Extraction from Dihadron Production and

Their Universal Nature,” _[Phys. Rev. Lett.](http://dx.doi.org/10.1103/PhysRevLett.132.091901)_ **132** no. 9,
[(2024) 091901,](http://dx.doi.org/10.1103/PhysRevLett.132.091901) `[arXiv:2306.12998 [hep-ph]](http://arxiv.org/abs/2306.12998)` .

[7] **Jefferson Lab Angular Momentum (JAM)**
Collaboration, L. Gamberg, M. Malda, J. A. Miller,



12


used in Section III. The DHM distribution is constructed
from two Gaussian rings of the form:



The unnormalized DHM density is then defined as


_p_ DHM ( _x, y_ ) = � _f_ ring ( _x, y_ ; _**r**_ _[i]_ ) gate _[i]_ ( _y_ ) _._ (A4)

_i_ =+ _,−_


and we have set the DHM parameters of as ∆ _x_ = 0 _._ 1,
∆ _y_ = 0 _._ 1, _w_ = 0 _._ 1, _R_ = 0 _._ 2, _σ_ = 0 _._ 02.


D. Pitonyak, A. Prokudin, and N. Sato, “Updated QCD
global analysis of single transverse-spin asymmetries:
Extracting H˜, and the role of the Soffer bound and
lattice QCD,” _Phys. Rev. D_ **106** [no. 3, (2022) 034014,](http://dx.doi.org/10.1103/PhysRevD.106.034014)
`[arXiv:2205.00999 [hep-ph]](http://arxiv.org/abs/2205.00999)` .

[8] **Jefferson Lab Angular Momentum, HadStruc**
Collaboration, J. Karpie, R. M. Whitehill,
W. Melnitchouk, C. Monahan, K. Orginos, J. W. Qiu,
D. G. Richards, N. Sato, and S. Zafeiropoulos, “Gluon
helicity from global analysis of experimental data and
lattice QCD Ioffe time distributions,” _[Phys. Rev. D](http://dx.doi.org/10.1103/PhysRevD.109.036031)_ **109**
[no. 3, (2024) 036031,](http://dx.doi.org/10.1103/PhysRevD.109.036031) `[arXiv:2310.18179 [hep-ph]](http://arxiv.org/abs/2310.18179)` .

[9] **JAM** Collaboration, N. T. Hunt-Smith, C. Cocuzza,
W. Melnitchouk, N. Sato, A. W. Thomas, and M. J.
White, “New Data-Driven Constraints on the Sign of
Gluon Polarization in the Proton,” _[Phys. Rev. Lett.](http://dx.doi.org/10.1103/PhysRevLett.133.161901)_ **133**
[no. 16, (2024) 161901,](http://dx.doi.org/10.1103/PhysRevLett.133.161901) `[arXiv:2403.08117 [hep-ph]](http://arxiv.org/abs/2403.08117)` .

[10] A. Ablat _et al._, “New results in the CTEQ-TEA global
analysis of parton distributions in the nucleon,” _[Eur.](http://dx.doi.org/10.1140/epjp/s13360-024-05865-x)_
_Phys. J. Plus_ **[139](http://dx.doi.org/10.1140/epjp/s13360-024-05865-x)** no. 12, (2024) 1063,
`[arXiv:2408.04020 [hep-ph]](http://arxiv.org/abs/2408.04020)` .

[11] **NNPDF** Collaboration, R. D. Ball _et al._,
“Determination of the theory uncertainties from missing
higher orders on NNLO parton distributions with
percent accuracy,” _Eur. Phys. J. C_ **[84](http://dx.doi.org/10.1140/epjc/s10052-024-12772-z)** no. 5, (2024)
517, `[arXiv:2401.10319 [hep-ph]](http://arxiv.org/abs/2401.10319)` .

[12] S. Bhattacharya, Z.-B. Kang, A. Metz, G. Penn, and
D. Pitonyak, “First global QCD analysis of the TMD
g1T from semi-inclusive DIS data,” _[Phys. Rev. D](http://dx.doi.org/10.1103/PhysRevD.105.034007)_ **105**
[no. 3, (2022) 034007,](http://dx.doi.org/10.1103/PhysRevD.105.034007) `[arXiv:2110.10253 [hep-ph]](http://arxiv.org/abs/2110.10253)` .

[13] T. Cridge _et al._, “Combination of aN [3] LO PDFs and
implications for Higgs production cross-sections at the



� 2 [�]



_f_ ring ( _x, y_ ; _**r**_ _[i]_ ) = exp



�



_−_ [1]



2



_i_
_r_ ( _x,_ _y_ ; _**r**_ ) _−_ _R_
� _σ_



_,_ (A1)



where _R_ is the mean ring radius, and _r_ ( _x, y_ ; _**r**_ _[i]_ ) denotes
the radial distance from the point ( _x, y_ ) to the center
_**r**_ _[i]_ = ( _r_ _x_ _[i]_ _[, r]_ _y_ _[i]_ [). We take the centers of the two Gaussian]
rings as _**r**_ _[±]_ = � 12 _[∓]_ [∆] _[x,]_ 2 [1] _[±]_ [ ∆] _[y]_ � _,_ and restrict each ring



1 [1]

2 _[∓]_ [∆] _[x,]_ 2



rings as _**r**_ _[±]_ = � 12 _[∓]_ [∆] _[x,]_ 2 [1] _[±]_ [ ∆] _[y]_ � _,_ and restrict each ring

to one side of the plane using soft gating functions:



_,_ (A2)
��


_._ (A3)
��



gate [+] ( _y_ ) = [1]

2



gate _[−]_ ( _y_ ) = [1]

2



1 + tanh _y −_ _r_ _y_ +
� � _w_

1 + tanh _r_ _y−_ _[−]_ _[y]_
� � _w_


LHC,” _J. Phys. G_ **[52](http://dx.doi.org/10.1088/1361-6471/adde78)** (2025) 6, `[arXiv:2411.05373](http://arxiv.org/abs/2411.05373)`

`[[hep-ph]](http://arxiv.org/abs/2411.05373)` .

[14] **Jefferson Lab Angular Momentum (JAM)**
Collaboration, P. C. Barry, C.-R. Ji, N. Sato, and
W. Melnitchouk, “Global QCD Analysis of Pion Parton
Distributions with Threshold Resummation,” _[Phys.](http://dx.doi.org/10.1103/PhysRevLett.127.232001)_
_Rev. Lett._ **127** [no. 23, (2021) 232001,](http://dx.doi.org/10.1103/PhysRevLett.127.232001)
`[arXiv:2108.05822 [hep-ph]](http://arxiv.org/abs/2108.05822)` .

[15] **Jefferson Lab Angular Momentum (JAM)**
Collaboration, C. Cocuzza, C. E. Keppel, H. Liu,
W. Melnitchouk, A. Metz, N. Sato, and A. W. Thomas,
“Isovector EMC Effect from Global QCD Analysis with
MARATHON Data,” _Phys. Rev. Lett._ **[127](http://dx.doi.org/10.1103/PhysRevLett.127.242001)** no. 24,
[(2021) 242001,](http://dx.doi.org/10.1103/PhysRevLett.127.242001) `[arXiv:2104.06946 [hep-ph]](http://arxiv.org/abs/2104.06946)` .

[16] C. Cocuzza, N. T. Hunt-Smith, W. Melnitchouk,
N. Sato, and A. W. Thomas, “Global QCD analysis of
spin PDFs in the proton with high- _x_ and lattice
constraints,” `[arXiv:2506.13616 [hep-ph]](http://arxiv.org/abs/2506.13616)` .

[17] I. Borsa, M. Stratmann, W. Vogelsang, D. de Florian,
and R. Sassot, “Next-to-Next-to-Leading Order Global
Analysis of Polarized Parton Distribution Functions,”
_Phys. Rev. Lett._ **133** [no. 15, (2024) 151901,](http://dx.doi.org/10.1103/PhysRevLett.133.151901)
`[arXiv:2407.11635 [hep-ph]](http://arxiv.org/abs/2407.11635)` .

[18] I. Borsa, D. de Florian, R. Sassot, and M. Stratmann,
“Pion fragmentation functions at high energy colliders,”
_Phys. Rev. D_ **105** [no. 3, (2022) L031502,](http://dx.doi.org/10.1103/PhysRevD.105.L031502)
`[arXiv:2110.14015 [hep-ph]](http://arxiv.org/abs/2110.14015)` .

[19] Y. Guo, F. Yuan, and W. Zhao, “Bayesian Inferring
Nucleon’s Gravitation Form Factors via Near-threshold
_J/ψ_ Photoproduction,” `[arXiv:2501.10532 [hep-ph]](http://arxiv.org/abs/2501.10532)` .

[20] **MAP (Multi-dimensional Analyses of Partonic**
**distributions)** Collaboration, A. Bacchetta,
V. Bertone, C. Bissolotti, M. Cerutti, M. Radici,
S. Rodini, and L. Rossi, “Neural-Network Extraction of
Unpolarized Transverse-Momentum-Dependent
Distributions,” _Phys. Rev. Lett._ **[135](http://dx.doi.org/10.1103/csc2-bj91)** no. 2, (2025)
[021904,](http://dx.doi.org/10.1103/csc2-bj91) `[arXiv:2502.04166 [hep-ph]](http://arxiv.org/abs/2502.04166)` .

[21] V. Moos, I. Scimemi, A. Vladimirov, and P. Zurita,
“Determination of unpolarized TMD distributions from
the fit of Drell-Yan and SIDIS data at N [4] LL,”
`[arXiv:2503.11201 [hep-ph]](http://arxiv.org/abs/2503.11201)` .

[22] **Jefferson Lab Angular Momentum (JAM)**
Collaboration, P. C. Barry, L. Gamberg,
W. Melnitchouk, E. Moffat, D. Pitonyak, A. Prokudin,
and N. Sato, “Tomography of pions and protons via
transverse momentum dependent distributions,” _[Phys.](http://dx.doi.org/10.1103/PhysRevD.108.L091504)_
_Rev. D_ **108** [no. 9, (2023) L091504,](http://dx.doi.org/10.1103/PhysRevD.108.L091504) `[arXiv:2302.01192](http://arxiv.org/abs/2302.01192)`

`[[hep-ph]](http://arxiv.org/abs/2302.01192)` .

[23] J. Cruz-Martinez, T. Hasenack, F. Hekhorn, G. Magni,
E. R. Nocera, T. R. Rabemananjara, J. Rojo,
T. Sharma, and G. van Seeventer, “NNPDFpol2.0: a
global determination of polarised PDFs and their
uncertainties at next-to-next-to-leading order,”
`[arXiv:2503.11814 [hep-ph]](http://arxiv.org/abs/2503.11814)` .

[24] **MAP (Multi-dimensional Analyses of Partonic**
**distributions)** Collaboration, R. Abdul Khalek,
V. Bertone, A. Khoudli, and E. R. Nocera, “Pion and
kaon fragmentation functions at next-to-next-to-leading
order,” _Phys. Lett. B_ **[834](http://dx.doi.org/10.1016/j.physletb.2022.137456)** (2022) 137456,
`[arXiv:2204.10331 [hep-ph]](http://arxiv.org/abs/2204.10331)` .

[25] A. Boehnlein _et al._, “Colloquium: Machine learning in
nuclear physics,” _Rev. Mod. Phys._ **[94](http://dx.doi.org/10.1103/RevModPhys.94.031003)** no. 3, (2022)
[031003,](http://dx.doi.org/10.1103/RevModPhys.94.031003) `[arXiv:2112.02309 [nucl-th]](http://arxiv.org/abs/2112.02309)` .



13


[26] M. Arratia _et al._, “Publishing unbinned differential
cross section results,” _JINST_ **17** [no. 01, (2022) P01024,](http://dx.doi.org/10.1088/1748-0221/17/01/P01024)
`[arXiv:2109.13243 [hep-ph]](http://arxiv.org/abs/2109.13243)` .

[27] L. Benato, C. Giordano, C. Krause, A. Li,
R. Sch¨ofbeck, D. Schwarz, M. Shooshtari, and D. Wang,
“Unbinned inclusive cross-section measurements with

machine-learned systematic uncertainties,” 5, 2025.
`[arXiv:2505.05544 [hep-ph]](http://arxiv.org/abs/2505.05544)` .

[28] J. S. Kaastra and J. A. M. Bleeker, “Optimal binning of
X-ray spectra and response matrix design,” _[Astron.](http://dx.doi.org/10.1051/0004-6361/201527395)_
_Astrophys._ **[587](http://dx.doi.org/10.1051/0004-6361/201527395)** (2016) A151, `[arXiv:1601.05309](http://arxiv.org/abs/1601.05309)`

`[[astro-ph.IM]](http://arxiv.org/abs/1601.05309)` .

[29] P. De Castro and T. Dorigo, “INFERNO:
Inference-Aware Neural Optimisation,” _[Comput. Phys.](http://dx.doi.org/10.1016/j.cpc.2019.06.007)_
_Commun._ **[244](http://dx.doi.org/10.1016/j.cpc.2019.06.007)** (2019) 170–179, `[arXiv:1806.04743](http://arxiv.org/abs/1806.04743)`

`[[stat.ML]](http://arxiv.org/abs/1806.04743)` .

[30] D. Dannheim, A. Voigt, K.-J. Grahn, P. Speckmayer,
and T. Carli, “PDE-Foam: A probability density
estimation method using self-adapting phase-space
binning,” _[Nucl. Instrum. Meth. A](http://dx.doi.org/10.1016/j.nima.2009.05.028)_ **606** (2009) 717–727,
`[arXiv:0812.0922 [physics.data-an]](http://arxiv.org/abs/0812.0922)` .

[31] **TMVA** Collaboration, A. Hocker _et al._, “TMVA Toolkit for Multivariate Data Analysis,”
`[arXiv:physics/0703039](http://arxiv.org/abs/physics/0703039)` .

[32] J. Brehmer, K. Cranmer, G. Louppe, and J. Pavez,
“Constraining Effective Field Theories with Machine
Learning,” _Phys. Rev. Lett._ **121** [no. 11, (2018) 111801,](http://dx.doi.org/10.1103/PhysRevLett.121.111801)
`[arXiv:1805.00013 [hep-ph]](http://arxiv.org/abs/1805.00013)` .

[33] K. Cranmer, J. Brehmer, and G. Louppe, “The frontier
of simulation-based inference,” _[Proc. Nat. Acad. Sci.](http://dx.doi.org/10.1073/pnas.1912789117)_
**117** [no. 48, (2020) 30055–30062,](http://dx.doi.org/10.1073/pnas.1912789117) `[arXiv:1911.01429](http://arxiv.org/abs/1911.01429)`

`[[stat.ML]](http://arxiv.org/abs/1911.01429)` .

[34] **JETSCAPE** Collaboration, D. Everett _et al._,
“Multisystem Bayesian constraints on the transport
coefficients of QCD matter,” _Phys. Rev. C_ **[103](http://dx.doi.org/10.1103/PhysRevC.103.054904)** no. 5,
[(2021) 054904,](http://dx.doi.org/10.1103/PhysRevC.103.054904) `[arXiv:2011.01430 [hep-ph]](http://arxiv.org/abs/2011.01430)` .

[35] R. Mastandrea, B. Nachman, and T. Plehn,
“Constraining the Higgs potential with neural
simulation-based inference for di-Higgs production,”
_Phys. Rev. D_ **110** [no. 5, (2024) 056004,](http://dx.doi.org/10.1103/PhysRevD.110.056004)
`[arXiv:2405.15847 [hep-ph]](http://arxiv.org/abs/2405.15847)` .

[36] B. Assi, S. H¨oche, K. Lee, and J. Thaler, “QCD Theory
meets Information Theory,” `[arXiv:2501.17219](http://arxiv.org/abs/2501.17219)`

`[[hep-ph]](http://arxiv.org/abs/2501.17219)` .

[37] C. L. Cheng, R. Das, R. Li, R. Mastandrea, V. Mikuni,
B. Nachman, D. Shih, and G. Singh, “Generator Based
Inference (GBI),” `[arXiv:2506.00119 [hep-ph]](http://arxiv.org/abs/2506.00119)` .

[38] S. Frixione, P. Nason, and C. Oleari, “Matching NLO
QCD computations with Parton Shower simulations:
the POWHEG method,” _JHEP_ **11** [(2007) 070,](http://dx.doi.org/10.1088/1126-6708/2007/11/070)
`[arXiv:0709.2092 [hep-ph]](http://arxiv.org/abs/0709.2092)` .

[39] **NNLOJET** Collaboration, A. Huss _et al._, “NNLOJET:
a parton-level event generator for jet cross sections at
NNLO QCD accuracy,” `[arXiv:2503.22804 [hep-ph]](http://arxiv.org/abs/2503.22804)` .

[40] I. Borsa, D. de Florian, and I. Pedron, “NNLO jet
production in neutral and charged current polarized
deep inelastic scattering,” _Phys. Rev. D_ **[107](http://dx.doi.org/10.1103/PhysRevD.107.054027)** no. 5,
[(2023) 054027,](http://dx.doi.org/10.1103/PhysRevD.107.054027) `[arXiv:2212.06625 [hep-ph]](http://arxiv.org/abs/2212.06625)` .

[41] B. Berthou _et al._, “PARTONS: PARtonic Tomography
Of Nucleon Software: A computing framework for the
phenomenology of Generalized Parton Distributions,”
_Eur. Phys. J. C_ **[78](http://dx.doi.org/10.1140/epjc/s10052-018-5948-0)** no. 6, (2018) 478,
`[arXiv:1512.06174 [hep-ph]](http://arxiv.org/abs/1512.06174)` .


[42] E. C. Aschenauer _et al._, “Study of deeply virtual
Compton scattering at the future Electron-Ion
Collider,” `[arXiv:2503.05908 [hep-ph]](http://arxiv.org/abs/2503.05908)` .

[43] E. Perez, L. Schoeffel, and L. Favart, “MILOU: A
Monte-Carlo for deeply virtual Compton scattering,”
`[arXiv:hep-ph/0411389](http://arxiv.org/abs/hep-ph/0411389)` .

[44] T. Toll and T. Ullrich, “The dipole model Monte Carlo
generator Sar _t_ re 1,” _[Comput. Phys. Commun.](http://dx.doi.org/10.1016/j.cpc.2014.03.010)_ **185**
[(2014) 1835–1853,](http://dx.doi.org/10.1016/j.cpc.2014.03.010) `[arXiv:1307.8059 [hep-ph]](http://arxiv.org/abs/1307.8059)` .

[45] M. Lomnitz and S. Klein, “Exclusive vector meson
production at an electron-ion collider,” _[Phys. Rev. C](http://dx.doi.org/10.1103/PhysRevC.99.015203)_ **99**
[no. 1, (2019) 015203,](http://dx.doi.org/10.1103/PhysRevC.99.015203) `[arXiv:1803.06420 [nucl-ex]](http://arxiv.org/abs/1803.06420)` .

[46] O. Gryniuk, S. Joosten, Z.-E. Meziani, and
M. Vanderhaeghen, “Υ photoproduction on the proton
at the Electron-Ion Collider,” _[Phys. Rev. D](http://dx.doi.org/10.1103/PhysRevD.102.014016)_ **102** no. 1,
[(2020) 014016,](http://dx.doi.org/10.1103/PhysRevD.102.014016) `[arXiv:2005.09293 [hep-ph]](http://arxiv.org/abs/2005.09293)` .

[47] M. Bellagente, A. Butter, G. Kasieczka, T. Plehn,
A. Rousselot, R. Winterhalder, L. Ardizzone, and
U. K¨othe, “Invertible Networks or Partons to Detector
and Back Again,” _[SciPost Phys.](http://dx.doi.org/10.21468/SciPostPhys.9.5.074)_ **9** (2020) 074,
`[arXiv:2006.06685 [hep-ph]](http://arxiv.org/abs/2006.06685)` .

[48] B. Nachman and S. Prestel, “Morphing parton showers
with event derivatives,” `[arXiv:2208.02274 [hep-ph]](http://arxiv.org/abs/2208.02274)` .

[49] T. Heimel, R. Winterhalder, A. Butter, J. Isaacson,
C. Krause, F. Maltoni, O. Mattelaer, and T. Plehn,
“MadNIS - Neural multi-channel importance sampling,”
_SciPost Phys._ **15** [no. 4, (2023) 141,](http://dx.doi.org/10.21468/SciPostPhys.15.4.141) `[arXiv:2212.06172](http://arxiv.org/abs/2212.06172)`

`[[hep-ph]](http://arxiv.org/abs/2212.06172)` .

[50] V. S. Ngairangbam and M. Spannowsky, “Interpretable
deep learning models for the inference and classification
of LHC data,” _JHEP_ **05** [(2024) 004,](http://dx.doi.org/10.1007/JHEP05(2024)004) `[arXiv:2312.12330](http://arxiv.org/abs/2312.12330)`

`[[hep-ph]](http://arxiv.org/abs/2312.12330)` .

[51] J. Chan, X. Ju, A. Kania, B. Nachman, V. Sangli, and
A. Siodmok, “Fitting a deep generative hadronization
model,” _JHEP_ **09** [(2023) 084,](http://dx.doi.org/10.1007/JHEP09(2023)084) `[arXiv:2305.17169](http://arxiv.org/abs/2305.17169)`

`[[hep-ph]](http://arxiv.org/abs/2305.17169)` .

[52] H. Bahl, N. Elmer, L. Favaro, M. Haußmann, T. Plehn,
and R. Winterhalder, “Accurate Surrogate Amplitudes
with Calibrated Uncertainties,” `[arXiv:2412.12069](http://arxiv.org/abs/2412.12069)`

`[[hep-ph]](http://arxiv.org/abs/2412.12069)` .

[53] T. Heimel, O. Mattelaer, T. Plehn, and
R. Winterhalder, “Differentiable MadNIS-Lite,” _[SciPost](http://dx.doi.org/10.21468/SciPostPhys.18.1.017)_
_Phys._ **18** [no. 1, (2025) 017,](http://dx.doi.org/10.21468/SciPostPhys.18.1.017) `[arXiv:2408.01486](http://arxiv.org/abs/2408.01486)`

`[[hep-ph]](http://arxiv.org/abs/2408.01486)` .

[54] J. Y. Araz, A. Beck, M. Reboud, M. Spannowsky, and
D. van Dyk, “Communicating Likelihoods with
Normalising Flows,” `[arXiv:2502.09494 [hep-ph]](http://arxiv.org/abs/2502.09494)` .

[55] **GEANT4** Collaboration, S. Agostinelli _et al._,
“GEANT4 - A Simulation Toolkit,” _[Nucl. Instrum.](http://dx.doi.org/10.1016/S0168-9002(03)01368-8)_
_Meth. A_ **[506](http://dx.doi.org/10.1016/S0168-9002(03)01368-8)** (2003) 250–303.

[56] K. Datta, D. Kar, and D. Roy, “Unfolding with
Generative Adversarial Networks,” `[arXiv:1806.00433](http://arxiv.org/abs/1806.00433)`

`[[physics.data-an]](http://arxiv.org/abs/1806.00433)` .

[57] M. Bellagente, A. Butter, G. Kasieczka, T. Plehn, and
R. Winterhalder, “How to GAN away Detector Effects,”
_SciPost Phys._ **8** [no. 4, (2020) 070,](http://dx.doi.org/10.21468/SciPostPhys.8.4.070) `[arXiv:1912.00477](http://arxiv.org/abs/1912.00477)`

`[[hep-ph]](http://arxiv.org/abs/1912.00477)` .

[58] J. N. Howard, S. Mandt, D. Whiteson, and Y. Yang,
“Learning to simulate high energy particle collisions
from unlabeled data,” _Sci. Rep._ **12** [(2022) 7567,](http://dx.doi.org/10.1038/s41598-022-10966-7)
`[arXiv:2101.08944 [hep-ph]](http://arxiv.org/abs/2101.08944)` .

[59] M. Vandegar, M. Kagan, A. Wehenkel, and G. Louppe,
“Neural Empirical Bayes: Source Distribution
Estimation and its Applications to Simulation-Based



14


Inference,” `[arXiv:2011.05836 [stat.ML]](http://arxiv.org/abs/2011.05836)` .

[60] M. Backes, A. Butter, M. Dunford, and B. Malaescu,
“An unfolding method based on conditional invertible
neural networks (cINN) using iterative training,”
_SciPost Phys. Core_ **[7](http://dx.doi.org/10.21468/scipostphyscore.7.1.007)** no. 1, (2024) 007,
`[arXiv:2212.08674 [hep-ph]](http://arxiv.org/abs/2212.08674)` .

[61] H. Du, C. Krause, V. Mikuni, B. Nachman, I. Pang,
and D. Shih, “Unifying simulation and inference with
normalizing flows,” _Phys. Rev. D_ **111** [no. 7, (2025)](http://dx.doi.org/10.1103/PhysRevD.111.076004)
[076004,](http://dx.doi.org/10.1103/PhysRevD.111.076004) `[arXiv:2404.18992 [hep-ph]](http://arxiv.org/abs/2404.18992)` .

[62] S. Diefenbacher, G.-H. Liu, V. Mikuni, B. Nachman,
and W. Nie, “Improving generative model-based
unfolding with Schr¨odinger bridges,” _[Phys. Rev. D](http://dx.doi.org/10.1103/PhysRevD.109.076011)_ **109**
[no. 7, (2024) 076011,](http://dx.doi.org/10.1103/PhysRevD.109.076011) `[arXiv:2308.12351 [hep-ph]](http://arxiv.org/abs/2308.12351)` .

[63] K. Desai, B. Nachman, and J. Thaler, “Moment
extraction using an unfolding protocol without
binning,” _Phys. Rev. D_ **110** [no. 11, (2024) 116013,](http://dx.doi.org/10.1103/PhysRevD.110.116013)
`[arXiv:2407.11284 [hep-ph]](http://arxiv.org/abs/2407.11284)` .

[64] A. Butter, S. Diefenbacher, N. Huetsch, V. Mikuni,
B. Nachman, S. Palacios Schweitzer, and T. Plehn,
“Generative unfolding with distribution mapping,”
_SciPost Phys._ **18** [no. 6, (2025) 200,](http://dx.doi.org/10.21468/SciPostPhys.18.6.200) `[arXiv:2411.02495](http://arxiv.org/abs/2411.02495)`

`[[hep-ph]](http://arxiv.org/abs/2411.02495)` .

[65] L. Favaro, R. Kogler, A. Paasch, S. Palacios Schweitzer,
T. Plehn, and D. Schwarz, “How to Unfold Top
Decays,” `[arXiv:2501.12363 [hep-ph]](http://arxiv.org/abs/2501.12363)` .

[66] A. Andreassen, P. T. Komiske, E. M. Metodiev,
B. Nachman, and J. Thaler, “OmniFold: A Method to
Simultaneously Unfold All Observables,” _[Phys. Rev.](http://dx.doi.org/10.1103/PhysRevLett.124.182001)_
_Lett._ **124** [no. 18, (2020) 182001,](http://dx.doi.org/10.1103/PhysRevLett.124.182001) `[arXiv:1911.09107](http://arxiv.org/abs/1911.09107)`

`[[hep-ph]](http://arxiv.org/abs/1911.09107)` .

[67] A. Andreassen, P. T. Komiske, E. M. Metodiev,
B. Nachman, A. Suresh, and J. Thaler, “Scaffolding
Simulations with Deep Learning for High-dimensional
Deconvolution,” in _9th International Conference on_
_Learning Representations_ . 5, 2021. `[arXiv:2105.04448](http://arxiv.org/abs/2105.04448)`

`[[stat.ML]](http://arxiv.org/abs/2105.04448)` .

[68] C.-C. Pan, X. Dong, Y.-C. Sun, A.-Y. Cheng, A.-B.
Wang, Y.-X. Hu, and H. Cai, “SwdFold:A Reweighting
and Unfolding method based on Optimal Transport
Theory,” `[arXiv:2406.01635 [physics.data-an]](http://arxiv.org/abs/2406.01635)` .

[69] R. Milton, V. Mikuni, T. Lee, M. Arratia,
T. Wamorkar, and B. Nachman, “Tools for unbinned
unfolding,” _JINST_ **20** [no. 05, (2025) P05034,](http://dx.doi.org/10.1088/1748-0221/20/05/P05034)
`[arXiv:2503.09720 [hep-ph]](http://arxiv.org/abs/2503.09720)` .

[70] A. Falc˜ao and A. Takacs, “High-Dimensional Unfolding
in Large Backgrounds,” `[arXiv:2507.06291 [hep-ph]](http://arxiv.org/abs/2507.06291)` .

[71] **H1** Collaboration, V. Andreev _et al._, “Measurement of
Lepton-Jet Correlation in Deep-Inelastic Scattering
with the H1 Detector Using Machine Learning for
Unfolding,” _Phys. Rev. Lett._ **128** [no. 13, (2022) 132002,](http://dx.doi.org/10.1103/PhysRevLett.128.132002)
`[arXiv:2108.12376 [hep-ex]](http://arxiv.org/abs/2108.12376)` .

[72] **ATLAS** Collaboration, G. Aad _et al._, “Simultaneous
Unbinned Differential Cross-Section Measurement of
Twenty-Four Z+jets Kinematic Observables with the
ATLAS Detector,” _Phys. Rev. Lett._ **[133](http://dx.doi.org/10.1103/PhysRevLett.133.261803)** no. 26, (2024)
[261803,](http://dx.doi.org/10.1103/PhysRevLett.133.261803) `[arXiv:2405.20041 [hep-ex]](http://arxiv.org/abs/2405.20041)` .

[73] **ATLAS** Collaboration, G. Aad _et al._, “Measurement of
off-shell Higgs boson production in the _H_ _[∗]_ _→_ _ZZ →_ 4 _ℓ_
decay channel using a neural simulation-based inference
technique in 13 TeV pp collisions with the ATLAS
detector,” _Rept. Prog. Phys._ **88** [no. 5, (2025) 057803,](http://dx.doi.org/10.1088/1361-6633/adcd9a)
`[arXiv:2412.01548 [hep-ex]](http://arxiv.org/abs/2412.01548)` .


[74] O. Amram _et al._, “CaloChallenge 2022: A Community
Challenge for Fast Calorimeter Simulation,”
`[arXiv:2410.21611 [physics.ins-det]](http://arxiv.org/abs/2410.21611)` .

[75] K. Desai, O. Long, and B. Nachman, “Unbinned
Inference with Correlated Events,” `[arXiv:2504.14072](http://arxiv.org/abs/2504.14072)`

`[[physics.data-an]](http://arxiv.org/abs/2504.14072)` .

[76] **NNPDF** Collaboration, R. D. Ball _et al._, “Parton
distributions from high-precision collider data,” _[Eur.](http://dx.doi.org/10.1140/epjc/s10052-017-5199-5)_
_Phys. J. C_ **77** [no. 10, (2017) 663,](http://dx.doi.org/10.1140/epjc/s10052-017-5199-5) `[arXiv:1706.00428](http://arxiv.org/abs/1706.00428)`

`[[hep-ph]](http://arxiv.org/abs/1706.00428)` .

[77] L. Del Debbio, T. Giani, and M. Wilson, “Bayesian
approach to inverse problems: an application to
NNPDF closure testing,” _[Eur. Phys. J. C](http://dx.doi.org/10.1140/epjc/s10052-022-10297-x)_ **82** no. 4,
[(2022) 330,](http://dx.doi.org/10.1140/epjc/s10052-022-10297-x) `[arXiv:2111.05787 [hep-ph]](http://arxiv.org/abs/2111.05787)` .

[78] J. Pumplin, D. R. Stump, J. Huston, H. L. Lai, P. M.
Nadolsky, and W. K. Tung, “New generation of parton
distributions with uncertainties from global QCD
analysis,” _JHEP_ **07** [(2002) 012,](http://dx.doi.org/10.1088/1126-6708/2002/07/012) `[arXiv:hep-ph/0201195](http://arxiv.org/abs/hep-ph/0201195)` .

[79] **Jefferson Lab Angular Momentum** Collaboration,
N. Sato, W. Melnitchouk, S. E. Kuhn, J. J. Ethier, and
A. Accardi, “Iterative Monte Carlo analysis of
spin-dependent parton distributions,” _[Phys. Rev. D](http://dx.doi.org/10.1103/PhysRevD.93.074005)_ **93**
[no. 7, (2016) 074005,](http://dx.doi.org/10.1103/PhysRevD.93.074005) `[arXiv:1601.07782 [hep-ph]](http://arxiv.org/abs/1601.07782)` .

[80] N. Sato, J. J. Ethier, W. Melnitchouk, M. Hirai,
S. Kumano, and A. Accardi, “First Monte Carlo
analysis of fragmentation functions from single-inclusive
_e_ [+] _e_ _[−]_ annihilation,” _Phys. Rev. D_ **94** [no. 11, (2016)](http://dx.doi.org/10.1103/PhysRevD.94.114004)
[114004,](http://dx.doi.org/10.1103/PhysRevD.94.114004) `[arXiv:1609.00899 [hep-ph]](http://arxiv.org/abs/1609.00899)` .

[81] **MAP (Multi-dimensional Analyses of Partonic**
**distributions)** Collaboration, A. Bacchetta,
V. Bertone, C. Bissolotti, G. Bozzi, M. Cerutti,
F. Delcarro, M. Radici, L. Rossi, and A. Signori,
“Flavor dependence of unpolarized quark transverse
momentum distributions from a global fit,” _JHEP_ **[08](http://dx.doi.org/10.1007/JHEP08(2024)232)**
[(2024) 232,](http://dx.doi.org/10.1007/JHEP08(2024)232) `[arXiv:2405.13833 [hep-ph]](http://arxiv.org/abs/2405.13833)` .

[82] X.-D. Ji, “Deeply virtual Compton scattering,” _[Phys.](http://dx.doi.org/10.1103/PhysRevD.55.7114)_
_Rev. D_ **55** [(1997) 7114–7125,](http://dx.doi.org/10.1103/PhysRevD.55.7114) `[arXiv:hep-ph/9609381](http://arxiv.org/abs/hep-ph/9609381)` .

[83] A. V. Belitsky and A. V. Radyushkin, “Unraveling
hadron structure with generalized parton distributions,”
_Phys. Rept._ **[418](http://dx.doi.org/10.1016/j.physrep.2005.06.002)** (2005) 1–387, `[arXiv:hep-ph/0504030](http://arxiv.org/abs/hep-ph/0504030)` .

[84] A. Freese, D. Adamiak, I. Clo¨et, W. Melnitchouk, J. W.
Qiu, N. Sato, and M. Zaccheddu, “Kernel methods for
evolution of generalized parton distributions,” _[Comput.](http://dx.doi.org/10.1016/j.cpc.2025.109552)_
_[Phys. Commun.](http://dx.doi.org/10.1016/j.cpc.2025.109552)_ **311** (2025) 109552, `[arXiv:2412.13450](http://arxiv.org/abs/2412.13450)`

`[[hep-ph]](http://arxiv.org/abs/2412.13450)` .

[85] A. V. Vinnikov, “Code for prompt numerical
computation of the leading order GPD evolution,”
`[arXiv:hep-ph/0604248](http://arxiv.org/abs/hep-ph/0604248)` .

[86] V. Bertone, H. Dutrieux, C. Mezrag, J. M. Morgado,
and H. Moutarde, “Revisiting evolution equations for
generalised parton distributions,” _[Eur. Phys. J. C](http://dx.doi.org/10.1140/epjc/s10052-022-10793-0)_ **82**
[no. 10, (2022) 888,](http://dx.doi.org/10.1140/epjc/s10052-022-10793-0) `[arXiv:2206.01412 [hep-ph]](http://arxiv.org/abs/2206.01412)` .

[87] V. Bertone, H. Dutrieux, C. Mezrag, H. Moutarde, and
P. Sznajder, “Deconvolution problem of deeply virtual



15


Compton scattering,” _Phys. Rev. D_ **[103](http://dx.doi.org/10.1103/PhysRevD.103.114019)** no. 11, (2021)
[114019,](http://dx.doi.org/10.1103/PhysRevD.103.114019) `[arXiv:2104.03836 [hep-ph]](http://arxiv.org/abs/2104.03836)` .

[88] E. Moffat, A. Freese, I. Clo¨et, T. Donohoe, L. Gamberg,
W. Melnitchouk, A. Metz, A. Prokudin, and N. Sato,
“Shedding light on shadow generalized parton
distributions,” _Phys. Rev. D_ **108** [no. 3, (2023) 036027,](http://dx.doi.org/10.1103/PhysRevD.108.036027)
`[arXiv:2303.12006 [hep-ph]](http://arxiv.org/abs/2303.12006)` .

[89] J.-W. Qiu and Z. Yu, “Extraction of the Parton
Momentum-Fraction Dependence of Generalized Parton
Distributions from Exclusive Photoproduction,” _[Phys.](http://dx.doi.org/10.1103/PhysRevLett.131.161902)_
_Rev. Lett._ **131** [no. 16, (2023) 161902,](http://dx.doi.org/10.1103/PhysRevLett.131.161902)
`[arXiv:2305.15397 [hep-ph]](http://arxiv.org/abs/2305.15397)` .

[90] **GEANT4** Collaboration, S. Agostinelli _et al._, “Geant4

  - A Simulation Toolkit,” _Nucl.Instrum.Meth.A_ **506**
(2003) 250–303.

[91] M. S. Albergo, G. Kanwar, and P. E. Shanahan,
“Flow-based generative models for Markov chain Monte
Carlo in lattice field theory,” _Phys. Rev. D_ **[100](http://dx.doi.org/10.1103/PhysRevD.100.034515)** no. 3,
[(2019) 034515,](http://dx.doi.org/10.1103/PhysRevD.100.034515) `[arXiv:1904.12072 [hep-lat]](http://arxiv.org/abs/1904.12072)` .

[92] V. Stimper, B. Sch¨olkopf, and J. Miguel
Hernandez-Lobato, “Resampling base distributions of
normalizing flows,” in _Proceedings of The 25th_
_International Conference on Artificial Intelligence and_
_Statistics_, G. Camps-Valls, F. J. R. Ruiz, and I. Valera,
eds., vol. 151 of _Proceedings of Machine Learning_
_Research_, pp. 4915–4936. PMLR, 28–30 mar, 2022.
```
  https:
```

`[//proceedings.mlr.press/v151/stimper22a.html](https://proceedings.mlr.press/v151/stimper22a.html)` .

[93] N. T. Hunt-Smith, W. Melnitchouk, F. Ringer, N. Sato,
A. W. Thomas, and M. J. White, “Accelerating Markov
Chain Monte Carlo sampling with diffusion models,”
_[Comput. Phys. Commun.](http://dx.doi.org/10.1016/j.cpc.2023.109059)_ **296** (2024) 109059,
`[arXiv:2309.01454 [hep-ph]](http://arxiv.org/abs/2309.01454)` .

[94] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,
“Generative adversarial nets,” in _Advances in Neural_
_Information Processing Systems_, Z. Ghahramani,
M. Welling, C. Cortes, N. Lawrence, and K. Weinberger,
eds., vol. 27. Curran Associates, Inc., 2014. `[https:](https://proceedings.neurips.cc/paper_files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Paper.pdf)`
```
  //proceedings.neurips.cc/paper_files/paper/2014/
```

`[file/f033ed80deb0234979a61f95710dbe25-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Paper.pdf)` .

[95] D. Lersch, M. Schram, Z. Dai, K. Rajput, X. Wu,
N. Sato, and J. T. Childers, “Sagips: A scalable
asynchronous generative inverse problem solver,” _[arXiv](http://dx.doi.org/10.48550/arXiv.2407.00051)_
_[preprint arXiv:2407.00051](http://dx.doi.org/10.48550/arXiv.2407.00051)_ (2024) .
`[https://arxiv.org/abs/2407.00051](https://arxiv.org/abs/2407.00051)` .

[96] D. P. Kingma and J. Ba, “Adam: A method for
stochastic optimization,” in _International Conference_
_on Learning Representations_ . 2015.
`[https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)` .


