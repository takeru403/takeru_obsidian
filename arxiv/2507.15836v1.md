## Optimizing Canaries for Privacy Auditing with Metagradient Descent



Matteo Boglioni [*]
ETH Zurich

```
mboglioni@ethz.ch

```


Terrance Liu [*]
Carnegie Mellon University
```
  terrancl@cmu.edu

```


Andrew Ilyas
Stanford Statistics

```
andrewi@stanford.edu

```


Zhiwei Steven Wu
Carnegie Mellon University
```
               zstevenwu@cmu.edu

```

**Abstract**


In this work we study _black-box privacy auditing_, where the goal is to lower bound the privacy parameter
of a differentially private learning algorithm using only the algorithm’s outputs (i.e., final trained model).
For DP-SGD (the most successful method for training differentially private deep learning models), the
canonical approach auditing uses _membership inference_ —an auditor comes with a small set of special “canary” examples, inserts a random subset of them into the training set, and then tries to discern which of
their canaries were included in the training set (typically via a membership inference attack). The auditor’s success rate then provides a lower bound on the privacy parameters of the learning algorithm. Our
main contribution is a method for _optimizing_ the auditor’s canary set to improve privacy auditing, leveraging recent work on metagradient optimization [EIC+25]. Our empirical evaluation demonstrates that by
using such optimized canaries, we can improve empirical lower bounds for differentially private image
classification models by over **2x** in certain instances. Furthermore, we demonstrate that our method is
_transferable_ and _efficient_ : canaries optimized for non-private SGD with a small model architecture remain
effective when auditing larger models trained with DP-SGD.

#### **1 Introduction**


Differential privacy (DP) [DMN+06] offers a rigorous mathematical framework for safeguarding individual data in machine learning. Within this framework, differentially private stochastic gradient descent
(DP-SGD) [ACG+16] has emerged as the standard for training differentially private deep learning models.
Although DP-SGD provides theoretical upper bounds on privacy loss based on its hyperparameters, these
guarantees are likely conservative, which mean they tend to overestimate the privacy leakage in practice

[NHS+23]. In many cases, however, they may not reflect the true privacy leakage that occurs during training. To address this gap, researchers have developed empirical techniques known as privacy audits, which
aim to establish lower bounds on privacy loss. In addition to quantifying real-world leakage, privacy
auditing can also help detect bugs or unintended behaviors in the implementation of private algorithms

[TTS+22].
Providing a lower bound on the privacy leakage of an algorithm typically requires the auditor to guess
some private information ( _membership inference_ ) using a set of examples (also referred to as _canaries_ ). For
example, in one-run auditing procedures [SNJ23; MMC24] (which we discuss further in Section 2.1.2), a
random subset of these canaries is inserted into the training dataset, and once the model is trained, the auditor guesses which of these samples belong to the subset. While recent work has made significant progress
in tightening these bounds through privacy auditing, the strongest results typically assume unrealistic levels of access or control of the private training process (broadly speaking, such settings fall under the term
_white-box_ auditing [NHS+23]). In contrast, this work focuses on a more practical and restrictive _black-box_


   - Equal contribution


1


setting. Here, the auditor can only insert a subset of carefully crafted examples (called the _canaries_ ) into
the training set and observe the model’s output at the _last iterate_ (without access to intermediate model
states or gradient computations). In other words, the goal of black-box DP auditing reduces to performing
membership inference on the canaries based on the _final model output_ .
In this work, we study how to optimize canary samples for the purpose of black-box auditing in differentially private stochastic gradient descent (DP-SGD). Leveraging metagradient descent [EIC+25], we
introduce an approach for crafting canaries specifically tailored for insertion into the training set during DP
auditing. Through empirical evaluation on single-run auditing protocols for differentially private image
classification models, we find that our method consistently yields canaries that surpass standard baselines
by more than a factor of two in certain regimes. Notably, our algorithm is computationally efficient: although it involves running (non-private) SGD on a lightweight ResNet-9 architecture, the resulting canaries
demonstrate strong performance even when deployed in larger models, such as Wide ResNets, trained under DP-SGD. Furthermore, this improvement persists whether DP-SGD is used for end-to-end training or
private finetuning on pretrained networks.


**1.1** **Related Work**


Early works in DP auditing [DWW+18; BGD+18] introduce methods that detect violations of formal DP
guarantees, relying on a large number of runs to identify deviations from expected behavior. These techniques, however, are not directly applicable to the domain of differentially private machine learning, as
they were developed for auditing simpler DP mechanisms. To tackle this issue, Jagielski et al. [JUO20] and
Nasr et al. [NST+21] introduce new approaches based on membership inference attacks (MIA) to empirically determine privacy lower bounds for more complex algorithms like DP-SGD. Membership inference
consists of accurately determining whether a specific sample was part of the model’s training dataset. If
the guesser (i.e. _attacker_ ) can reliably make accurate guesses, it suggests that the model retains information about individual samples observed during training, thereby comprising individuals’ privacy. Hence,
MIA can be used as a practical DP auditing tool in which lower bounds on how much privacy leakage has
occurred can be directly be estimated from the success rate of the attacker.


**One-run auditing.** The first auditing methods for DP-SGD relied on many runs of the algorithm, making
auditing very expensive and often impractical. To remedy this issue, Steinke et al. [SNJ23] and Mahloujifar
et al. [MMC24] reduce the computational cost of auditing by proposing procedures that require only one
training run. Kazmi et al. [KLA+24] and Liu et al. [LBF+25] further study how to incorporate stronger MIA
methods to empirically improve auditing in this one-run setting. Similarly, Keinan et al. [KSL25] study the
theoretical maximum efficacy of one-run auditing.


**Last-iterate auditing.** Our work builds on the aforementioned one-run auditing methods and focuses
specifically on the _last-iterate_ auditing regime, which restricts the auditor’s access to just the final model
weights after the last iteration of DP-SGD. Many works have also studied this regime as well. For example,
Muthu Selva Annamalai [Mut24] investigate whether the analysis on the last iteration can be as tight as
analysis on the sequence of all iterates. Meanwhile, Nasr et al. [NSB+25] propose a heuristic that predicts
empirical lower bounds derived from auditing the last iterate. Other works instead focus on on partial
relaxations of the problem: Cebere et al. [CBP25] assume that the auditor can inject crafted-gradients, and
Muthu Selva Annamalai and De Cristofaro [MD24] audit models that initialized to worst-case parameters.


**Canaries Optimization.** Rather than proposing new auditing procedures, our work studies how to make
existing ones more effective by focusing on optimizing canary sets for privacy auditing. Similarly, Jagielski
et al. [JUO20] develop a method, _CLIPBKD_, that uses singular value decomposition to obtain canaries more
robust to gradient clipping. Nasr et al. [NHS+23] evaluate various procedures to optimize canaries for their
_white-box_ auditing experiments. To better audit differentially private federated learning, Maddock et al.

[MSS23] craft an adversarial sample that is added to a client’s dataset used to send model updates. Finally,
in the context of auditing LLMs, Panda et al. [PTC+25] proposes using tokens sequences not present in


2


the training dataset as canaries, while Meeus et al. [MWZ+25] create canaries with a low-perplexity, indistribution prefixes and high-perplexity suffixes.


**Metagradient computation.** Our work also makes use of recent advancements in computing _metagradi-_
_ents_, gradients of machine learning models’ outputs with respect to their hyperparameters or other quantities decided on prior to training. Prior work on metagradient computation falls under two categories:
_implicit differentiation_ [Ben00; KL17; RFK+19; FAL17; LVD20; CH20; BNL+22] aims to approximate the
metagradient. On one hand, approximating metagradients allows for scalability to large-scale metagradient computation; on the other, this approach loosens correctness guarantees and imposes restrictions on
what learning algorithms can be used. In contrast, _explicit differentiation_ directly computes metagradients
using automatic differentiation. However, these works [MDA15; MS21; FDF+17; LSY18] are limited by their
scalability to larger models and number of hyperparameters and by numerical instability. We leverage recent work by Engstrom et al. [EIC+25], which takes the explicit approach, but addresses the aforementioned
issues by proposing a scalable and memory-efficient method to computing metagradients.

#### **2 Preliminaries**


Informally, differential privacy provides bounds on the extent to which the output distribution of a randomized algorithm _M_ can change when a data point is removed or swapped out.


**Definition 2.1** ((Approximate-) Differential Privacy (DP) [DMN+06]) **.** A randomized algorithm _M_ : _X_ _[N]_ _→_
**R** satisfies ( _ε_, _δ_ ) -differential privacy if for all neighboring datasets _D_, _D_ _[′]_ (i.e., all _D_, _D_ _[′]_ such that _|_ _D_ _[′]_ _\_ _D_ _|_ = 1
and for all outcomes _S_ _⊆_ **R** we have


_P_ ( _M_ ( _D_ ) _∈_ _S_ ) _≤_ _e_ _[ε]_ _P_ ( _M_ ( _D_ _[′]_ ) _∈_ _S_ ) + _δ_


In the context of machine learning, _M_ would be a learning algorithm, and this definition requires the
model to be insensitive to the exclusion of one training data point. In essence, it bounds the change in the
output distribution of the model when trained on neighboring datasets. This implies that the model does
not overly depend on any single sample observed.
Since the seminal work of Dwork et al. [DMN+06], various relaxations of differential privacy have been
proposed. Below, we define _f_ -differential privacy, which we later reference when describing the auditing
procedure proposed by Mahloujifar et al. [MMC24].


**Definition 2.2** ( _f_ -Differential Privacy [DRS22]) **.** A mechanism _M_ is _f_ -DP if for all neighboring datasets
_S_, _S_ _[′]_ and all measurable sets _T_ with _|S△S_ _[′]_ _|_ = 1, we have


Pr [ _M_ ( _S_ ) _∈_ _T_ ] _≤_ _f_ [¯] �Pr [ _M_ ( _S_ _[′]_ ) _∈_ _T_ ] � . (1)


Importantly, _f_ -DP relates to approximate DP in the following way:


**Proposition 1.** A mechanism is ( _ε_, _δ_ ) -DP if it is _f_ -DP with respect to _f_ [¯] ( _x_ ) = _e_ _[ε]_ _x_ + _δ_, where _f_ [¯] ( _x_ ) = 1 _−_ _f_ ( _x_ ) .


While a wide range of methods for adding differentially private guarantees to machine learning algorithms have been proposed over the years, DP-SGD [ACG+16] has been established as one of the de facto
algorithms for training deep neural networks with DP. At a high-level, DP-SGD makes SGD differentially
private by modifying it in the following ways: (1) gradients are clipped to some maximum Euclidean norm
and (2) random noise is added to the clipped gradients prior to each update step. In Algorithm 1, we
present DP-SGD in detail.


**2.1** **Auditing Differential Privacy**


Differentially private algorithms like DP-SGD are accompanied by analysis upper bounding the DP parameters _ε_ and _δ_ . Privacy auditing instead provides an empirical _lower bound_ on these parameters. In this work,
we focus on a specific formulation of privacy audits called _last-iterate, black-box, one-run_ auditing.


3


**Algorithm 1:** Differentially Private Stochastic Gradient Descent (DP-SGD)

**Input:** _x_ _∈X_ _[n]_

**Requires:** Loss function _f_ : **R** _[d]_ _× X →_ **R**
**Parameters:** Number of iterations _ℓ_, learning rate _η_, clipping threshold _c_ _>_ 0, noise multiplier
_σ_ _>_ 0, sampling probability _q_ _∈_ ( 0, 1 ]
**1** Initialize _w_ 0 _∈_ **R** _[d]_ ;
**2 for** _t_ = 1, . . ., _ℓ_ **do**
**3** Sample _S_ _[t]_ _⊆_ [ _n_ ] where each _i_ _∈_ [ _n_ ] is included independently with probability _q_ ;
**4** Compute _g_ _i_ _[t]_ [=] _[ ∇]_ _w_ _[t]_ _[−]_ [1] _[ f]_ [ (] _[w]_ _[t]_ _[−]_ [1] [,] _[ x]_ _[i]_ [)] _[ ∈]_ **[R]** _[d]_ [ for all] _[ i]_ _[ ∈]_ _[S]_ _[t]_ [;]



**5** Clip ˜ _g_ _i_ _[t]_ [=] [ min] �1, _∥_ _gc_ _i_ ~~_[t]_~~ _[∥]_ [2]




_·_ _g_ _i_ _[t]_ _[∈]_ **[R]** _[d]_ [ for all] _[ i]_ _[ ∈]_ _[S]_ _[t]_ [;]
�



**6** Sample _ξ_ _[t]_ _∈_ **R** _[d]_ from _N_ ( 0, _σ_ [2] _c_ [2] _I_ ) ;
**7** Sum ˜ _g_ _[t]_ = _ξ_ _[t]_ + ∑ _i_ _∈_ _S_ _t_ ˜ _g_ _i_ _[t]_ _[∈]_ **[R]** _[d]_ [;]
**8** Update _w_ _[t]_ = _w_ _[t]_ _[−]_ [1] _−_ _η_ _·_ ˜ _g_ _[t]_ _∈_ **R** _[d]_ ;

**Output:** _w_ [0], _w_ [1], . . ., _w_ _[ℓ]_


**Algorithm 2:** Black-box Auditing - Single Run [SNJ23]

**Input:** probability threshold _τ_, privacy parameter _δ_, training algorithm _A_, dataset _D_, set of _m_
canaries _C_ = _{_ _c_ 1, . . ., _c_ _m_ _}_
**Requires:** scoring function `score`
**Parameters:** number of positive and negative guesses _k_ + and _k_ _−_
**1** Randomly split canaries _C_ into two equally-sized sets _C_ IN and _C_ OUT


1 if _c_ _i_ _∈_ _C_ IN

**2** Let _S_ = _{_ _s_ _i_ _}_ _i_ _[m]_ = 1 [, where] _[ s]_ _[i]_ [ =] � _−_ 1 if _c_ _i_ _∈_ _C_ OUT

**3** Train model _w_ _←A_ ( _D_ _∪_ _C_ IN )
**4** Compute vector of scores _Y_ = _{_ `score` ( _w_, _c_ _i_ ) _}_ _i_ _[m]_ = 1
**5** Sort scores in ascending order _Y_ _[′]_ _←_ `sort` ( _Y_ )
**6** Construct vector of guesses _T_ = _{_ _t_ _i_ _}_ _i_ _[m]_ = 1 [, where]



1 if _Y_ _i_ is among the top _k_ + scores in _Y_ (i.e., _Y_ _i_ _≥_ _Y_ _m_ _[′]_ _−_ _k_ + [)] `[ //][ guess]` _[ c]_ _[i]_ _[ ∈]_ _[C]_ [IN]

_t_ _i_ =  _−_ 1 if _Y_ _i_ is among the bottom _k_ _−_ scores in _Y_ (i.e., _Y_ _i_ _≤_ _Y_ _k_ _[′]_ _−_ [)] `[ //][ guess]` _[ c]_ _[i]_ _[ ∈]_ _[C]_ [OUT]

0 otherwise `// abstain`

**7** Compute empirical epsilon ˜ _ε_ (i.e., find the largest ˜ _ε_ such that _S_, _T_, _τ_, and _δ_ satisfy Theorem 1)



_t_ _i_ =







**Output:** ˜ _ε_


**2.1.1** **Last-iterate black-box auditing**


Our work focuses on _last-iterate black-box_ auditing, where the auditor can only insert samples (i.e., canaries)
into the training set and can only access the resulting model at the final training iteration. We note that, in
contrast, previous works have also studied white-box settings. While the exact assumptions made in this
setting can vary [NST+21; NHS+23; SNJ23; KM25], it can be characterized as having fewer restrictions (e.g.,
access to intermediate training iterations or the ability to inject and modify gradients). While auditing in
white-box settings generally leads to higher lower bound estimates due to the strength of the auditor, its
assumptions are often far less realistic than those made in black-box auditing.


**2.1.2** **One-run auditing**


Early works [JUO20; TTS+22; NHS+23] design privacy auditing “attacks” that align with the definition
of DP, which bounds the difference in outputs on neighboring datasets that differ by one data sample.
Specifically, these audits detect the presence (or absence) of an individual sample over hundreds—if not,


4


**Algorithm 3:** Black-box Auditing - Single Run [MMC24]

**Input:** privacy parameter _δ_, training algorithm _A_, dataset _D_, set of _m_ canaries _C_ = _{_ _c_ 1, . . ., _c_ _m_ _}_
**Requires:** scoring function `score`
**Parameters:** number of guesses _k_
**1** Randomly split canaries _C_ into two equally-sized sets _C_ IN and _C_ OUT
**2** Create disjoint canary sets _E_ = _{_ _e_ _i_ _}_ _i_ _[m]_ = [/2] 1 [by randomly pairing canaries from] _[ C]_ [IN] [ and] _[ C]_ [OUT] [ such that]
_e_ _i_ = ( _c_ _i_,1, _c_ _i_,2 ) for _c_ _i_,1 _∈_ _C_ IN and _c_ _i_,2 _∈_ _C_ OUT (each canary _c_ _∈_ _C_ appears in **exactly** one set _e_ _i_ )
**3** Train model _w_ _←A_ ( _D_ _∪_ _C_ IN )
**4** Compute vector of scores _Y_ = _{|_ `score` ( _w_, _c_ _i_,1 ) _−_ `score` ( _w_, _c_ _i_,2 ) _|}_ _i_ _[m]_ = [/2] 1
**5** Sort scores in ascending order _Y_ _[′]_ _←_ `sort` ( _Y_ )
**6** Construct vector of guesses _T_ = _{_ _t_ _i_ _}_ _i_ _[m]_ = [/2] 1 [, where]



_t_ _i_ =







1 if _Y_ _i_ is among the top _k_ values in _Y_ (i.e., _Y_ _i_ _≥_ _Y_ _m_ _[′]_ _−_ _k_ [)]
and `score` ( _w_, _c_ _i_,1 ) _>_ `score` ( _w_, _c_ _i_,2 ) `// guess` _c_ _i_,1 _∈_ _C_ IN
_−_ 1 if _Y_ _i_ is among the top _k_ values in _Y_ (i.e., _Y_ _i_ _≥_ _Y_ _m_ _[′]_ _−_ _k_ [)]
and `score` ( _w_, _c_ _i_,1 ) _≤_ `score` ( _w_, _c_ _i_,2 ) `// guess` _c_ _i_,2 _∈_ _C_ IN
0 otherwise `// abstain`



**7** Let number of correct guesses _k_ _[′]_ = ∑ _i_ _[m]_ = [/2] 1 [1] _[{]_ _[t]_ _[i]_ [ =] [ 1] _[}]_
**8** Compute empirical epsilon ˜ _ε_ (i.e., find the largest ˜ _ε_ whose corresponding _f_ -DP function _f_ passes
Algorithm 4 for _m_, _k_, _k_ _[′]_ _τ_, and _δ_ .)
**Output:** ˜ _ε_


**Algorithm 4:** Upper bound probability of making correct guesses [MMC24]



**Input:** probability threshold _τ_, functions _f_ and _f_ _[−]_ [1], number of guesses _k_, number of correct guesses
_k_ _[′]_, number of samples _m_, alphabet size _s_
**1** _∀_ 0 _<_ _i_ _<_ _k_ _[′]_ set _h_ [ _i_ ] = 0, and _r_ [ _i_ ] = 0
**2** Set _r_ [ _k_ _[′]_ ] = _τ_ _·_ _[c]_



_m_
**3** Set _h_ [ _k_ _[′]_ ] = _τ_ _·_ _[c]_ _[′]_ _[−]_ _[c]_



_m_
**4 for** _i_ _∈_ [ _k_ _[′]_ _−_ 1, . . ., 0 ] **do**
**5** _h_ [ _i_ ] = ( _s_ _−_ 1 ) _f_ _[−]_ [1] ( _r_ [ _i_ + 1 ])
**6** _r_ [ _i_ ] = _r_ [ _i_ + 1 ] + _k_ _−_ _i_ _i_ _[·]_ [ (] _[h]_ [[] _[i]_ []] _[ −]_ _[h]_ [[] _[i]_ [ +] [ 1] [])]



**7 if** _r_ [ 0 ] + _h_ [ 0 ] _≥_ _m_ _[k]_ **[then]**

**8** Return True (probability of _k_ _[′]_ correct guesses (out of _k_ ) is less than _τ_ )
**9 else**
**10** Return False (probability of having _k_ _[′]_ correct guesses (out of _k_ ) could be more than _τ_ )


thousands—of runs of DP-SGD. The auditing procedure then gives a lower bound on _ε_ based on the true
and false positive rates of the membership inference attacks.
While effective, these multi-run auditing procedures are computational expensive. Consequently, Steinke
et al. [SNJ23] propose an alternative procedure that requires only _one_ training run. Rather than inserting
and inferring membership on a single example, their one-run strategy instead inserts multiple canary examples and obtains a lower bound based on how well an attacker can guess whether some canary was used
in training. While one-run auditing can sacrifice bound tightness, its ability to audit without multiple runs
of DP-SGD make it much more efficient and therefore, practical for larger models.
In our work, we consider two primary auditing procedures:


**Steinke et al. [SNJ23].** Steinke et al. [SNJ23] introduce the concept of privacy auditing using one training
run. Given some set of canaries _C_, samples are randomly sampled from _C_ with probability [1] 2 [and inserted]


5


into the training set. Once the model is trained, the auditor guesses which samples in _C_ were or were
not included in the training set. The auditor can make any number of guesses or abstain. We present this
procedure in Algorithm 2. The final lower bound on _ε_ is determined using Theorem 1, which is based on
the total number of canaries, the number of guesses, and the number of correct guesses.


**Theorem 1** (Analytic result for approximate DP [SNJ23]) **.** Suppose _A_ : _{−_ 1, 1 _}_ _[m]_ _→{−_ 1, 0, 1 _}_ _[m]_ satisfy
( _ε_, _δ_ ) -DP. Let _S_ _∈{−_ 1, 1 _}_ _[m]_ be uniformly random and _T_ = _A_ ( _S_ ) . Suppose **P** [ _∥_ _T_ _∥_ 1 _≤_ _r_ ] = 1. Then, for all
_v_ _∈_ **R**,



�



,
�



_f_ ( _v_ _−_ _i_ ) _−_ _f_ ( _v_ )
� _i_



**P** _S_ _←{−_ 1,1 _}_ _m_

_T_ _←_ _M_ ( _S_ )



_m_
#### ∑ max { 0, T i · S i } ≥ v
� _i_ = 1



_≤_ _f_ ( _v_ ) + 2 _mδ_ _·_ max
_i_ _∈{_ 1,..., _m_ _}_



where


˜
_f_ ( _v_ ) : = **P** _W_ ˜ _←_ Binomial ( _r_, _e_ ~~_[ε]_~~ _e_ + _[ε]_ 1 [)] � _W_ _≥_ _v_ � .


At a very high level, _A_ is DP-SGD, which takes in as input some set of _m_ canaries that are labeled
( _S_ _∈{−_ 1, 1 _}_ _[m]_ ) as being included or excluded from the training set. The auditor uses the output of DP-SGD
to produce a vector of guesses _T_ _∈{−_ 1, 0, 1 _}_ _[m]_ for the _m_ canaries. Theorem 1 bounds the probability of
making at least _v_ correct guesses (∑ _i_ _[m]_ = 1 [max] _[{]_ [0,] _[ T]_ _[i]_ _[ ·]_ _[ S]_ _[i]_ _[} ≥]_ _[v]_ [, where] _[ T]_ _[i]_ _[ ·]_ _[ S]_ _[i]_ [ =] [ 1 if the guess is correct). More]
informally, this theorem bounds the success rate (number of correct guesses) of the auditor assuming the
parameter _ε_ . Practically speaking, one runs binary search [SNJ23, Appendix D] to estimate the largest _ε_
such that Theorem 1 still holds.


**Mahloujifar et al. [MMC24].** Mahloujifar et al. [MMC24] propose an alternative auditing procedure that
they empirically show achieves tighter privacy estimates in the white-box setting. In their guessing game,
the set of canaries _C_ is randomly partitioned in disjoint sets. One canary is sampled from each set and
inserted into the training set. Again, once the model is trained with DP-SGD, the auditor must make
guesses. Unlike in Steinke et al. [SNJ23], however, the auditor must guess which canary out of each set was
included in training. We present this procedure in Algorithm 3 for canary sets of size 2.
Similar to Steinke et al. [SNJ23], the final lower bound on _ε_ is determined based on the total number of
canary sets, the number of guesses, and the number of correct guesses. At a high level, Mahloujifar et al.

[MMC24] first construct a set of candidate values for _ε_ and a corresponding _f_ -DP function for each. Using
Algorithm 4, they then run a hypothesis test, with probability threshold _τ_, for the number of correct guesses
(i.e., output of Algorithm 4) occurring given function _f_ . The final empirical lower bound is the maximum _ε_
among those corresponding to the functions _f_ that pass Algorithm 4.


**Scoring function.** Finally, to determine membership for either procedure, the auditor must first choose
some `score` function _s_ ( _·_ ) from the training process. In the black-box setting for image classification models,
one natural choice for _s_ ( _·_ ) is to use negative cross-entropy loss [SNJ23]. When _s_ ( _w_, _x_ ) is large (i.e., crossentropy loss is small) for some canary _x_ and model _w_, the auditor guesses that _x_ was included in training,
and vice-versa. In Section 4, we provide more details about how we use the score function for Algorithms 2
and 3.

#### **3 Canary Optimization with Metagradient Descent**


For a fixed black-box auditing algorithm `BBaudit` : ( _τ_, _δ_, _A_, _D_, _C_ ) _→_ � _ε_ (e.g., Algorithm 2 or 3), the main
degree of freedom available to the auditor is the choice of canary set _C_ . Typically, one chooses _C_ to be a
random subset of the training dataset _D_, or a random set of mislabled examples [SNJ23; MMC24]. A natural
question to ask is whether such choices are (near-)optimal; in other words, _can we significantly improve the_
_efficacy of a given auditing algorithm by carefully designing the canary set C_ ?


6


In this section, we describe an optimization-based approach to choosing the canary set. At a high level,
our goal is to solve an optimization problem of the form


max `BBaudit` ( _τ_, _δ_, _A_, _D_, _C_ ), (2)
_C_


where `BBaudit` is the (fixed) differential privacy auditing algorithm, _τ_ and _δ_ are the privacy parameters,
_A_ is the learning algorithm (e.g., DP-SGD), _D_ is the dataset, and _C_ is the set of canary samples. The
high-dimensional nature of this problem (e.g., for CIFAR-10, _C_ _∈_ **R** _[m]_ _[×]_ [32] _[×]_ [32] _[×]_ [3] ) makes it impossible to
exhaustively search over all possible canary sets _C_ .
Instead, the main idea behind our approach is to use _gradient descent_ to optimize the canary set _C_ . To do
so, we first design a surrogate objective function to `audit` by leveraging the connection between membership inference and differential privacy auditing. We then use recent advances in _metagradient_ computation

[EIC+25] to optimize this surrogate objective with respect to the canary set _C_ .


**Key primitive: metagradient descent.** A metagradient is a gradient taken _through_ the process of training
a machine learning model [MDA15; Dom12; Ben00; BP14]. Specifically, given a learning algorithm _A_, a
(continuous) design parameter _z_ (e.g., learning rate, weight decay, data weights, etc.), and a loss function
_ϕ_, the metagradient _∇_ _z_ _ϕ_ ( _A_ ( _z_ )) is the gradient of the final loss _ϕ_ with respect to the design parameter _z_
(see Figure 1). For very small-scale learning algorithms (e.g., training shallow neural networks), one can
compute metagradients by backpropagating through the entire model training process.
While this approach does not scale directly to larger-scale training routines, the recent work of Engstrom et al. [EIC+25] proposes a method to compute metagradients efficiently and at scale. The method,
called `REPLAY`, enables gradient-based optimization of data importance weights, training hyperparameters,
and—most relevant to our setting—poisonous training data that hurts overall model accuracy. To tackle
the latter setting, Engstrom et al. [EIC+25] show how to compute metagradients of a model’s final test loss
with respect to the pixels in its training data. Leveraging this method, we will assume that we can efficiently compute metagradients of any differentiable loss function with respect to the pixels of any number
of training data points.
#### _z w w_
### ϕ ( )


Training setup Trained model Observed behavior


Figure 1: An illustration of the metagradient. We embed the canaries into a continuous metaparameter
_z_ _∈_ **R** _[m]_ _[×]_ _[H]_ _[×]_ _[W]_ _[×]_ _[C]_ with one coordinate per training data pixel. All aspects of the learning process other than
_z_ —the base training data, optimizer hyperparameters, etc.—are baked into the learning algorithm _A_ . The
metaparameter thus defines a model _w_ = _A_ ( _z_ ), which we use to compute an output metric _ϕ_ ( _w_ ) . The
metagradient _∇_ _z_ _ϕ_ ( _A_ ( _z_ )) is the gradient of the metric with respect to the metaparameter _z_ .


**Surrogate objective function.** Even with the ability to compute metagradients efficiently, the optimization problem in (2) is still challenging to solve in the black-box setting. First, the objective function `BBaudit`
has explicit non-differentiable components (e.g., thresholding). Second, taking the metagradient requires
more fine-grained access to the model training process than simply observing the final model outputs.
To address both these challenges, we design a surrogate objective function that approximates the original objective (2), inspired by the connection between black-box privacy auditing and membership inference.
In particular, inspecting Algorithms 2 and 3, we observe that in both algorithms, we randomly split the canary set into two sets _C_ IN and _C_ OUT ; trains a model on _D_ _∪_ _C_ IN ; and runs a membership inference attack to


7


**Algorithm 5:** Metagradient Canary Optimization

**Input:** dataset _D_
**Requires:** training algorithm _A_, loss function _L_
**Parameters:** number of canaries _m_, number of meta-iterations _N_
**1** Initialize canaries _C_ 0 = _{_ _c_ 1, . . ., _c_ _m_ _}_
**2 for** _t_ _←_ 0 **to** _N_ _−_ 1 **do**
**3** Randomly split _C_ _t_ into two equally-sized sets: _C_ IN, _t_ and _C_ OUT, _t_
**4** Train model: _w_ _t_ _←A_ ( _D_ _∪_ _C_ IN, _t_ )
**5** Compute loss gap _ϕ_ ( _w_ _t_ ) = _L_ ( _w_ _t_, _C_ IN, _t_ ) _−L_ ( _w_ _t_, _C_ OUT, _t_ )
**6** Compute gradient w.r.t. canaries: _∇_ _C_ _t_ _←_ `REPLAY` ( _w_ _t_, _ϕ_ ( _θ_ _t_ ))
**7** Update canaries: _C_ _i_ + 1 _←_ update ( _C_ _i_, _∇_ _C_ _i_ )
**Output:** optimized canaries _C_ _N_


distinguish between samples in _C_ IN and _C_ OUT . Intuitively, a good canary sample _z_ _i_ should thus satisfy the
following properties:


 - **Memorizability** : if _z_ _i_ _∈_ _C_ _in_, the model should have _low_ loss on _z_ _i_ ;


 - **Non-generalizability** : if _z_ _i_ _∈_ _C_ _out_, the model should have _high_ loss on _z_ _i_ .


These properties motivate the following simple surrogate objective function:


_m_
#### ϕ ( w ) = ∑ ( 1 { z i ∈ C in } − 1 { z i ∈ C out } ) · L ( w, z i ), (3)

_i_ = 1


where _L_ is the training loss (i.e., cross-entropy) and **1** _{·}_ is the indicator function. [1] Finally, to optimize
this objective in the black-box setting, we consider a “standard” (i.e., not differentially private) training
algorithm _A_, and then transfer the resulting canaries to whatever learning algorithm we are auditing.


**Optimizing canaries with (meta-)gradients.** Our final optimization process (given in more detail in Algorithm 5) proceeds in _T_ _>_ 1 _metasteps_ . Let _D_ be the set of non-canaries (e.g., the CIFAR-10 dataset) and
_C_ be the set of canaries (i.e., metaparameters _z_ ) we are optimizing. During each _metastep t_, we randomly
partition the canaries _C_ into two sets _C_ IN, _t_ and _C_ OUT, _t_, and randomly sample a model initialization and
data ordering which define a learning algorithm _A_ . After training a model _w_ = _A_ ( _z_ ), we take a gradient step to minimize the objective (equation 3) with respect to the canary set _C_ . By repeating this process
several times (essentially running stochastic gradient descent across random seeds and random data orderings partitionings of the canary set), we obtain a set of canary examples that are robustly memorizable and
non-generalizable.

#### **4 Experimental Setup**


We present the details of our empirical evaluation.


**Additional auditing procedure details** As implemented in Mahloujifar et al. [MMC24], we align Algorithms 2 and 3 by fixing the canary set size to 2 so that half of _C_ is included in training for both auditing
setups. When running Algorithm 2, we split _C_ randomly in half (instead of sampling with probability half)
so that the set of _r_ non-auditing examples are the same for both auditing procedures. In addition, we use
negative cross-entropy loss as the scoring function _s_ ( _·_ ) for both algorithms. In more detail,


1 We note that in this case, _ϕ_ depends on the model weights _w_ but also has a direct dependence on the canary set (i.e., the metaparameters _z_ ), making Figure 1 a slight over-simplification. In practice, we can still use the law of total derivative to compute the
gradient of _ϕ_ with respect to _z_, since _∇_ _z_ _ϕ_ ( _z_, _A_ ( _z_ )) = _∂_ _[∂][ϕ]_ _[· ∇]_ _[z]_ _[A]_ [(] _[z]_ [) +] _[∂]_ _∂_ _[ϕ]_ [.]



_∂_ _[∂]_ _w_ _[ϕ]_ _[· ∇]_ _[z]_ _[A]_ [(] _[z]_ [) +] _[∂]_ _∂_ _[ϕ]_ _z_



_∂z_ [.]



8


Table 1: Hyperparameters for training Wide ResNet 16-4 models using DP-SGD with _ε_ = 8.0 and _δ_ = 10 _[−]_ [5] .
For training from scratch, we use the optimal hyperparameters reported in De et al. [DBH+22]. For DP
finetuning, we decrease the noise multiplier to achieve slightly higher train and test accuracy.


Hyperparameter DP Training DP Finetuning


Augmentation multiplicity 16 16
Batch size 4096 4096
Clipping norm 1.0 1.0
Learning rate 4.0 4.0
Noise multiplier 3.0 1.75


 - **[Steinke et al. [SNJ23], Algorithm 2** ] We sort the canaries _x_ _∈_ _C_ by _s_ ( _x_ ) and take the top _k_ + canaries
in the sorted list as positive guesses and bottom _k_ _−_ as negative guesses.


 - **[Mahloujifar et al. [MMC24], Algorithm 3** ] We score canaries in each pair and predict the one with
the higher score to have been included in training. We then score each pairing by taking the absolute
difference scores _s_ ( _·_ ) between the canaries in each set and ranking the pairs by the difference. We take
the top _k_ sets as our guesses.


**Audited models.** Following prior work [NHS+23; SNJ23; MMC24], we audit Wide ResNet models [ZK16]
trained on CIFAR-10 [KH+09] with DP-SGD. We use the Wide ResNet 16-4 architecture proposed by De et
al. [DBH+22], which they modify for DP training, and train the model using the `JAX-Privacy` package

[BBC+25].
To audit the models, we use canary sets of size _m_ = 1000. To remain consistent with Steinke et al. [SNJ23]
and Mahloujifar et al. [MMC24], where _C_ is sampled from the training set, we have in total _r_ = 49000 noncanaries training images for CIFAR-10. Thus, in total, _n_ = 49500 images are used in training for any given

run.
We run DP-SGD on models both initialized randomly and pretrained nonprivately (i.e., DP-finetuning).
For DP-finetuning experiments, we use CINIC-10 [DCA+18], which combines images from CIFAR-10 with
images from ImageNet [DDS+09] that correspond to the classes in CIFAR-10. For our pretraining dataset,
we use CINIC-10 with the CIFAR-10 images removed. We report the hyperparameters used for DP-SGD in
Table 1.


**Metagradient canary optimization.** Following Engstrom et al. [EIC+25], we optimize the canary samples
by training a ResNet-9 model (i.e., _w_ in Algorithm 5), allowing us to optimize _C_ efficiently. As demonstrated
in Section 5, despite using a relatively compact model, our metagradient canaries are effective for much
larger model architectures (i.e., Wide ResNets).


**Baselines.** We compare our method against canaries randomly sampled from the training set [SNJ23;
MMC24], as well as canaries that have been mislabeled [NHS+23; SNJ23].

#### **5 Results**


We first verify that our metagradient canaries work correctly when auditing models trained with _non-private_
SGD. In Figure 2, we plot the empirical epsilon estimated by the auditing procedures introduced in Steinke
et al. [SNJ23] and Mahloujifar et al. [MMC24] against the number of steps that the Wide ResNet 16-4 model
is trained for. We observe that even when applied on different model architectures (i.e., transferring from
ResNet-9 to WRN 16-4), our metagradient canaries perform well. Interestingly, once training continues
past the point where the model achieves nearly perfect training accuracy (at around 4000 training steps),
membership inference on mislabeled images becomes easier, resulting in a higher empirical epsilon for
those canaries. However, for the regime relevant to DP training in which memorization does not occur to


9


Table 2: We audit a Wide ResNet 16-4 model that has been trained with DP-SGD ( _ε_ = 8.0, _δ_ = 10 _[−]_ [5] ) on
CIFAR-10 with the auditing parameters: _n_ = 49500, _m_ = 1000, and _r_ = 49000. We present results for
models **(a)** initialized from scratch and **(b)** pretrained on CINIC-10 (with CIFAR-10 images removed). We
report the average and median empirical epsilon over 5 runs for auditing procedures introduced by **(1)**
Steinke et al. [SNJ23] and **(2)** Mahloujifar et al. [MMC24].


(a) DP Training (b) DP Finetuning
**Audit Procedure** **Canary Type** **Avg.** **Med.** **Avg.** **Med.**


random 0.204 0.001 0.218 0.145
(1) Steinke et al. [SNJ23] mislabeled 0.187 0.221 0.271 0.054
metagradient ( _ours_ ) **0.408** **0.284** **0.362** **0.290**


random 0.150 0.000 0.121 0.095
(2) Mahloujifar et al. [MMC24] mislabeled 0.128 0.047 0.384 0.320
metagradient ( _ours_ ) **0.320** **0.368** **0.489** **0.496**



















(a) Steinke et al. [SNJ23]



(b) Mahloujifar et al. [MMC24]



Figure 2: We evaluate the effectiveness of our metagradient canaries for the purpose of auditing _non-private_
SGD. We train a Wide ResNet 16-4 model on CIFAR-10 for 10 _k_ steps with each canary type, plotting the
empirical epsilon when auditing the model at every 100 steps with the auditing procedures introduced by
**(a)** Steinke et al. [SNJ23] and **(b)** Mahloujifar et al. [MMC24]. We take an average over 5 runs and plot an
error band to denote _±_ 1 standard deviation. For reference, we plot the training error of the model trained
on our metagradient canaries (note that the training accuracy is approximately the same, regardless of
choice of canary).


the same extent (e.g., at _ε_ = 8.0, WRN 16-4 only achieves a training accuracy of _≈_ 80%), our proposed
canaries significantly outperform the baselines.
Having verified that our metagradient canaries work properly for auditing SGD, we now evaluate their
effectiveness in auditing _DP-SGD_ . In Table 2, we presents our main results for both DP training (i.e., training
from scratch) and DP finetuning (i.e., first pretraining non-privately). We find that our method performs
the best, exceeding the empirical epsilon of baseline canaries by up to 2x for DP training, regardless of the
auditing procedure used. Moreover, even when evaluated on DP finetuning, our canaries outperform the
baselines, despite our method (Algorithm 5) not using CINIC-10 for pretraining _w_ at each metagradient
step.


10


#### **6 Conclusion**

We propose an efficient method for canary optimization that leverages metagradient descent. Optimizing
for an objective tailored towards privacy auditing, our canaries significantly outperform standard canaries,
which are sampled from the training dataset. Specifically, we show that despite being optimized for nonprivate SGD on a small ResNet model, our canaries work better on larger Wide ResNets for both DPtraining and DP-finetuning. In future work, we hope to apply our metagradient method to optimizing
canaries for multi-run auditing procedures.

#### **References**


[ACG+16] Martin Abadi et al. “Deep learning with differential privacy”. In: _Proceedings of the 2016 ACM_
_SIGSAC conference on computer and communications security_ . 2016, pp. 308–318.


[BBC+25] Borja Balle et al. _JAX-Privacy: Algorithms for Privacy-Preserving Machine Learning in JAX_ . Version 0.4.0. 2025. URL : `[http://github.com/google-deepmind/jax_privacy](http://github.com/google-deepmind/jax_privacy)` .


[Ben00] Yoshua Bengio. “Gradient-based optimization of hyperparameters”. In: _Neural computation_
12.8 (2000), pp. 1889–1900.


[BGD+18] Benjamin Bichsel et al. “DP-Finder: Finding Differential Privacy Violations by Sampling and
Optimization”. In: _Proceedings of the 2018 ACM SIGSAC Conference on Computer and Commu-_
_nications Security_ . CCS ’18. Toronto, Canada: Association for Computing Machinery, 2018,
pp. 508–524. ISBN : 9781450356930. DOI : `[10.1145/3243734.3243863](https://doi.org/10.1145/3243734.3243863)` . URL : `[https://doi.](https://doi.org/10.1145/3243734.3243863)`
`[org/10.1145/3243734.3243863](https://doi.org/10.1145/3243734.3243863)` .


[BNL+22] Juhan Bae et al. “If influence functions are the answer, then what is the question?” In: _Advances_
_in Neural Information Processing Systems_ 35 (2022), pp. 17953–17967.


[BP14] Atilim Gunes Baydin and Barak A Pearlmutter. “Automatic differentiation of algorithms for
machine learning”. In: _arXiv preprint arXiv:1404.7456_ (2014).


[CBP25] Tudor Ioan Cebere, Aur´elien Bellet, and Nicolas Papernot. “Tighter Privacy Auditing of DPSGD in the Hidden State Threat Model”. In: _The Thirteenth International Conference on Learning_
_Representations_ . 2025. URL : `[https://openreview.net/forum?id=xzKFnsJIXL](https://openreview.net/forum?id=xzKFnsJIXL)` .


[CH20] Xiangning Chen and Cho-Jui Hsieh. “Stabilizing differentiable architecture search via perturbationbased regularization”. In: _International conference on machine learning_ . PMLR. 2020, pp. 1554–
1565.


[DBH+22] Soham De et al. “Unlocking high-accuracy differentially private image classification through
scale”. In: _arXiv preprint arXiv:2204.13650_ (2022).


[DCA+18] Luke N Darlow et al. “Cinic-10 is not imagenet or cifar-10”. In: _arXiv preprint arXiv:1810.03505_
(2018).


[DDS+09] Jia Deng et al. “Imagenet: A large-scale hierarchical image database”. In: _2009 IEEE conference_
_on computer vision and pattern recognition_ . Ieee. 2009, pp. 248–255.


[DMN+06] Cynthia Dwork et al. “Calibrating noise to sensitivity in private data analysis”. In: _Theory of_
_cryptography conference_ . Springer. 2006, pp. 265–284.


[Dom12] Justin Domke. “Generic methods for optimization-based modeling”. In: _Artificial Intelligence_
_and Statistics_ . PMLR. 2012, pp. 318–326.


[DRS22] Jinshuo Dong, Aaron Roth, and Weijie J. Su. “Gaussian differential privacy”. In: _Journal of the_
_Royal Statistical Society: Series B (Statistical Methodology)_ 84.1 (2022), pp. 3–37. eprint: `[https:](https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12454)`
`[//rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12454](https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12454)` .


[DWW+18] Zeyu Ding et al. “Detecting violations of differential privacy”. In: _Proceedings of the 2018 ACM_
_SIGSAC Conference on Computer and Communications Security_ . 2018, pp. 475–489.


[EIC+25] Logan Engstrom et al. “Optimizing ml training with metagradient descent”. In: _arXiv preprint_
_arXiv:2503.13751_ (2025).


11


[FAL17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. “Model-agnostic meta-learning for fast adaptation of deep networks”. In: _International conference on machine learning_ . PMLR. 2017, pp. 1126–
1135.


[FDF+17] Luca Franceschi et al. “Forward and reverse gradient-based hyperparameter optimization”.
In: _International conference on machine learning_ . PMLR. 2017, pp. 1165–1173.


[JUO20] Matthew Jagielski, Jonathan Ullman, and Alina Oprea. “Auditing differentially private machine learning: How private is private sgd?” In: _Advances in Neural Information Processing Sys-_
_tems_ 33 (2020), pp. 22205–22216.


[KH+09] Alex Krizhevsky, Geoffrey Hinton, et al. “Learning multiple layers of features from tiny images”. In: _University of Toronto_ (2009).


[KL17] Pang Wei Koh and Percy Liang. “Understanding black-box predictions via influence functions”. In: _International conference on machine learning_ . PMLR. 2017, pp. 1885–1894.


[KLA+24] Mishaal Kazmi et al. “Panoramia: Privacy auditing of machine learning models without retraining”. In: _Advances in Neural Information Processing Systems_ 37 (2024), pp. 57262–57300.


[KM25] Antti Koskela and Jafar Aco Mohammadi. “Auditing Differential Privacy Guarantees Using Density Estimation”. In: _2025 IEEE Conference on Secure and Trustworthy Machine Learning_
_(SaTML)_ . IEEE. 2025, pp. 1007–1026.


[KSL25] Amit Keinan, Moshe Shenfeld, and Katrina Ligett. “How Well Can Differential Privacy Be
Audited in One Run?” In: _arXiv preprint arXiv:2503.07199_ (2025).


[LBF+25] Terrance Liu et al. “Enhancing One-run Privacy Auditing with Quantile Regression-Based
Membership Inference”. In: _arXiv preprint arXiv:2506.15349_ (2025).


[LSY18] Hanxiao Liu, Karen Simonyan, and Yiming Yang. “Darts: Differentiable architecture search”.
In: _arXiv preprint arXiv:1806.09055_ (2018).


[LVD20] Jonathan Lorraine, Paul Vicol, and David Duvenaud. “Optimizing millions of hyperparameters by implicit differentiation”. In: _International conference on artificial intelligence and statistics_ .
PMLR. 2020, pp. 1540–1552.


[MD24] Meenatchi Sundaram Muthu Selva Annamalai and Emiliano De Cristofaro. “Nearly tight
black-box auditing of differentially private machine learning”. In: _Advances in Neural Infor-_
_mation Processing Systems_ 37 (2024), pp. 131482–131502.


[MDA15] Dougal Maclaurin, David Duvenaud, and Ryan Adams. “Gradient-based hyperparameter optimization through reversible learning”. In: _International conference on machine learning_ . PMLR.
2015, pp. 2113–2122.


[MMC24] Saeed Mahloujifar, Luca Melis, and Kamalika Chaudhuri. “Auditing _f_ -Differential Privacy in
One Run”. In: _arXiv preprint arXiv:2410.22235_ (2024).


[MS21] Paul Micaelli and Amos J Storkey. “Gradient-based hyperparameter optimization over long
horizons”. In: _Advances in Neural Information Processing Systems_ 34 (2021), pp. 10798–10809.


[MSS23] Samuel Maddock, Alexandre Sablayrolles, and Pierre Stock. “CANIFE: Crafting Canaries for
Empirical Privacy Measurement in Federated Learning”. In: _ICLR_ . 2023.


[Mut24] Meenatchi Sundaram Muthu Selva Annamalai. “It’s Our Loss: No Privacy Amplification for
Hidden State DP-SGD With Non-Convex Loss”. In: _Proceedings of the 2024 Workshop on Ar-_
_tificial Intelligence and Security_ . AISec ’24. Salt Lake City, UT, USA: Association for Computing Machinery, 2024, pp. 24–30. ISBN : 9798400712289. DOI : `[10.1145/3689932.3694767](https://doi.org/10.1145/3689932.3694767)` . URL :
`[https://doi.org/10.1145/3689932.3694767](https://doi.org/10.1145/3689932.3694767)` .


[MWZ+25] Matthieu Meeus et al. “The Canary’s Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text”. In: _Forty-second International Conference on Machine Learning_ . 2025.


[NHS+23] Milad Nasr et al. “Tight auditing of differentially private machine learning”. In: _32nd USENIX_
_Security Symposium (USENIX Security 23)_ . 2023, pp. 1631–1648.


12


[NSB+25] Milad Nasr et al. “The Last Iterate Advantage: Empirical Auditing and Principled Heuristic
Analysis of Differentially Private SGD”. In: _The Thirteenth International Conference on Learning_
_Representations_ . 2025. URL : `[https://openreview.net/forum?id=DwqoBkj2Mw](https://openreview.net/forum?id=DwqoBkj2Mw)` .


[NST+21] Milad Nasr et al. “Adversary instantiation: Lower bounds for differentially private machine
learning”. In: _2021 IEEE Symposium on security and privacy (SP)_ . IEEE. 2021, pp. 866–882.


[PTC+25] Ashwinee Panda et al. “Privacy Auditing of Large Language Models”. In: _The Thirteenth Inter-_
_national Conference on Learning Representations_ . 2025. URL : `[https://openreview.net/forum?](https://openreview.net/forum?id=60Vd7QOXlM)`
`[id=60Vd7QOXlM](https://openreview.net/forum?id=60Vd7QOXlM)` .


[RFK+19] Aravind Rajeswaran et al. “Meta-learning with implicit gradients”. In: _Advances in neural in-_
_formation processing systems_ 32 (2019).


[SNJ23] Thomas Steinke, Milad Nasr, and Matthew Jagielski. “Privacy auditing with one (1) training
run”. In: _Proceedings of the 37th International Conference on Neural Information Processing Systems_ .
2023, pp. 49268–49280.


[TTS+22] Florian Tramer et al. _Debugging Differential Privacy: A Case Study for Privacy Auditing_ . 2022.
arXiv: `[2202.12219 [cs.LG]](https://arxiv.org/abs/2202.12219)` . URL : `[https://arxiv.org/abs/2202.12219](https://arxiv.org/abs/2202.12219)` .


[ZK16] Sergey Zagoruyko and Nikos Komodakis. “Wide residual networks”. In: _arXiv preprint arXiv:1605.07146_
(2016).


13


