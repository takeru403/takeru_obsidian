1


# Multi-Modal Sensor Fusion for Proactive Blockage Prediction in mmWave Vehicular Networks

Ahmad M. Nazar, Abdulkadir Celik, Mohamed Y. Selim, Asmaa Abdallah, Daji Qiao, and Ahmed M. Eltawil



_**Abstract**_ **—Vehicular communication systems operating in the**
**millimeter wave (mmWave) band are highly susceptible to signal**
**blockage from dynamic obstacles such as vehicles, pedestrians,**
**and infrastructure. To address this challenge, we propose a**
**proactive blockage prediction framework that utilizes multi-**
**modal sensing, including camera, GPS, LiDAR, and radar inputs**
**in an infrastructure-to-vehicle (I2V) setting. This approach uses**
**modality-specific deep learning models to process each sensor**
**stream independently and fuses their outputs using a softmax-**
**weighted ensemble strategy based on validation performance.**
**Our evaluations, for up to 1.5s in advance, show that the camera-**
**only model achieves the best standalone trade-off with an F1-**
**score of 97.1% and an inference time of 89.8ms. A camera+radar**
**configuration further improves accuracy to 97.2% F1 at 95.7ms.**
**Our results display the effectiveness and efficiency of multi-modal**
**sensing for mmWave blockage prediction and provide a pathway**
**for proactive wireless communication in dynamic environments.**


_**Index Terms**_ **—Blockage Prediction, LSTM, Multi-Modal, Inte-**
**grated Sensing and Communications**


I. I NTRODUCTION


Millimeter wave (mmWave) frequencies have emerged as
key next-generation vehicular communication systems enablers, promising ultra-high data rates and low latency. However, the high-frequency nature of these bands makes them
inherently vulnerable to signal blockage from dynamic obstacles such as vehicles and pedestrians. These disruptions are
problematic in fast-changing environments, where traditional
network handling mechanisms often react too slowly to prevent communication hinderances [1]–[3].
Recent research has turned toward generative AI due to its
alluring capabilities [4]. Methods such as proactive blockage
prediction enable preemptive actions such as beam steering,
link rerouting, or adjusting reconfigurable intelligent surface
(RIS) [5], [6]. Prediction remains a significant challenge, as it
requires understanding the spatial layout and temporal dynamics of complex traffic environments. This challenge motivates
the need for real-time sensing and learning frameworks that
temporally reason over multiple sensor inputs [4], [7].
Initial efforts to characterize blockage in vehicular settings
have relied on analytical models. For example, [8] derives
the unconditional blockage probability in highway multi-lane
scenarios using parameters such as vehicle dimensions, antenna placement, and traffic density. The work in [9] proposes


A. M. Nazar, M. Y. Selim, and D. Qiao are with the Department of Electrical
and Computer Engineering, Iowa State University of Science and Technology,
Ames, IA, 50014, USA.
A. Celik, A. Abdallah, and A. M. Eltawil are with Computer, Electrical, and
Mathematical Sciences & Engineering (CEMSE) Division at King Abdullah
University of Science and Technology (KAUST), Thuwal, 23955 KSA.



a radar-based deep learning model that uses object tracking
to anticipate future blockages. Similarly, [10] incorporates
LiDAR-based user localization into a graph neural network to
optimize RIS beamforming. However, their method assumes
blockages are already present and focuses on improving communication through relay mechanisms. Additionally, the work
in [11] highlights the limitations of beamforming and relaybased solutions, and further emphasizes the importance of
proactive sensing in mmWave vehicular communication.
This work proposes a modular, learning-based framework
for proactive blockage prediction in I2V scenarios. Our architecture integrates time-synchronized data from camera, GPS,
LiDAR, and radar sensors co-located with a base station (BS)
at a roadside unit (RSU). Each modality is processed independently through a dedicated blockage model, and their predictions are fused using a softmax-weighted ensemble approach
based on validation performance. This late fusion strategy
enables modularity, interpretability, and real-time feasibility
and allows the system to adapt to varying sensing conditions.
The contributions of this work are threefold. First, we
formulate the I2V blockage prediction problem as a multimodal prediction task and construct a real-world, temporally
aligned sensor processing pipeline using the DeepSense6G
dataset [12], [13]. Second, we design and implement modalityspecific deep learning models for camera, GPS, LiDAR, and
radar data with a confidence-weighted fusion strategy that
balances predictive accuracy and computational efficiency.
Third, we perform extensive evaluation across multiple sensor
configurations. Our findings show that the camera and radar
configuration models provide a notable balance between inference latency and predictive performance, making them ideal
for real-time vehicular applications.
The remainder of the paper is organized as follows: Section II describes the system model and problem formulation.
Section III details the proposed model architecture. Section
IV presents the models’ performance evaluation. Section V
concludes the paper and discusses directions for future work.


II. S YSTEM M ODEL AND P ROBLEM F ORMULATION

This work considers a mmWave I2V communication sce
nario involving two geo-tagged components, as depicted in
Fig. 1. The first component is a vehicle equipped with a
uniform linear array (ULA) consisting of _M_ antennas. The
second component is a fixed RSU, which includes a singleantenna BS co-located and time-synchronized with a multisensor suite comprising a camera, GPS, radar, and LiDAR.
In this setting, the RSU periodically captures environmental
data using the sensors and transmits it to the blockage model.


Fig. 1. Illustration of the system model.


The model receives fused multi-sensor input from the RSU at
discrete time steps _t ∈{t_ 1 _, t_ 2 _, ..., t_ _n_ _}_ . Each input consists of
the latest five sensor observations (i.e., a window of ∆ _T_ = 1 _._ 5
seconds, and _t_ = 300ms), denoted as:


_X_ _t_ = _{_ **x** _t−_ 4 _,_ **x** _t−_ 3 _,_ **x** _t−_ 2 _,_ **x** _t−_ 1 _,_ **x** _t_ _},_ (1)


where **x** _i_ _∈_ R _[C][×][H][×][W]_ represents the features extracted
from the multi-modal sensors at time _i_ .

The blockage prediction model aims to predict the likelihood of blockage events up to _k_ steps ahead by estimating the
blockage probability vector within _k_ denoted as:


**p** _t_ = [ _p_ _t_ +1 _, p_ _t_ +2 _, . . ., p_ _t_ + _k_ ] _,_ where _p_ _t_ + _i_ _∈_ [0 _,_ 1] _._ (2)


Each element _p_ _t_ + _i_ represents the probability of a blockage
occurring at future time _t_ + _i_, given the past sensor window
_X_ _t_ . The model is trained as a binary classifier for each future
step, where the labels _y_ _t_ + _i_ _∈{_ 0 _,_ 1 _}_ indicate the presence or
absence of a blockage event at the corresponding time.
We define the blockage prediction function, _f_ _θ_ ( _·_ ), parameterized by _θ_ as:
**p** _t_ = _f_ _θ_ ( _X_ _t_ ) _._ (3)


III. P ROACTIVE M ULTI -M ODAL B LOCKAGE P REDICTION


We propose a modular multi-modal architecture for predicting future blockage events in a mmWave I2V setting through
camera, GPS, LiDAR, and radar inputs. As illustrated in Fig. 2,
the overall pipeline comprises four stages designed to process,
infer, and fuse sensory observations temporally. Stage **1** _⃝_ performs modality-specific preprocessing to convert raw sensor
data into spatially and temporally consistent representations;
Stage _⃝_ **2** applies independent deep learning models tailored
to each modality to predict blockage probabilities based on
past observations; Stage _⃝_ **3** combines the outputs from all
modalities using a weighted probability fusion strategy, where
each prediction is scaled according to its estimated reliability;
Finally, Stage _⃝_ **4** outputs the blockage classification.
This architecture enables each sensor stream to be modeled

independently and flexibly fused at the decision level for
interpretability, modularity, and resilience to sensor-specific
failure or noise. The remainder of this section elaborates

on each modality model’s architecture and learning strategy,
followed by the fusion design that unifies their predictions.



2


Fig. 2. Architecture of the proposed blockage prediction model.


_A. Data Preprocessing_


This work leverages real-world, multi-modal sensor measurements from the DeepSense6G dataset [13], [14] to construct a robust input representation for blockage prediction.
The dataset provides heterogeneous sensor modalities captured
in realistic vehicular environments. In particular, we utilize
Scenarios 31-34, which include temporally synchronized data
streams such as LiDAR point clouds, radar reflections, camera
images, and GPS traces from the vehicle and the RSU.
_1) Data Augmentation:_ We apply tailored data augmentation strategies during training to improve generalization across
sensor modalities. For images, random horizontal flips, smallangle rotations, and Gaussian blurring are used to simulate
variations in perspective and lighting. LiDAR point clouds and
projections are augmented through random flips, rotations, and
spatial scaling to introduce geometric diversity. For radar data,
random Gaussian noise is added to the raw samples to simulate
sensor uncertainty and measurement variability.
_2) Class Imbalance Handling:_ The blockage prediction
task is inherently imbalanced, with significantly more nonblocked samples (label 0) than blocked ones (label 1). To
address this, we apply a loss weighting strategy that penalizes
misclassified positive samples more heavily during training.
Specifically, we compute a scalar positive class weight as:



where _N_ 0 and _N_ 1 denote the number of negative and
positive samples, respectively, and _α_ = 1 _._ 1 is a tunable scaling
factor used to adjust the degree of compensation.



_N_ 0
_w_ pos = _α ·_ � _N_ 1



_,_ (4)
�


_3) Camera Preprocessing:_ Camera frames are resized to
256 _×_ 256 resolution, and pixel values are normalized to ensure
consistency in subsequent feature extraction.
_4) GPS Preprocessing:_ We implement a handcrafted feature extraction pipeline applied over five consecutive GPS
readings to extract meaningful motion dynamics from raw
GPS trajectories. We compute first- and second-order temporal
derivatives from these sequences to characterize vehicle movement. Specifically, we extract displacements, speeds, angular
changes, accelerations, angular velocity, and curvature. These
features are chosen to capture both linear and rotational
motion, indicative of sudden maneuvers or mobility patterns
associated with potential blockages. The final feature vector
contains 18 elements: six displacement values, four instantaneous speeds, three angular changes, three accelerations, one
angular velocity, and one curvature metric. For consistency
across the dataset, we apply min-max normalization using a
pre-fitted scaler learned from the training set.
_5) LiDAR Preprocessing:_ A multi-step pipeline is applied
to efficiently process the high-density LiDAR point clouds to
remove noise, segment objects, and transform raw 3D data
into a structured representation suitable for deep learning.
A voxel-grid downsampling technique reduces the density
of LiDAR point clouds while preserving spatial structure [15].
Outliers in the point cloud are removed using a statistical
analysis-based filtering method, where each point’s mean
distance to its nearest _k_ neighbors is computed. Points that
significantly deviate from the local distribution are discarded
based on a threshold derived from a standard deviation factor

calculated directly from the point distribution. Following outlier removal, the ground plane is extracted and removed using a
random sample consensus (RANSAC) plane-fitting algorithm,
which iteratively fits a plane model to the LiDAR points and
eliminates those within a fixed distance from the estimated
ground surface [15].
To segment individual objects, density-based spatial clustering of applications with noise (DBSCAN) clustering is applied
to the filtered point cloud. The clustering parameters, _ϵ_ lidar =
0 _._ 75 and min_samples lidar = 5, are selected by analyzing a
_k_ -distance graph, where the optimal _ϵ_ lidar corresponds to the
maximum curvature point. Clusters not meeting the minimum
size criteria are treated as noise and removed.

The processed LiDAR point cloud is then projected into a
bird’s-eye view (BEV) representation, which provides a structured 2D encoding of the scene. The point cloud is mapped
onto a fixed grid spanning _X ∈_ [ _−_ 50 _,_ 50] m, _Y ∈_ [ _−_ 50 _,_ 50]
m, and _Z ∈_ [ _−_ 2 _._ 5 _,_ 15] m, with a discretization resolution of
∆ _r_ = 0 _._ 25 m per cell. Each grid cell encodes three key
features: the maximum _Z_ -coordinate of points in the cell
to form a height map, the log-normalized count of points
per cell to construct a density map, and the variance of
height values within the cell to capture surface roughness and
structural details. The final BEV image has a resolution of
( _H, W_ ) = (700 _,_ 1200) pixels with three feature channels per
frame. Sequential LiDAR frames are stacked along the channel
axis, yielding a ( _H, W,_ 15) tensor representation representing
temporal features.
Filtered point clouds are then normalized such that height



3


values are min-max normalized within the defined _Z_ range,
density values are log-scaled to balance sparsity and variance
values are standardized based on their maximum observed

values.

_6) Radar Preprocessing:_ We construct a multi-channel
feature representation from raw radar data to enhance the
discriminative capacity of radar inputs for blockage prediction.
Each radar file contains complex-valued tensors from four
virtual antenna channels, with a native shape of (4 _,_ 256 _,_ 250)
representing azimuth, range, and Doppler bins, respectively.
From these, we extract a comprehensive set of eight feature
maps. First, the raw data is decomposed into magnitude and
phase components to capture amplitude and phase variation
across spatial cells. A 1D Fast Fourier Transform (FFT) is
then applied along the Doppler dimension to extract spectral
information, particularly the distribution of returned signal
energy across velocity bins.
In addition to raw features, we compute several statistical
descriptors to encode spatial structure and motion dynamics.
These include the mean and standard deviation of magnitude
across antenna channels and an entropy map computed from
the normalized magnitude, which serves as a proxy for scene
complexity or target diversity. Doppler-specific features such
as mean velocity and spectral width are derived from the power
spectrum to characterize dynamic motion across bins.
All feature maps are normalized to a consistent spatial
resolution of (256 _×_ 64) using trimming or zero-padding to
accommodate variable Doppler bin lengths. One-dimensional
descriptors are repeated across antenna channels where needed
to ensure shape consistency. The final radar input is constructed by stacking all eight features into a tensor of shape
(8 _,_ 256 _,_ 64), capturing a rich blend of spatial, spectral, and
kinematic cues essential for robust blockage forecasting.


_B. Camera Blockage Prediction_


The camera model is designed to capture spatial and temporal patterns indicative of future blockage events. The spatial
feature extraction backbone is a ResNet-18 convolutional

neural network pretrained on ImageNet, with the final fully
connected classification layer removed.
To model temporal dynamics, we process the input sequences using an long short-term memory (LSTM). The
LSTM allows the model to capture motion cues, object trajectories, and evolving occlusions. For the LSTM configuration,
the encoded frame embeddings are passed through a singlelayer LSTM with 128 hidden dimensions, and the final hidden
state is used for downstream classification. This layer’s output
is then passed through a two-layer fully connected classifier
with a ReLU activation and dropout regularization ( _p_ = 0 _._ 4),
and outputs the blockage probability.


_C. GPS-Based Blockage Prediction Model_


The GPS blockage prediction model is designed to extract
mobility features from GPS samples and predict blockages.
We extract an 18-dimensional feature vector from the input
sequence encoding displacement, velocity, acceleration, angular change, and curvature using a custom feature engineering


pipeline. These features are normalized and passed to a temporal encoder composed of a two-layer LSTM with a hidden size
of 128. The LSTM’s final hidden state is processed through a
fully connected classifier consisting of a 64-unit hidden layer,
ReLU activation, dropout, and outputs the blockage prediction.


_D. LiDAR Blockage Prediction_


The LiDAR model is built to extract spatial features from
a temporally stacked BEV representation of LiDAR point
clouds. Each input sample comprises five consecutive BEV
frames containing three spatial channels, resulting in a 15channel tensor input. We adopt a ResNet-18 convolutional
backbone to accommodate this high-dimensional input by
modifying the first convolutional layer to accept 15 channels.
The ResNet-18 architecture then processes the stacked LiDAR frames through a hierarchy of residual blocks, progressively extracting deep spatial features relevant to the environment’s obstacle presence, object shapes, and occlusion patterns. Unlike the camera and radar models, which incorporate
explicit temporal modeling, the temporal information in the
LiDAR stream is implicitly learned through the early fusion
of the sequential BEV frames. The final fully connected layer
of the ResNet is modified to output the blockage probability.


_E. Radar Blockage Prediction_


The radar-based model is designed to extract spatial motion
signatures and temporal evolution from sequential radar scans.
Each radar frame is an 8-channel tensor of size 256 _×_ 64,
capturing various magnitude, phase, frequency, and Dopplerderived features. These are processed through a stack of three
2D convolutional layers with ReLU activations and batch
normalization, progressively reducing spatial resolution while
increasing channel depth from 8 to 128.
An adaptive average pooling layer is applied to each radar
frame to further condense spatial information. The resulting
sequence of feature vectors is then passed through a singlelayer LSTM with 64 hidden dimensions. The LSTM’s final
hidden state is input to a fully connected classification head
consisting of two linear layers with ReLU activation, dropout
regularization ( _p_ = 0 _._ 3), and outputs the blockage probability.


_F. Multi-Modal Fusion_


To effectively combine predictions from the LiDAR, radar,
and camera models, we adopt a late fusion strategy that avoids
the architectural and computational complexity of end-toend multi-modal networks. Each modality processes its input
independently and produces a probability score indicating the
likelihood of a future blockage event. These modality-specific
predictions—denoted as _P_ LiDAR _, P_ Radar _,_ and _P_ Camera —are then
fused at the decision level. The fusion mechanism is based on

a weighted averaging scheme:


_P_ fused = � _w_ _i_ _· P_ _i_ _._ (5)

_i∈{_ LiDAR _,_ Radar _,_ Camera _}_


To determine the weights _w_ _i_, we adopt a data-driven
approach grounded in validation performance. Specifically,



4


each modality’s weight is computed using the softmax over
its F1-score obtained from a held-out validation set. Let
**s** = [ _s_ LiDAR _, s_ Radar _, s_ Camera ] denote the vector of validation F1scores. Then, the normalized fusion weights are given by:


exp( _s_ _i_ )
**w** = softmax( **s** ) = (6)
~~�~~ _j_ [exp(] _[s]_ _[j]_ [)] _[.]_


This formulation ensures that higher-performing modalities
have more influence in the final prediction while retaining
contributions from less dominant modalities. It also avoids the

potential bias of manually assigned weights, and adapts to the
relative strengths of the models during training.


IV. E VALUATION


This section evaluates the performance of 15 unordered
permutation-based blockage models using F1-score, area under
the receiver operating characteristic curve (AUC-ROC), and
inference time as key performance indicators (KPIs). Each
model completes inference within the 300ms interval between
consecutive inputs to maintain compatibility with real-time
deployment requirements. Table I summarizes predictive performance, while Table II presents timing performance.
Among all configurations, the camera-only and camera+radar models offer the best trade-offs between accuracy
and latency. With inference times of 89.8ms and 95.7ms,
respectively, these models are ideally suited for latencysensitive applications. Focusing on the _t_ +5 prediction horizon,
camera-based models consistently outperform all other modality combinations. The camera-only model achieves an F1score of 97.1% while maintaining the lowest overall latency,
demonstrating that visual cues alone provide rich spatiotemporal information sufficient for robust blockage prediction. The
camera+radar model further improves performance to 97.2%
F1 through the benefits of radar’s motion and depth perception
capabilities without significantly increasing inference latency.
In contrast, including the LiDAR does not provide additional
predictive value in high-performing configurations. Although
LiDAR contributes detailed 3D structural information, its addition consistently leads to lower F1 scores than camera-only
or camera+radar variants. For example, the camera+LiDAR
model achieves 95.5% F1 at _t_ + 5 while incurring a significantly higher latency of 121.8ms. The marginal structural
advantages offered by LiDAR are outweighed by the increased
input dimensionality and redundancy, which may lead to
overfitting, especially when temporal correlations are weak.
The addition of GPS data similarly offers negligible or
even negative value for blockage prediction. Across all GPSinclusive models, performance degrades relative to models
without GPS despite noticeable increases in latency. For
instance, the camera+GPS model achieves only 93.8% F1 at
_t_ + 5, and the full sensor fusion model reaches just 92.0% F1
which is well below the top performing configurations. These
results confirm that GPS-based data lacks the temporal and
environmental awareness necessary to predict blockages.
Radar-only models also demonstrate strong performance,
achieving 93.5% F1 at _t_ + 5, with low inference latency
(92.6ms). This result suggests that radar remains a reliable


5



TABLE I

B LOCKAGE PREDICTION MODEL PERFORMANCE SUMMARY


|Modality|t + 1|Col3|t + 2|Col5|t + 3|Col7|t + 4|Col9|t + 5|Col11|
|---|---|---|---|---|---|---|---|---|---|---|
|**Modality**|**F1**<br>**Score**|**AUC-ROC**|**F1**<br>**Score**|**AUC-ROC**|**F1**<br>**Score**|**AUC-ROC**|**F1**<br>**Score**|**AUC-ROC**|**F1**<br>**Score**|**AUC-ROC**|
|camera radar|98.4%|0.988|98.0%|0.985|97.9%|0.982|97.5%|0.971|97.2%|0.968|
|camera only|98.1%|0.983|97.8%|0.981|97.5%|0.970|97.2%|0.969|97.1%|0.963|
|camera lidar|96.2%|0.955|96.1%|0.947|95.7%|0.939|95.5%|0.935|95.5%|0.933|
|camera gps|94.4%|0.931|94.1%|0.930|93.9%|0.927|93.9%|0.927|93.8%|0.924|
|camera radar lidar|94.0%|0.928|93.9%|0.926|93.8%|0.925|93.7%|0.922|93.7%|0.920|
|radar only|93.7%|0.922|93.7%|0.921|93.6%|0.919|93.5%|0.916|93.5%|0.915|
|camera gps radar lidar|93.1%|0.914|93.0%|0.913|92.7%|0.910|92.2%|0.909|92.0%|0.909|
|camera gps radar|92.3%|0.911|92.2%|0.907|92.0%|0.904|91.8%|0.902|91.7%|0.898|
|radar ladar|91.8%|0.902|91.6%|0.899|91.5%|0.899|91.4%|0.896|91.3%|0.891|
|camera gps lidar|91.2%|0.897|91.1%|0.895|91.1%|0.894|90.8%|0.890|90.8%|0.890|
|gps lidar radar|89.9%|0.889|89.8%|0.887|89.5%|0.885|89.4%|0.881|89.0%|0.879|
|gps radar|89.3%|0.880|89.2%|0.879|89.0%|0.877|88.9%|0.875|88.6%|0.873|
|lidar only|87.9%|0.872|87.9%|0.872|87.7%|0.870|87.4%|0.868|87.3%|0.865|
|gps lidar|84.1%|0.855|84.0%|0.852|83.6%|0.849|83.2%|0.847|83.0%|0.831|
|gps only|61.7%|0.603|61.4%|0.600|60.9%|0.599|60.7%|0.592|60.6%|0.588|



TABLE II

B LOCKAGE PREDICTION MODEL TIMINGS IN MILLISECONDS ( MS )

|Modality|Preprocessing Time|Inference Time|
|---|---|---|
|camera gps radar lidar|56.0|137.2|
|camera radar lidar|54.3|128.9|
|camera gps lidar|43.3|127.5|
|gps lidar radar|51.8|125.0|
|radar lidar|50.7|124.1|
|camera lidar|42.4|121.8|
|gps lidar|38.4|121.6|
|camera gps radar|18.3|121.2|
|lidar only|37.9|117.3|
|camera gps|4.96|109.4|
|gps radar|13.5|96.1|
|camera radar|17.8|95.7|
|radar only|12.4|92.6|
|camera only|4.11|89.8|
|gps only|_<_ 1|37.3|



sensing modality. Meanwhile, LiDAR-only models trail behind, reaching only 87.3% F1, reaffirming their limited standalone effectiveness for the blockage prediction task.


V. C ONCLUSION


Our research introduces a proactive blockage prediction
framework for mmWave I2V communication systems through
multi-modal sensor fusion. We predict future blockage events
up to 1.5 seconds in advance through temporally synchronized
multi-modal data with an F1-score of up to 98.4%. Our
findings highlight the significance of selecting appropriate
modalities in real-time blockage prediction systems, revealing
that lightweight, vision-centric configurations are practical and
conducive to deployment. Future research aims to investigate
online adaptation strategies, quantify uncertainties, and integrate our framework with beam selection to enhance resilience
in dynamic vehicular environments.


R EFERENCES


[1] J. Fontaine, A. Shahid, and E. De Poorter, “Towards a wireless physicallayer foundation model: Challenges and strategies,” _arXiv preprint_
_arXiv:2403.12065_, 2024.




[2] G. Zhu, D. Liu, Y. Du, C. You, J. Zhang, and K. Huang, “Toward
an intelligent edge: Wireless communication meets machine learning,”
_IEEE Commun. Mag._, vol. 58, no. 1, pp. 19–25, Jan. 2020.

[3] J. Tan, T. H. Luan, W. Guan, Y. Wang, H. Peng, Y. Zhang, D. Zhao, and
N. Lu, “Beam alignment in mmwave v2x communications: A survey,”
_IEEE Communications Surveys & Tutorials_, 2024.

[4] K. B. Letaief, W. Chen, Y. Shi, J. Zhang, and Y.-J. A. Zhang, “The
roadmap to 6G: AI empowered wireless networks,” _IEEE Commun._
_Mag._, vol. 57, no. 8, pp. 84–90, Aug. 2019.

[5] A. Abdallah, A. Celik, M. M. Mansour, and A. M. Eltawil, “Multi-agent
deep reinforcement learning for beam training in cell-free RIS-aided
systems,” _IEEE Transactions on Wireless Communications_, 2024.

[6] ——, “RIS-aided mmwave MIMO channel estimation using deep learning and compressive sensing,” _IEEE Trans. on Wireless Comms._, 2022.

[7] C. Zhang, P. Patras, and H. Haddadi, “Deep learning in mobile and
wireless networking: A survey,” _IEEE Communications Surveys &_
_Tutorials_, vol. 21, no. 3, pp. 2224–2287, 2019.

[8] K. Dong, M. Mizmizi, D. Tagliaferri, and U. Spagnolini, “Vehicular blockage modelling and performance analysis for mmwave v2v
communications,” in _ICC 2022 - IEEE International Conference on_
_Communications_, 2022, pp. 3604–3609.

[9] U. Demirhan and A. Alkhateeb, “Radar aided proactive blockage
prediction in real-world millimeter wave systems,” 2021. [Online].
[Available: https://arxiv.org/abs/2111.14805](https://arxiv.org/abs/2111.14805)

[10] A. M. Nazar, M. Y. Selim, and D. Qiao, “Revolutionizing
ris networks: Lidar-based data-driven approach to enhance ris
beamforming,” _Sensors_, vol. 25, no. 1, 2025. [Online]. Available:
[https://www.mdpi.com/1424-8220/25/1/75](https://www.mdpi.com/1424-8220/25/1/75)

[11] J. Choi, V. Va, N. Gonzalez-Prelcic, R. Daniels, C. R. Bhat, and R. W.
Heath, “Millimeter-wave vehicular communication to support massive
automotive sensing,” _IEEE Communications Magazine_, 2016.

[12] A. Alkhateeb, G. Charan, T. Osman, A. Hredzak, J. Morais,
U. Demirhan, and N. Srinivas, “Deepsense 6g: A large-scale real-world
multi-modal sensing and communication dataset,” _IEEE Communica-_
_tions Magazine_, 2023.

[13] G. Charan, U. Demirhan, J. Morais, A. Behboodi, H. Pezeshki, and
A. Alkhateeb, “Multi-modal beam prediction challenge 2022: Towards
generalization,” 2022.

[14] A. Alkhateeb, G. Charan, T. Osman, A. Hredzak, J. Morais,
U. Demirhan, and N. Srinivas, “Deepsense 6g: A large-scale real-world
multi-modal sensing and communication dataset,” _IEEE Communica-_
_tions Magazine_, 2023.

[15] B. Wang, J. Lan, and J. Gao, “Lidar filtering in 3d object detection
based on improved ransac,” _Remote Sensing_, vol. 14, no. 9, 2022.

[[Online]. Available: https://www.mdpi.com/2072-4292/14/9/2110](https://www.mdpi.com/2072-4292/14/9/2110)


