## **Dynamics is what you need for time-series forecasting!**

**Alexis-Raja Brachet** [1] _[,]_ [2] **Pierre-Yves Richard** [2] **Céline Hudelot** [1]

1 MICS, CentraleSupélec, Université Paris-Saclay, France
2 CentraleSupélec, IETR UMR CNRS 6164, France
```
          alexisraja.brachet@centralesupelec.fr
      {pierre-yves.richard,celine.hudelot}@centralesupelec.fr

```

**Abstract**


While boundaries between data modalities are vanishing, the usual successful deep
models are still challenged by simple ones in the time-series forecasting task. Our
hypothesis is that this task needs models that are able to learn the data underlying
dynamics. We propose to validate it through both systemic and empirical studies.
We develop an original `PRO-DYN` nomenclature to analyze existing models through
the lens of dynamics. Two observations thus emerged: **1.** under-performing
architectures learn dynamics at most partially, **2.** the location of the dynamics
block at the model end is of prime importance. We conduct extensive experiments
to confirm our observations on a set of performance-varying models with diverse
backbones. Results support the need to incorporate a learnable dynamics block and
its use as the final predictor.


**1** **Introduction**


In recent years, data-driven models, especially deep learning ones, have successfully processed data
in various tasks. While specific models were designed regarding the modality, we face a model
homogenization (Bommasani et al., 2022): Transformer models (Vaswani et al., 2017) originating
from the text modality are becoming state-of-the-art (SOTA) across various fields (Veliˇckovi´c et al.,
2018; Dosovitskiy et al., 2021; Chen et al., 2023). Boundaries between modalities are vanishing.
However, in the specific case of time-series forecasting, these usual models are still challenged by
quite simple ones (Zeng et al., 2022; Xu et al., 2024; Tan et al., 2024).


Most of the text-based models, including RNNs (Elman, 1990), LSTMs (Hochreiter & Schmidhuber,
1997), Transformers (Vaswani et al., 2017), or more recently, State-Space Models (SSMs) (Gu &
Dao, 2024), follow the sequence-to-sequence paradigm, turning one sequence into another sequence.
They received a lot of success in text generation, fitting quite well to this modality: the bigger the
model capacity is, the better the performance, see (Hoffmann et al., 2022) scaling law. As time-series
forecasting (TSF) can also be seen as a sequence-to-sequence task, or more precisely, a series-toseries task, the previous text-based models have naturally been adapted to it (as Informer (Zhou
et al., 2021) or FEDformer (Zhou et al., 2022a)). However, it has been shown that these models
are well challenged by basic ones, among which the LSTF-Linear models (Zeng et al., 2022) or
FITS (Xu et al., 2024). These simple models map the input and output data by a linear layer after a
non-learnable pre-processing. Recent SOTA approaches are now built upon the basic linear models,
with complex backbones as pre-processing units (Nie et al., 2023; Liu et al., 2024b; Hu et al., 2024;
Qiu et al., 2025).


These observations raise two points: **a.** generating time series are inherently different from generating
text, even though they have a similar structure; [1] **b.** the problem does not seem to come from using
text-based architectures but from the way we use them on this kind of data. To our knowledge no


1 In classification or anomaly detection, we don’t observe this phenomenon, see results in (Xu et al., 2024)


Preprint. Under review.


systemic study to explain these observations has been proposed in the literature. A previous work
on Transformer failure in TSF (Ke et al., 2025) only focused on the attention mechanism, without
explaining recent Transformer-based model achievements (Nie et al., 2023; Liu et al., 2024b).


Text-based models were designed to replicate the text generation mechanism (Rayner, 1998; Cheng
et al., 2016). **We argue that TSF models should replicate the time-series generation mechanism** .
This mechanism is, in a majority of fields, modeled as a data evolution law, called a dynamical
system, a priori known in physics (Raissi et al., 2019; Li et al., 2021; Kovachki et al., 2024) or
economics (Liu et al., 2024a) or estimated a posteriori (Shojaee et al., 2024). It legitimates the
modeling of a time-series evolution by its underlying dynamics. **We thus hypothesize that TSF**
**models should be able to learn a time-series dynamics** .


This work focuses on the study of this hypothesis. We develop an original nomenclature named
`PRO-DYN` . It enables making explicit how dynamics is involved in a model ( `DYN` function), surrounded
by processing units ( `PRO` functions). We explicit the dynamics learned by LSTF-Linear models (Zeng
et al., 2022). We then perform a systemic study of existing TSF models (Qiu et al., 2024). We
derive two main observations: **1.** under-performing models have no (or partial) learnable dynamics
modeling (supporting our hypothesis); **2.** SOTA architectures do learn a dynamics (again supporting
our hypothesis), combining deep blocks as pre-processing units and a dynamics block at the end as
the predictor, giving clues to model design considerations.


To study empirically the first observation, we incorporate linear dynamics, without any structural
hyperparameter modification, into targeted models which have no or partial dynamics modeling
capabilities: two Transformer-based ones, Informer (Zhou et al., 2021), FEDformer (Zhou et al.,
2022a), the CNN-based MICN (Wang et al., 2023), and the SSM-based FiLM (Zhou et al., 2022b).
Our experiments show tangible performance improvements, which support that learnable dynamics
modeling capabilities drive the performance. Then, to study the second observation, we add a
Linear dynamics layer at the entry of recent SOTA foundation models, iTransformer (Liu et al.,
2024b), PatchTST (Nie et al., 2023), and Crossformer (Zhang & Yan, 2023) to employ them as
post-processing units, again without any structural hyperparameter modification. Our experiments
show that pre-processing-like architectures are the best choice as they take better advantage of longer
look-back windows.


**2** **Related work**


**TSF by adapting text-based Transformers** The main concern when adapting text-based models
for time-series forecasting was efficiency; the development of the LogSparse attention was the pioneer
work in Transformer-based TSF (Li et al., 2019), but it kept the slow autoregressive process. The
most influential work became Informer, where they defined the ProbSparse attention and generated
predictions in one forward pass (Zhou et al., 2021). Models like Autoformer (Wu et al., 2021),
FEDformer (Zhou et al., 2022a), and Non-stationary Transformers (Liu et al., 2023), inherited
from Informer computations: initialize a decoder with a simple non-learnable prediction (mean
or zero-padding) without any learnable dynamics. Later, these models were beaten by the LSTFLinear models (Zeng et al., 2022). The focus of complex deep model failure has been on attention
mechanism (Zeng et al., 2022; Ke et al., 2025), but recent SOTA models are still attention-based (Liu
et al., 2024b). Our work focuses on the learning dynamics capabilities of TSF models, a possible
major performance driver.


**Models inheriting from LSTF-Linear models** LSTF-Linear models (Zeng et al., 2022) were
introduced in earlier works on Direct Multi-step (DMS) forecasting (Chevillon, 2005), again for
simplicity and efficiency, avoiding accumulated errors from Iterated Multi-step (IMS). They have been
automatically adopted by the vast majority of diverse models for their performance and efficiency,
as in TiDE (Das et al., 2023), iTransformer (Liu et al., 2024b), or Attraos (Hu et al., 2024), beating
previous LSTF-Linear models. To our knowledge, no systemic justification has been proposed
to support the integration of Linear functions. Our work proposes one based on Linear learning
dynamics capabilities.


**Models in TSF when an a priori is known** The a priori knowledge on time-series data is usually
in the form of dynamics, which defines the relation between current and future states, as Partial
Differential Equations (PDEs). It strongly conditions model design as in Physical Informed Neural


2


Networks (Raissi et al., 2019), which includes the PDE residuals into the loss term, Neural operators
(Li et al., 2021; Kovachki et al., 2024), which map initial states and boundary conditions to the PDE
solution, and Neural ODEs (Chen et al., 2019; Liu et al., 2024a) which apply the data evolution law
to the latent state evolution. A recent work on TSF thus supposes a strong a priori PDE knowledge
and combines patching and Neural ODEs (Qi et al., 2024). Different from this work, we hypothesize
TSF models should be able to learn a dynamics and analyze how they do so.


**3** **Systemic analysis through the lens of dynamics**


**Time-series and time-series space** We consider a time-series **X** = _{x_ _d_ ( _t_ 1 ) _, . . ., x_ _d_ ( _t_ _L_ ) _}_ _[D]_ _d_ =1 _[∈]_
R _[L][×][D]_ which is the historical data of _D_ variates along _L_ regularly sampled timestamps _t_ _i_ _∈_ R [+] with
_t_ _i_ _< t_ _j_ _, ∀i, j ∈{_ 1 _, . . ., L}|i < j_ . A time-series space T is a real-valued space from the product of
a time interval _T ⊂_ [0 _,_ + _∞_ [ and a latent space dimension ( _d_ 1 _, . . ., d_ _k_ ) _∈_ N _[k]_, with _k ∈_ N . A time
interval of a time-series **X** is the smallest interval containing _{t_ 1 _, . . ., t_ _N_ _}_ . With _T_ **X** = [ _t_ 1 _, t_ _L_ ], the
historical time interval of **X**, the time-series space of **X** is T **X** = R _[T]_ **[X]** _[×][D]_ .


**TSF task** The time-series forecasting task of **X** is to infer the _H_ future timestamps
**Y** = _{x_ ˜ _d_ ( _t_ _L_ +1 ) _, . . .,_ ˜ _x_ _d_ ( _t_ _L_ + _H_ ) _}_ _[D]_ _d_ =1 [based on the] _[ L]_ [ historical ones, i.e.] **X** . We denote
_T_ **Y** = [ _t_ _L_ +1 _, t_ _L_ + _H_ ] the prediction time interval.


**Dynamical systems** Based on a current system state **x** ( _t_ ) _∈_ R _[D]_ at time _t ∈_ R [+], the system is
called _dynamic_ when there exists an evolution function Φ : R [+] _×_ R [+] _×_ R _[D]_ _→_ R _[D]_ mapping the
current state **x** ( _t_ ) to the future states at _t_ + _τ, ∀τ ∈_ R [+] such as: Φ( _t, τ,_ **x** ( _t_ )) = **x** ( _t_ + _τ_ ) . It defines
a direct link between current observations and future ones (dynamics can evolve through time).


**3.1** **The** `PRO-DYN` **nomenclature**


We characterize computations performed in TSF models regarding time, the basis of our nomenclature.


Let _E_ = R _[T]_ _[E]_ _[×][d]_ _[E]_ a time-series space. We consider a function _f_ mapping _E_ to another time-series
space _F_ = R _[T]_ _[F]_ _[×][d]_ _[F]_ . Depending on the task assigned to _f_, it involves temporal relations between _T_ _E_
and _T_ _F_ . In this paper, we rely on the popular Allen’s interval algebra (Allen, 1983) that defines 13
different relations between two time intervals (illustrated in Appendix A). We introduce the notions
of `PRO` ( `PRO` cessing) and `DYN` ( `DYN` amics) function, based on Allen’s temporal interval relations: _f_ is
`PRO` if and only _T_ _E_ **contains, started by, finished by, or equals** _T_ _F_ . _f_ is `DYN` if and only if _T_ _E_ **starts,**
**overlaps, meets, or before** _T_ _F_ .


Based on time-evolution considerations, we propose to introduce three types of functions that can be
used to decompose any model designed for a TSF task. More precisely, we consider that any model
_M_ _θ_ designed for a TSF task, with learnable parameters _θ_, that takes data points in the historical time
interval _T_ **X** and outputs predictions in the future _T_ **Y**, can be decomposed as follows:


_M_ _θ_ ( **X** ) = _f_ _θ_ _[post]_ _post_ [(] **[X]** _[, f]_ _θ_ _[ pre]_ _pre_ [(] **[X]** [)] _[, f]_ _θ_ _[ dyn]_ _dyn_ [(] **[X]** _[, f]_ _θ_ _[ pre]_ _pre_ [(] **[X]** [)))] (1)


where _f_ _θ_ _[dyn]_ _dyn_ [, a] `[ DYN]` [ function, defines] _[ M]_ _[θ]_ [ dynamics performing a prediction going from] _[ T]_ **[X]** [ to] _[ T]_ **[Y]** [ (or]

_T_ **X** _→T_ **X** _∪T_ **Y** in a start/overlap case); _f_ _θ_ _[pre]_ _pre_ [,] _[ f]_ _θ_ _[ post]_ _post_ [, two] `[ PRO]` [ functions, are pre and post (relatively]

to _f_ _θ_ _[dyn]_ _dyn_ [) processing functions, performing computations while staying in their input time interval.]
We illustrate our framework in Figure 1 for univariate time-series ( _D_ = 1 ) for the sake of simplicity.


Based on this, we introduce our original `PRO-DYN` nomenclature. For any TSF model :


1. we decompose it as a composition of `PRO` and `DYN` functions;

2. we identify the nature of the `DYN` function;

3. we identify the backbone and the location of the `PRO` functions;

4. we identify the `PRO` computations that change the temporal dimension while staying in the
same time interval (mapping to a latent dimension, sampling, filtering,...).


3


Figure 1: `PRO` and `DYN` functions illustrated in the processing chain of a TSF model _M_ _θ_ . `PRO`
functions are framed and blue while `DYN` function is encircled and orange. Solid lines represent the
main data flow. _f_ _θ_ _[post]_ _post_ [can be fed by] **[ X]** [ or/and] _[ f]_ _θ_ _[ pre]_ _pre_ [(] **[X]** [)] [ (dotted lines). Dotted line from] **[ X]** [ to] _[ f]_ _θ_ _[ dyn]_ _dyn_
and time interval start/overlap case are not drawn for better clarity.


From the `PRO-DYN` nomenclature, we first analyze the LSTF-Linear models (Zeng et al., 2022), the
basic models challenging deep complex ones, through the lens of dynamics.


**3.2** **Linear dynamics**


Without loss of generality, let us suppose _L ≥_ _H_ (see Appendix B for the _L < H_ case). Let us
denote **X** _[T]_ ( _t_ _L_ ) _∈_ R _[D][×][L]_, the transpose of **X** while explicitly showing the current timestamp _t_ _L_,
such as **X** _[T]_ ( _t_ _L_ ) = _{x_ 1 ( _t_ _i_ ) _, . . ., x_ _D_ ( _t_ _i_ ) _}_ _[L]_ _i_ =1 [: we consider the] _[ L]_ [ previous timestamps (the look-back]
window) of **X** from _t_ _L_ as features. LSTF-Linear models are the composition of a `PRO` pre-processing
step _f_ _pre_ : **X** _[T]_ ( _t_ _L_ ) _�→_ **X** _[T]_ _pre_ [(] _[t]_ _[L]_ [)] [ (where] _[ f]_ _[pre]_ [is identity for Linear, normalization for NLinear, or]
seasonal-trend decomposition [2] for DLinear), and a `DYN` function `Linear` _θ_ such as:


**X** _[T]_ ( _t_ _L_ + _H_ ) _|_ [ _tL_ +1 _,tL_ + _H_ ] = `Linear` _θ_ _◦_ _f_ _pre_ ( **X** _[T]_ ( _t_ _L_ )) = **X** _[T]_ _pre_ [(] _[t]_ _[L]_ [)] _[W]_ _[θ]_ [+] _[ b]_ _[θ]_ (2)


where **X** _[T]_ ( _t_ _L_ + _H_ ) _|_ [ _tL_ +1 _,tL_ + _H_ ] is the extraction of **X** _[T]_ ( _t_ _L_ + _H_ ) on the prediction time interval

[ _t_ _L_ +1 _, t_ _L_ + _H_ ] ; _W_ _θ_ _∈_ R _[L][×][H]_ and _b_ _θ_ _∈_ R _[H]_, are the parameters of `Linear` _θ_ (for DLinear, the output is
the sum of seasonal and trend linear layer outputs). These parameters are, respectively, in terms of
dynamics, the dynamics matrix, and an external force applied to the system. LSTF-Linear models
training corresponds to studying one iteration of this dynamics at different timestamps _t_ _L_ . **LSTF-**
**Linear models do have learnable dynamics modeling capabilities, which would explain their**
**performance.**


**3.3** **TSF models through the** `PRO-DYN` **nomenclature**


The goal here is to identify features from the `PRO-DYN` nomenclature driving model performance. We
analyze all the deep models tested in the benchmark (Qiu et al., 2024) (chosen for its diversity of
datasets). We end up with Table 1, keeping the same row-order performance as in the benchmark on
the multivariate TSF task.


There are from Table 1 two _performance-based_ groups, identified by the horizontal line: models
better than NLinear (chosen as the reference as it is the best performing simple model), and models
worse than it. From the `PRO-DYN` nomenclature, models in the first group have two features (identified
with green color) in common: a learnable `DYN` Linear function and a `PRO` function for pre-processing
only, while in the second group, the main shared features (identified with magenta color) are an, at
most partially, non-learnable `DYN` function and `PRO` functions for pre- and post-processing.


2 The trend component **T** is a moving average over the input and the seasonal component **S** is the input
without the trend component, such as **X** = **S** + **T** .


4


Table 1: TSF deep models through the `PRO-DYN` nomenclature. Attn stands for attention, CNN for
Convolution Neural Network, Norm. for normalization, Seas.-trend for seasonal-trend decomposition,
SConv. for Spectral Convolution, discr. for discretization, NS for Non-stationary, AR for autoregressive. Latent means temporal dimension mapped to a hidden latent dimension. Colors correspond to
features identified to drive (green) or drag (magenta) the performance on the TSF task.

|Model|DYN function PRO backbone PRO role PRO time dim. changes Reference|
|---|---|
|**DUET**<br>**PDF**<br>**Pathformer**<br>**iTransformer**<br>**PatchTST**<br>**Crossformer**<br>**TimeMixer**<br>**NLinear**|Linear<br>Attention<br>Pre-processing<br>Latent<br>(Qiu et al., 2025)<br>Linear<br>Attn. & CNN<br>Pre-processing<br>Latent<br>(Dai et al., 2024)<br>Linear<br>Attention<br>Pre-processing<br>Latent<br>(Chen et al., 2024)<br>Linear<br>Attention<br>Pre-processing<br>Latent<br>(Liu et al., 2024b)<br>Linear<br>Attention<br>Pre-processing<br>Latent<br>(Nie et al., 2023)<br>Linear<br>Attention<br>Pre-processing<br>Latent<br>(Zhang & Yan, 2023)<br>Linear<br>MLP<br>Pre-processing<br>Sub-sampling<br>(Wang et al., 2024)<br>Linear<br>Norm.<br>Pre-processing<br>None<br>(Zeng et al., 2022)|
|_TimesNet_<br>_FITS_<br>_FEDformer_<br>**DLinear**<br>**Triformer**<br>_MICN_<br>_FiLM_<br>_Informer_<br>_NS Transformer_<br>TCN<br>RNN|Linear<br>CNN<br>Pre-Post-processing<br>None<br>(Wu et al., 2023)<br>Linear & 0-padding<br>Filtering<br>Pre-processing<br>Filtering<br>(Xu et al., 2024)<br>Mean & 0-padding<br>Attention<br>Pre-post-processing<br>None<br>(Zhou et al., 2022a)<br>Linear<br>Seas.-trend<br>Pre-processing<br>None<br>(Zeng et al., 2022)<br>Linear<br>Attention<br>Pre-processing<br>Compression to 1<br>(Cirstea et al., 2022)<br>Linear & 0-padding<br>CNN<br>Pre-post-processing<br>Sub-sampling<br>(Wang et al., 2023)<br>Legendre discr.<br>SSM & SConv.<br>Pre-Post-processing<br>Compression to 1<br>(Zhou et al., 2022b)<br>0-padding<br>Attention<br>Pre-post-processing<br>None<br>(Zhou et al., 2021)<br>0-padding<br>Attention<br>Pre-post-processing<br>None<br>(Liu et al., 2023)<br>AR<br>CNN<br>Pre-Processing<br>Sub-sampling<br>(Bai et al., 2018)<br>AR<br>RNN<br>Pre-processing<br>Compression to 1<br>(Elman, 1990)|



We thus identify, directly in Table 1, two _feature-based_ groups: in green/bold, models with two green
features and, in magenta/italic, models with at least one magenta feature. They almost coincide with
the performance-based groups. [3] We thus derive two observations: **1.** a (partially) non-learnable `DYN`
function drowns the performance, and **2.** a `PRO` function for pre-processing just before the final `DYN`
function drives the performance.


We derive these two observations into two research questions (RQ) to validate them experimentally:


    - ( **RQ1** ) Can we enhance model performance by adding a full learnable dynamics?

    - ( **RQ2** ) Is `DYN` -(Pre-processing) the best-performing configuration? If so, why?


**(RQ1) Dynamics addition** We choose to study Informer (Zhou et al., 2021), FiLM (Zhou et al.,
2022b), MICN (Wang et al., 2023), and FEDformer (Zhou et al., 2022a), for their diverse performance,
`DYN` functions, and backbones. We incorporate full learnable dynamics for prediction in these models
by adding a linear `DYN` layer, while keeping the original model structures (see Figure 2):


    - for Informer, a Transformer-based model, we feed the decoder with the encoder output,
which has gone through a linear `DYN` layer. It replaces the zero-padding,

    - for FiLM, an SSM-based model, we add a linear `DYN` layer after the normalization step.
FiLM turns into a `PRO` post-processing block,

    - for MICN, a CNN-based model, we feed the seasonal block with the seasonal component
processed by the trend block, which is a linear `DYN` layer, replacing the zero-padding,

    - for FEDformer, a Transformer-based model, we add a linear `DYN` layer before the encoder
embedding layer, while recomputing the end of the input **X** _trunc_ to fit the original decoder
temporal embedding size. The decoder input (zero-padding for the seasonality **S** and input
mean for the trend **T** ) is not changed, but Keys ( **K** ) and Values ( **V** ) are now computed from
initial predictions performed by the added learnable `DYN` function.


Better performances of the `DYN` versions of the chosen models would answer positively to RQ1. The
variety of studied models and locations to incorporate dynamics would validate the generality of
dynamics considerations.


**(RQ2) (Post-processing)-** `DYN` **configuration** We identify three well-performing foundation models:
iTransformer (Liu et al., 2024b), an encoder-only model where time and variate dimensions are


3 Adding the `PRO` backbone and the `PRO` time dimension changes columns in the analysis would better
characterize the groups, see conclusion.


5


Figure 2: RQ1 models with now full learnable dynamics capabilities. `DYN` is Linear dynamics layer,
ENC-DEC is encoder-decoder, MHD is multi-scale hybrid decomposition. Tilde is a prediction
approximation by `DYN`, _trunc_ subscript is input start tokens.


inverted, PatchTST (Nie et al., 2023), and Crossformer (Zhang & Yan, 2023), patching-based models
along the time dimension in an encoder-only and encoder-decoder architecture, respectively. In each
model, we add a learnable linear `DYN` layer just before the embedding one, without removing the
linear `DYN` layer at the end (which becomes a linear `PRO` layer), to keep the same original architecture.
Only the `DYN` output feeds PatchTST and Crossformer, while part of the input is concatenated to it to
feed iTransformer. A performance drop in the modified models would answer positively to RQ2.


**4** **Experiments**


We conduct extensive experiments to answer RQ1 and RQ2. Modified models in RQ1 are referred to
as `DYN` added models, while ones in RQ2 are referred to as post-processing models. Original models
are referred to as vanilla.


**4.1** **Experimental setup**


**Datasets** We consider TSF on the 25 datasets of TFB benchmark (Qiu et al., 2024), including the
well-established ETTs, Exchange, Weather, Electricity, ILI, Traffic, and Solar datasets (Wu et al.,
2021). Details on datasets are shown in Appendix C.


**Settings** For both RQs, we compute the Mean Square Error (MSE) and Mean Average Error
(MAE) across each dataset and forecasting horizon ( 200 scores per model) for the modified model
and compare them to the vanilla results obtained in the benchmark (Qiu et al., 2024). We keep
the same architecture hyperparameters as their vanilla versions. We only adjust by hand learning
hyperparameters (epochs, learning rate, patience) and also apply them to their vanilla versions for
fair comparison. Each configuration can be found in the code, and the implementation details are in
Appendix D. Raw results can be found in Appendix E.1, and prediction visualizations in Appendix F.


**Results** We count the number of cases our updated models are better, equal, worse by at most 1%
(low degradation), or worse by at least 1%, than their vanilla version. For MSE and MAE, the lower,
the better. Global distributions are shown in Figure 3. For RQ1 `DYN` added models, we compute the
p-values of the unilateral Wilcoxon test to assess if the MSE and MAE are lower than the vanilla
versions with statistical significance (p-value _<_ 0 _._ 05 ). For RQ2 models, we perform the opposite test
to assess if the vanilla versions are better. Detailed results can be found in Appendix E.2. In addition,
for RQ1, we compute the relative performance to NLinear of the `DYN` and vanilla versions to analyze
the quantitative comparison against the basic model reference, shown in Figure 4.


**4.2** **First analysis**


**RQ1** From Figure 3, all `DYN` added models **are better or comparable in more than** 80% **of the**
**cases** than their vanilla versions, and **better with statistical significance** on at least one metric. In


6


Figure 3: Global performance distribution of the modified models. A name is underlined (resp.
double-underlined) when the `DYN` added model (left) is statistically better than its vanilla version on
either MSE or MAE (resp. both). Similarly, it is overlined (resp. double) when the vanilla model is
statistically better than the post-processing version (right) on one (resp. both) metric.


Figure 4: Comparison between `DYN` added models and their vanilla version against NLinear
performances. Each point is ( _x_ ; _y_ ) : (Rel_perf(Vanilla _|_ NLinear); Rel_perf( `DYN` _|_ NLinear)), with
score(NLinear) _−_ score(Model)
Rel_perf(Model _|_ NLinear) = score(NLinear) where score is MSE or MAE. The higher

Rel_perf indicator is, the better. Each model Rel_perf mean is shown on its axis. The average
gain is mean( _y −_ _x|y > x_ ), while the average loss is mean( _y −_ _x|y < x_ ) . Some outliers are removed
for visualization and consistency, see Figure 8 with the outliers in Appendix E.2.


.


7


particular, Informer and FiLM are greatly improved by the linear `DYN` layer addition. With just a data
flow update, MICN gets better or equal scores 66% of the time. Moving to Figure 4, **average relative**
**performance improves by at least** 7% with `DYN` added models. FiLM `DYN` gets slightly better results
than NLinear on average. For every model, **the absolute gain is greater than the absolute loss** . The
simpler the original `DYN` function is, the greater the average gain is with linear `DYN` addition, which is
coherent. All the above results seem to **support having full learning dynamics capabilities** for TSF.
However, `DYN` models (except FiLM) are still worse than NLinear, possible reasons are: keeping the
same hyperparameters is constraining, or `DYN` models are not in a pre-processing configuration (not
feasible to do so while keeping the same original architecture).


**RQ2** From Figure 3, PatchTST and Crossformer as post-processing units get worse results with
statistical significance, confirming the pre-processing-like adopted architecture. However, postprocessing iTransformer is only statistically worse on one metric, with better or equal results in 51%
of the time, supporting the possibility of using such models as post-processing blocks.


**Intermediate conclusion** Overall, current results seem to answer positively to both RQs. However,
adding a linear layer comes with a slight parameter addition and data length modification, which
have an impact on performance. The modified models (except Informer and MICN) process inputs
of different lengths than vanilla ones. A model with more points to deal with should get better
results (Zeng et al., 2022). We thus conduct an ablation study to identify the performance drivers.


**4.3** **Ablation study**


**Compare like with like** To study parameter addition side-effect, we compare `DYN` added models to
their `PRO` version, where the added linear `DYN` layer is replaced by a feed-forward one which does not
change the time dimension, turning it into a linear `PRO` layer. MICN is excluded here as no layer is
added. For Informer `PRO`, we either pad with zeros if _H > L_ or truncate the output if _H < L_ to fit
the decoder input dimension. In `PRO` versions, dynamics modeling is the same as vanilla ones.


**Data length influence** To study the data-length influence, we condition `DYN` VS `PRO` and Postprocessing VS Vanilla comparisons on three possible setups: ( _H > L_ ), ( _H_ = _L_ ), and ( _H < L_ ), and
analyze distribution shifts. For the ( _H_ = _L_ ) setup, `DYN` and `PRO` are the same, except for FEDformer,
where temporal embeddings depend on timestamp values and **X** [˜] _trunc_ is fed to the decoder in addition.
For the `DYN` VS `PRO` case, if the distributions are symmetric between ( _H > L_ ) and ( _H > L_ ), the
performance would be driven by data-length variation. If distributions are not symmetric and in favor
of `DYN` model, then the performance gain is mainly driven by the learnable dynamics capabilities.
Otherwise, it would be parameter addition.


**Results** To perform the comparison, `PRO` versions are trained with the same hyperparameters as
`DYN` ones. We count the number of cases when each `DYN` (resp. post-processing) model is better,
equal (iso), or worse than its `PRO` (resp. vanilla) version on MSE and MAE. Global and conditioned
distributions with p-values are shown in Figures 5 and 6. Detailed results are shown in Appendix E.3.
Conditioned comparisons between `DYN` added models (resp. post-processing) and their vanilla (resp.
`PRO` ) version are presented in Appendix E.4. Prediction visualizations are shown in Appendix F.


**RQ1** From Figure 5, for Informer and FEDformer, overall, `DYN` versions are statistically better
than their `PRO` version. There isn’t any symmetry between distributions for ( _H > L_ ) and ( _H < L_ ) .
For Informer, `DYN` and `PRO` are statistically similar on ( _H < L_ ) while `DYN` should be disadvantaged.
For FEDformer `DYN` is statistically better in both setups. It **confirms that the performance mainly**
**comes from the dynamics** . For ( _H_ = _L_ ), FEDformer `DYN` and `PRO` are statistically similar: the
timestamp embedding, in line with (Zeng et al., 2022), doesn’t influence the performance and the
recomputed **X** [˜] _trunc_ doesn’t give any advantage. On the contrary, for FiLM, the `DYN` version is not
statistically better than the `PRO` one, with a symmetry in the distributions: the performance comes
from both parameter addition and data-length variation. Indeed, the `DYN` layer **could be in conflict**
**with its SSM encoding part**, which is also defined to learn a dynamics (Gu & Dao, 2024).


**RQ2** From Figure 6, vanilla models for PatchTST and Crossformer are still better than postprocessing versions in a majority of setups, with a clear dominance when ( _H < L_ ) . PatchTST is


8


Figure 5: `DYN` model performance distribution against their `PRO` version with setup conditioning. As
in Figure 3, a setup is underlined (resp. double-underlined) when the `DYN` model is statistically better
than the `PRO` one on either (resp. both) MSE or MAE.


Figure 6: Post-processing model performance distribution against their vanilla version with setup
conditioning. _Whole_ bar is the same distribution as in Figure 3, where _Worse_ _≤_ 1% and _Worse_ _>_ 1%
are put together under _Worse_ . Again, a setup is overlined (resp. double-overlined) when the vanilla
version is statistically better than the post-processing one on either (resp. both) MSE or MAE.


better when ( _H_ = _L_ ) due to the parameter addition, while in Crossformer, it gets in conflict with
the embedding layer. For iTransformer, it reveals that the vanilla version goes from slight failure
to statistically significant dominance from ( _H > L_ ) to ( _H < L_ ) . Overall, vanilla models struggle
a bit more when ( _H > L_ ) against post-processing versions while surpassing them with statistical
significance in the ( _H < L_ ) setup: predicting points based on a greater number of observations is
an advantageous setup **when there are learning dynamics capabilities**, while RQ1 `PRO` models,
without such capabilities, don’t dominate `DYN` models when ( _H < L_ ).


**5** **Conclusion and future work**


This work considers TSF models through the lens of dynamics. We propose the original `PRO-DYN`
nomenclature, identify the dynamics defined by LSTF-Linear models, then assess which features can
contribute the most to model performance, which seemed to be **1.** the ability to learn a dynamics,
**2.** located at the end of the model. We perform experiments that validate the hypothesis that **models**
**should be able to learn dynamics**, by supporting the identified features as performance drivers,
and showing that models with learning dynamics capabilities take **better advantage of a longer**
**look-back window** : they are powerful (Zeng et al., 2022).


We only study the impact of a Linear layer as `DYN` function, which considers time as a feature
dimension. Other `DYN` functions should be explored, such as autoregressive mechanisms, which apply
computations aligned with the time sequential aspect. We also mainly study Transformer backbones,
while SSM-based models learn dynamics where observed data is the input/output of an evolving
state-based system. Focus on SSM-based models through the `PRO-DYN` nomenclature should be
performed. In addition, while being crucial, our experiments against NLinear show that dynamics is
not the only factor driving the performance. `PRO` function backbones and computations along the time
dimension (see Table 1) seem to have an impact on performance. Finally, analyzing the influence of
the dataset domain, which would influence the underlying dynamics, should also be performed. All
these points are considered as future work for us.


9


**References**


James F. Allen. Maintaining knowledge about temporal intervals. _Commun. ACM_, 26(11):832–843,
November 1983. ISSN 0001-0782. doi: 10.1145/182.358434. URL `[https://doi.org/10.](https://doi.org/10.1145/182.358434)`
`[1145/182.358434](https://doi.org/10.1145/182.358434)` .


Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and
recurrent networks for sequence modeling, 2018. URL `[https://arxiv.org/abs/1803.01271](https://arxiv.org/abs/1803.01271)` .


Rishi Bommasani et al. On the opportunities and risks of foundation models, 2022. URL `[https:](https://arxiv.org/abs/2108.07258)`
`[//arxiv.org/abs/2108.07258](https://arxiv.org/abs/2108.07258)` .


Peng Chen, Yingying ZHANG, Yunyao Cheng, Yang Shu, Yihang Wang, Qingsong Wen, Bin Yang,
and Chenjuan Guo. Pathformer: Multi-scale transformers with adaptive pathways for time series
forecasting. In _The Twelfth International Conference on Learning Representations_, 2024. URL
`[https://openreview.net/forum?id=lJkOCMP2aW](https://openreview.net/forum?id=lJkOCMP2aW)` .


Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary
differential equations, 2019. URL `[https://arxiv.org/abs/1806.07366](https://arxiv.org/abs/1806.07366)` .


Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu, Daniel Tompkins, Zhuo Chen, Wanxiang Che,
Xiangzhan Yu, and Furu Wei. BEATs: Audio pre-training with acoustic tokenizers. In Andreas
Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan
Scarlett (eds.), _Proceedings of the 40th International Conference on Machine Learning_, volume
202 of _Proceedings of Machine Learning Research_, pp. 5178–5193. PMLR, 23–29 Jul 2023. URL
`[https://proceedings.mlr.press/v202/chen23ag.html](https://proceedings.mlr.press/v202/chen23ag.html)` .


Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine
reading, 2016. URL `[https://arxiv.org/abs/1601.06733](https://arxiv.org/abs/1601.06733)` .


Guillaume Chevillon. Direct multi-step estimation and forecasting. Documents de Travail de
l’OFCE 2005-10, Observatoire Francais des Conjonctures Economiques (OFCE), 2005. URL
`[https://ideas.repec.org/p/fce/doctra/0510.html](https://ideas.repec.org/p/fce/doctra/0510.html)` .


Razvan-Gabriel Cirstea, Chenjuan Guo, Bin Yang, Tung Kieu, Xuanyi Dong, and Shirui Pan.
Triformer: Triangular, variable-specific attentions for long sequence multivariate time series
forecasting. In Lud De Raedt (ed.), _Proceedings of the Thirty-First International Joint Con-_
_ference on Artificial Intelligence, IJCAI-22_, pp. 1994–2001. International Joint Conferences
on Artificial Intelligence Organization, 7 2022. doi: 10.24963/ijcai.2022/277. URL `[https:](https://doi.org/10.24963/ijcai.2022/277)`
`[//doi.org/10.24963/ijcai.2022/277](https://doi.org/10.24963/ijcai.2022/277)` . Main Track.


Tao Dai, Beiliang Wu, Peiyuan Liu, Naiqi Li, Jigang Bao, Yong Jiang, and Shu-Tao Xia. Periodicity
decoupling framework for long-term series forecasting. In _The Twelfth International Conference_
_on Learning Representations_, 2024. URL `[https://openreview.net/forum?id=dp27P5HBBt](https://openreview.net/forum?id=dp27P5HBBt)` .


Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan Mathur, Rajat Sen, and Rose Yu. Longterm forecasting with tide: Time-series dense encoder. _CoRR_, abs/2304.08424, 2023. doi:
10.48550/ARXIV.2304.08424. URL `[https://doi.org/10.48550/arXiv.2304.08424](https://doi.org/10.48550/arXiv.2304.08424)` .


Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,
and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale,
2021. URL `[https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)` .


Jeffrey L. Elman. Finding structure in time. _Cognitive Science_, 14(2):179-211, 1990. doi: 10.1207/
s15516709cog1402_1. URL `[https://doi.org/10.1207/s15516709cog1402_1](https://doi.org/10.1207/s15516709cog1402_1)` .


Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024.
URL `[https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)` .


Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. _Neural Comput._, 9(8):
1735–1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL
`[https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735)` .


10


Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom
Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy,
Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre.
Training compute-optimal large language models, 2022. URL `[https://arxiv.org/abs/2203.](https://arxiv.org/abs/2203.15556)`
`[15556](https://arxiv.org/abs/2203.15556)` .


Jiaxi Hu, Yuehong Hu, Wei Chen, Ming Jin, Shirui Pan, Qingsong Wen, and Yuxuan Liang. Attractor
memory for long-term time series forecasting: A chaos perspective, 2024. URL `[https://arxiv.](https://arxiv.org/abs/2402.11463)`
`[org/abs/2402.11463](https://arxiv.org/abs/2402.11463)` .


Yekun Ke, Yingyu Liang, Zhenmei Shi, Zhao Song, and Chiwun Yang. Curse of attention: A
kernel-based perspective for why transformers fail to generalize on time series forecasting and
beyond, 2025. URL `[https://arxiv.org/abs/2412.06061](https://arxiv.org/abs/2412.06061)` .


Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew
Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces, 2024.
URL `[https://arxiv.org/abs/2108.08481](https://arxiv.org/abs/2108.08481)` .


Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng
Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and
R. Garnett (eds.), _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL `[https://proceedings.neurips.cc/paper_files/paper/2019/](https://proceedings.neurips.cc/paper_files/paper/2019/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf)`
`[file/6775a0635c302542da2c32aa19d86be0-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf)` .


Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew
Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations,
2021. URL `[https://arxiv.org/abs/2010.08895](https://arxiv.org/abs/2010.08895)` .


Mengpu Liu, Mengying Zhu, Xiuyuan Wang, Guofang Ma, Jianwei Yin, and Xiaolin Zheng. Echo-gl:
Earnings calls-driven heterogeneous graph learning for stock movement prediction. _Proceedings_
_of the AAAI Conference on Artificial Intelligence_, 38(12):13972–13980, Mar. 2024a. doi: 10.1609/
aaai.v38i12.29305. URL `[https://ojs.aaai.org/index.php/AAAI/article/view/29305](https://ojs.aaai.org/index.php/AAAI/article/view/29305)` .


Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Exploring
the stationarity in time series forecasting, 2023. URL `[https://arxiv.org/abs/2205.14415](https://arxiv.org/abs/2205.14415)` .


Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long.
itransformer: Inverted transformers are effective for time series forecasting. In _The Twelfth_
_International Conference on Learning Representations_, 2024b. URL `[https://openreview.](https://openreview.net/forum?id=JePfAI8fah)`
`[net/forum?id=JePfAI8fah](https://openreview.net/forum?id=JePfAI8fah)` .


Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64
words: Long-term forecasting with transformers. In _The Eleventh International Conference on_
_Learning Representations_, 2023. URL `[https://openreview.net/forum?id=Jbdc0vTOcol](https://openreview.net/forum?id=Jbdc0vTOcol)` .


Shiyi Qi, Zenglin Xu, Yiduo Li, Liangjian Wen, Qingsong Wen, Qifan Wang, and Yuan Qi. Pdetime:
Rethinking long-term multivariate time series forecasting from the perspective of partial differential
equations, 2024. URL `[https://arxiv.org/abs/2402.16913](https://arxiv.org/abs/2402.16913)` .


Xiangfei Qiu, Jilin Hu, Lekui Zhou, Xingjian Wu, Junyang Du, Buang Zhang, Chenjuan Guo, Aoying
Zhou, Christian S Jensen, Zhenli Sheng, and Bin Yang. Tfb: Towards comprehensive and fair
benchmarking of time series forecasting methods. _Proc. VLDB Endow._, 17:2363 – 2377, 2024.


Xiangfei Qiu, Xingjian Wu, Yan Lin, Chenjuan Guo, Jilin Hu, and Bin Yang. Duet: Dual clustering
enhanced multivariate time series forecasting, 2025. URL `[https://arxiv.org/abs/2412.](https://arxiv.org/abs/2412.10859)`
`[10859](https://arxiv.org/abs/2412.10859)` .


M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: A deep learning
framework for solving forward and inverse problems involving nonlinear partial differential
equations. _Journal of Computational Physics_, 378:686–707, 2019. ISSN 0021-9991. doi:
https://doi.org/10.1016/j.jcp.2018.10.045. URL `[https://www.sciencedirect.com/science/](https://www.sciencedirect.com/science/article/pii/S0021999118307125)`
`[article/pii/S0021999118307125](https://www.sciencedirect.com/science/article/pii/S0021999118307125)` .


11


Keith Rayner. Eye movements in reading and information processing : 20 years of research.

                       _Psychological Bulletin_, 124(3):372 422, 1998. doi: 10.1037/0033-2909.124.3.372. URL `[https:](https://pubmed.ncbi.nlm.nih.gov/9849112/)`
`[//pubmed.ncbi.nlm.nih.gov/9849112/](https://pubmed.ncbi.nlm.nih.gov/9849112/)` .


Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, and Chandan K Reddy.
Llm-sr: Scientific equation discovery via programming with large language models, 2024. URL
`[https://arxiv.org/abs/2404.18400](https://arxiv.org/abs/2404.18400)` .


Mingtian Tan, Mike A. Merrill, Vinayak Gupta, Tim Althoff, and Thomas Hartvigsen. Are
language models actually useful for time series forecasting? In A. Globerson, L. Mackey,
D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), _Advances in Neu-_
_ral Information Processing Systems_, volume 37, pp. 60162–60191. Curran Associates,
Inc., 2024. URL `[https://proceedings.neurips.cc/paper_files/paper/2024/file/](https://proceedings.neurips.cc/paper_files/paper/2024/file/6ed5bf446f59e2c6646d23058c86424b-Paper-Conference.pdf)`
`[6ed5bf446f59e2c6646d23058c86424b-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2024/file/6ed5bf446f59e2c6646d23058c86424b-Paper-Conference.pdf)` .


Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,
U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), _Advances in Neural Information Processing Systems_, volume 30. Curran Associates,
Inc., 2017. URL `[https://proceedings.neurips.cc/paper_files/paper/2017/file/](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)`
`[3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)` .


Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
Bengio. Graph attention networks, 2018. URL `[https://arxiv.org/abs/1710.10903](https://arxiv.org/abs/1710.10903)` .


Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao. MICN:
Multi-scale local and global context modeling for long-term series forecasting. In _The Eleventh_
_International Conference on Learning Representations_, 2023. URL `[https://openreview.net/](https://openreview.net/forum?id=zt53IDUR1U)`
`[forum?id=zt53IDUR1U](https://openreview.net/forum?id=zt53IDUR1U)` .


Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Y. Zhang, and
Jun Zhou. Timemixer: Decomposable multiscale mixing for time series forecasting, 2024. URL
`[https://arxiv.org/abs/2405.14616](https://arxiv.org/abs/2405.14616)` .


Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. In M. Ranzato,
A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), _Advances in_
_Neural Information Processing Systems_, volume 34, pp. 22419–22430. Curran Associates,
Inc., 2021. URL `[https://proceedings.neurips.cc/paper_files/paper/2021/file/](https://proceedings.neurips.cc/paper_files/paper/2021/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Paper.pdf)`
`[bcc0d400288793e8bdcd7c19a8ac0c2b-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Paper.pdf)` .


Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet:
Temporal 2d-variation modeling for general time series analysis, 2023. URL `[https://arxiv.](https://arxiv.org/abs/2210.02186)`
`[org/abs/2210.02186](https://arxiv.org/abs/2210.02186)` .


Zhijian Xu, Ailing Zeng, and Qiang Xu. Fits: Modeling time series with 10 _k_ parameters, 2024. URL
`[https://arxiv.org/abs/2307.03756](https://arxiv.org/abs/2307.03756)` .


Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series
forecasting?, 2022. URL `[https://arxiv.org/abs/2205.13504](https://arxiv.org/abs/2205.13504)` .


Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency
for multivariate time series forecasting. In _The Eleventh International Conference on Learning_
_Representations_, 2023. URL `[https://openreview.net/forum?id=vSVLM2j9eie](https://openreview.net/forum?id=vSVLM2j9eie)` .


Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.
Informer : Beyond efficient transformer for long sequence time-series forecasting. _Proceedings Of_

                                        _The AAAI Conference On Artificial Intelligence_, 35(12):11106 11115, 2021. doi: 10.1609/aaai.
v35i12.17325. URL `[https://doi.org/10.1609/aaai.v35i12.17325](https://doi.org/10.1609/aaai.v35i12.17325)` .


Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer: Frequency
enhanced decomposed transformer for long-term series forecasting. In Kamalika Chaudhuri,
Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), _Proceedings of_


12


_the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine_
_Learning Research_, pp. 27268–27286. PMLR, 17–23 Jul 2022a. URL `[https://proceedings.](https://proceedings.mlr.press/v162/zhou22g.html)`
`[mlr.press/v162/zhou22g.html](https://proceedings.mlr.press/v162/zhou22g.html)` .


Tian Zhou, Ziqing Ma, xue wang, Qingsong Wen, Liang Sun, Tao Yao, Wotao Yin, and Rong
Jin. FiLM: Frequency improved legendre memory model for long-term time series forecasting.
In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), _Advances in_
_Neural Information Processing Systems_, 2022b. URL `[https://openreview.net/forum?id=](https://openreview.net/forum?id=zTQdHSQUQWc)`
`[zTQdHSQUQWc](https://openreview.net/forum?id=zTQdHSQUQWc)` .


13


