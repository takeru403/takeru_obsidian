## D IFFUSION MODELS FOR MULTIVARIATE SUBSURFACE

### GENERATION AND EFFICIENT PROBABILISTIC INVERSION



**Roberto Miele**

Institute of Earth Sciences
University of Lausanne
Lausanne, Switzerland

```
roberto.miele@unil.ch

```


**Niklas Linde**

Institute of Earth Sciences
University of Lausanne
Lausanne, Switzerland

```
niklas.linde@unil.ch

```


July 21, 2025


**A** **BSTRACT**


Diffusion models offer stable training and state-of-the-art performance for deep generative modeling
tasks. Here, we consider their use in the context of multivariate subsurface modeling and probabilistic inversion. We first demonstrate that diffusion models enhance multivariate modeling capabilities
compared to variational autoencoders and generative adversarial networks. In diffusion modeling,
the generative process involves a comparatively large number of time steps with update rules that
can be modified to account for conditioning data. We propose different corrections to the popular
Diffusion Posterior Sampling approach by Chung et al. (2023). In particular, we introduce a likelihood approximation accounting for the noise-contamination that is inherent in diffusion modeling.
We assess performance in a multivariate geological scenario involving facies and correlated acoustic
impedance. Conditional modeling is demonstrated using both local hard data (well logs) and nonlinear geophysics (fullstack seismic data). Our tests show significantly improved statistical robustness,
enhanced sampling of the posterior probability density function and reduced computational costs,
compared to the original approach. The method can be used with both hard and indirect conditioning data, individually or simultaneously. As the inversion is included within the diffusion process,
it is faster than other methods requiring an outer-loop around the generative model, such as Markov
chain Monte Carlo.


_**K**_ **eywords** Diffusion models _·_ Diffusion Posterior Sampling _·_ Bayesian inversion _·_ Geophysics _·_ Multivariate
modeling _·_ Subsurface characterization


**1** **Introduction**


Predicting the spatial distribution of subsurface properties is a key challenge in the Earth and environmental sciences.
Probabilistic inverse modeling [Grana et al., 2021, Rubin and Hubbard, 2005] aims to approximate the posterior
probability density function (pdf) by integrating prior pdfs with observed data in the form of a likelihood function

[Tarantola, 2005]. Conditioning data may consist of local measurements (e.g., well logs) or indirect geophysical data
describing averaged subsurface properties through linear or nonlinear physical forward operators. Prior pdfs expressed
as explicit statistical models (e.g., two-point geostatistical models) are computationally convenient, yet are often gross
simplifications of geological heterogeneity, leading to biased predictions and incomplete uncertainty quantification

[e.g., Linde et al., 2017, Gómez-Hernández and Wen, 1998, Neuweiler et al., 2011, Journel and Deutsch, 1993, Zhou
et al., 2018]. Alternatively, prior geological knowledge can be represented using 2D or 3D gridded data, defined as
training images (TI), which implicitly describe lower- and higher-order spatial relationships. Multiple-Point Statistics
(MPS) simulation methods [Guardiano and Srivastava, 1993, Strebelle, 2002, Mariethoz et al., 2010a, Mariethoz and
Caers, 2014] rely on TIs to draw geologically-realistic model realizations and have found wide applicability in both
hydro-geology [e.g., Høyer et al., 2017, Le Coz et al., 2017, Straubhaar et al., 2020] and reservoir characterization

[e.g., Melnikova et al., 2015, Caers and Zhang, 2004, & references therein]. These realizations can easily be made to


_Preprint submitted to Computers & Geosciences_


honor local observations [Straubhaar et al., 2016, Hansen et al., 2018, Straubhaar and Renard, 2021], but struggle with
high computational costs and poor posterior exploration when used in the context of nonlinear inversion [e.g., Zahner
et al., 2016, Levy et al., 2024, Laloy et al., 2016, Hansen et al., 2012, Mariethoz et al., 2010b].


The substantial progress of deep generative models over the past decade [Goodfellow et al., 2016] have driven significant interest to leverage such techniques in geoscience. Initial work considered convolution-based deep neural
networks, such as variational autoencoders [VAE; Kingma and Welling, 2013] and generative adversarial networks

[GAN; Goodfellow et al., 2014]. These methods can encode complex geological patterns of a TI through a lowdimensional manifold, defining prior pdf models [Laloy et al., 2017, 2018]. Therefore, inversion can be performed
within the latent space of such pre-trained networks [Laloy et al., 2017, 2018, Oliviero-Durmus et al., 2025] using sampling methods, such as Markov chain Monte Carlo [MCMC; Laloy et al., 2017, 2018, Levy et al., 2021, Mosser et al.,
2020, Liu et al., 2022] and Sequential Monte Carlo [SMC; Doucet et al., 2001, Amaya et al., 2021], ensemble-based
approaches [Canchumuni et al., 2019, Liu and Grana, 2020, Mo et al., 2020], deterministic optimization [Laloy et al.,
2019, Dupont et al., 2018], or variational inference [Chan and Elsheikh, 2019, Levy et al., 2023, Miele et al., 2024].
Contrarily to MPS algorithms, conditioning to local data remains cumbersome. One can train the generative models as
inference methods on specific conditional data, but this necessitates retraining for each application task [e.g. Mosser
et al., 2018, Zhang et al., 2021, Laloy et al., 2020, Feng et al., 2024, Miele and Azevedo, 2024]. Moreover, GANs are
prone to unstable training and mode collapse [e.g., Lucic et al., 2018], and the non-linearity of the generative process
often adversely affects the performance of the inversion [Laloy et al., 2019, Lopez-Alvis et al., 2021]. Formulations
based on VAEs are more stable, but offer rather lossy representations [e.g., Levy et al., 2023, Miele et al., 2024].


Diffusion models [DM; Ho et al., 2020, Sohl-Dickstein et al., 2015] represent the current state-of-the-art in deep
generative modeling, outperforming the accuracy of GANs without being affected by mode collapse or unstable training [e.g., Dhariwal and Nichol, 2021]. These methods are based on reversing a _data corruption_ process transforming
samples of a target distribution (the TI) into those of a known distribution (typically Gaussian). The generative process
involves iteratively denoising samples originating from this distribution, with the denoising trajectory being learned
by a deep neural network [e.g., Ho et al., 2020, Song et al., 2021a,b, Karras et al., 2022]. So far, relatively few works
studied DMs in the context of geological subsurface modeling. For example, Mosser [2023] highlighted that DM, in
an autoregressive formulation [Hoogeboom et al., 2022], share similarities to sequential geostatistical modeling, and
demonstrated promising multi-scale performance in modeling 2D realizations of binary fluvial channel distributions.
Xu et al. [2024] demonstrated accuracy of DM in an analogous geological scenario, and proposed a conditioning approach, where the DM is trained with local point observations incorporated as masked sparse data in additional input
channels. Diffusion models were also successfully applied by Aouf et al. [2025] for 2D and 3D modeling of complex porous geological media. The authors further performed conditional modeling with respect to total porosity, by
training a DM on embedded representation of the conditioning data. Lee et al. [2025] considered a similar conditional
approach with latent diffusion models [LDM; Rombach et al., 2022], to generate multi-facies realizations conditioned on well log data. Here, the diffusion and its conditioning are performed within the latent space of a pre-trained
encoder-decoder network. Ovanger et al. [2025] extend the work of Lee et al. [2025] by providing a thorough geostatistical assessment of LDM against truncated Gaussian random field modeling [Matheron et al., 1987, Mannseth,
2014]. The authors observed good performance for most of the metrics considered, but note significant biases limiting
a full prior representation and errors in the conditional realizations. Finally, Di Federico and Durlofsky [2025] train an
unconditioned LDM for facies modeling and combine it with an ensemble-based method for data assimilation in the
context of history matching. To the best of our knowledge, the latter is the only study to date dealing with subsurface
inverse problems using DMs.


Inverse modeling with DMs is of broad interest in computer vision and biomedical imaging; we refer to Daras et al.

[2024] and Zhao et al. [2025] for comprehensive overviews. Analogously to GANs and VAEs, direct training on conditioning data is a popular approach [e.g., Dhariwal and Nichol, 2021, Xu et al., 2024, Aouf et al., 2025, Lee et al., 2025,
Ho and Salimans, 2021, Whang et al., 2022, Liu et al., 2023], but implies computationally demanding re-training when
considering different data types or designs. Alternatively, one can rely on inference frameworks that iterate _outside_
the generative process of pre-trained unconditional DMs. Conventional sampling algorithms (e.g., MCMC) are often
inefficient in this setting, due to the DM’s large dimensionality and comparatively long generation times [Zhao et al.,
2025]; more efficient variational inference algorithms have been proposed for posterior approximation with surrogate
modeling, but they often struggle to capture complex or multimodal posterior distributions [e.g., Mardani et al., 2023,
Alkan et al., 2023, Feng and Bouman, 2024]. Ensemble-based approaches [e.g., Di Federico and Durlofsky, 2025]
share similar issues of comparatively high computational costs. Even if LDMs can reduce dimensionality, the associated encoder-decoder networks (e.g., VAE or GAN) introduce additional complications such as reduced modeling
ability [e.g., bias or smoothing; Ovanger et al., 2025] and non-linearities [Daras et al., 2024].


Another type of conditioning approach leverages the sequential nature of the denoising process. For example, SMC can
be used to progressively sample, at different denoising steps, a posterior distribution that evolves towards the posterior


2


_Preprint submitted to Computers & Geosciences_


pdf [e.g., Trippe et al., 2022, Cardoso et al., 2023, Wu et al., 2024, Dou and Song, 2024]. These asymptotically
exact methods work under the assumption of infinite sampling and linear physics. Moreover, the denoising process
itself can be adapted to achieve approximate Bayesian posterior sampling at lower computational cost [Song et al.,
2021b], provided that the intractable likelihood is approximated at each denoising step. The approximated likelihood
score is then combined with the pre-trained denoising score to steer the denoising generative process towards posterior
samples. Among numerous such approaches [see e.g., Daras et al., 2024], Diffusion Posterior Sampling (DPS)

[Chung et al., 2023] has gained wide popularity as it allows inference for both linear and nonlinear inverse problems.
Nonetheless, this class of conditional samplers are affected by their likelihood approximation considered [Daras et al.,
2024]. For instance, the accuracy of DPS decreases with decreasing conditioning data noise magnitude and increasing
nonlinearity of the problem at hand. Therefore, various adjustments to the theoretical framework are used in the
implementation of DPS, including ad-hoc weighting of the likelihood score. Several studies have highlighted these
limitations and propose partial improvements for linear inverse problems [e.g., Hamidi and Yang, 2025, Boys et al.,
2024, Yismaw et al., 2025, Song et al., 2023].


In this work, we consider DMs for multivariate subsurface geological modeling and inversion tasks, proposing corrections to the DPS approach. The improved likelihood approximation of our corrected DPS (CDPS) accounts for
the model’s denoising uncertainties; these are propagated into the data domain for likelihood evaluation, accounting
for linearized Gaussian approximations when considering nonlinear geophysics conditioning cases, following Friedli
and Linde [2024] and Linde et al. [2017]. The modified formulation removes the need to arbitrarily weight the likelihood score, and aims at increased robustness. We further highlight inconsistencies in the original implementation
of DPS by Chung et al. [2023] implemented with Denoising Diffusion Probabilistic and Implicit Modeling [DDPM
and DDIM; Ho et al., 2020, Song et al., 2021a] and suggest corrections. For our study, we use a state-of-the-art DM
implementation [EDM; Karras et al., 2022] to model a bivariate prior pdf describing sedimentary channel sequences
and correlated acoustic impedances. We first demonstrate substantial increases in prior modeling performance against
a GAN and a VAE previously introduced for analogous tasks [Miele et al., 2024]. Then, we use CDPS to condition
realizations on both local observations (well logs) and nonlinear geophysical data (fullstack seismic data), considering
Gaussian data noise of different magnitudes. We demonstrate that CDPS yields significantly improved accuracy and
enhanced robustness compared to DPS across all considered applications, and allows for a reduction in the required
number of denoising steps compared to DPS. Our results for the linear conditioning case compare well with those
sampled with the preconditioned Crank–Nicolson (pCN) MCMC algorithm [Cotter et al., 2013]. The CDPS can easily
be adapted to different DM formulations and data error levels. We further present the algorithm for DDPM and DDIM,
with example applications, in the Appendix.


**2** **Methodology**


**2.1** **Diffusion modeling**


Diffusion models enable deep generative modeling by transforming samples from a tractable distribution (often Gaussian) into samples from a formally unknown target pdf _p_ ( **x** 0 ) on R _[d]_ from which samples are available (the TI). The
diffusion process typically involves corrupting these samples, **x** 0 _∼_ _p_ ( **x** 0 ), through the progressive addition of Gaussian noise at different time-steps _t_, thereby defining the _forward process p_ ( **x** _t_ _|_ **x** _t−_ 1 ). After a sufficient time _T_, such
corrupted data distribution _p_ ( **x** _T_ ) can be considered Gaussian [Ho et al., 2020, Sohl-Dickstein et al., 2015]. The
_backward process_, _q_ ( **x** 0 _|_ **x** _T_ :1 ), represents the sampling or generative process, which is analytically intractable for an
arbitrary _p_ ( **x** 0 ). Numerical methods can be used instead, by training a deep neural network _S_ _θ_ ( _−_ ) [Ho et al., 2020,
Sohl-Dickstein et al., 2015].


A generalized continuous-time formulation of diffusion modeling, referred to as _score-based_ [e.g., Song et al., 2021b,
Song and Ermon, 2019], represents the forward process through stochastic differential equations (SDE) of the form

[Song et al., 2021b]
_d_ **x** _t_ = _f_ ( **x** _t_ _, t_ ) _dt_ + _g_ ( _t_ ) _d_ **W** _t_ _,_ (1)


where _f_ ( **x** _t_ _, t_ ) is a drift term, _g_ ( _t_ ) is a diffusion coefficient and **W** _t_ is a Wiener process (i.e., Brownian motion). The
reverse-time SDE for unconditional sampling is given by [Song et al., 2021b, Anderson, 1982]


_dx_ _t_ = � _f_ ( **x** _t_ _, t_ ) _dt_ + _g_ ( _t_ ) [2] _∇_ **x** _t_ log _p_ _t_ ( **x** _t_ )� _dt_ + _g_ ( _t_ ) _d_ **W** _t_ _,_ (2)


where _∇_ **x** _t_ log _p_ _t_ ( **x** _t_ ) is the score function of the noisy data distribution **x** _t_ . The generation of samples using Eq. 2 is
possible using Langevin dynamics, implying that a distribution of samples _p_ ( **x** 0 _|_ **x** _T_ ) can be obtained given a single
**x** _T_ . As an alternative, deterministic sampling can be performed using the ordinary differential equation (ODE) [Song


3


_Preprint submitted to Computers & Geosciences_



et al., 2021b]
_d_ **x** _t_




[(] _[t]_ [)] [2]

2 _∇_ **x** _t_ log _p_ _t_ ( **x** _t_ )� _,_ (3)



**x** _t_

_f_ ( **x** _t_ _, t_ ) _dt_ + _[g]_ [(] _[t]_ [)] [2]
_dt_ [=] � 2



which enables a bivariate mapping between **x** _T_ and **x** 0, and is referred to as _probability flow ODE_ . The diffusion can
be simulated within _Variance preserving_ (VP) (i.e., the added noise variance is bounded) or _Variance exploding_ (VE)
frameworks. For the latter, the forward process (Eq. 1) at a given time-step _t_ can be defined by


**x** _t_ = **x** 0 + _σ_ _t_ _z,_ _z ∼N_ (0 _, I_ ) _,_ (4)


where _σ_ _t_ represents the time-dependent noise magnitude.Using Tweedie’s formula [Robbins, 1956, Efron, 2011] the
score function is

_∇_ **x** _t_ log _p_ _t_ ( **x** _t_ ) = [E][[] **[x]** [0] _[|]_ **[x]** _[t]_ []] _[ −]_ **[x]** _[t]_ _,_ (5)

_σ_ _t_ [2]


where E[ **x** 0 _|_ **x** _t_ ] is a time-dependent conditional expectation of **x** 0 given **x** _t_ . Neural networks can be trained to parameterize the score function from noisy data and a given time _t_, _S_ _θ_ ( **x** _t_ _, t_ ), by minimizing the mean square error [Song
et al., 2021b]
_L_ ( _S_ _θ_ ) = E **x** 0 E **x** _t_ _|_ **x** 0 � _||S_ _θ_ ( **x** _t_ _, t_ ) _−∇_ **x** _t_ log _p_ _t_ ( **x** _t_ _|_ **x** 0 ) _||_ 2 [2] � (6)


or directly as a denoiser _S_ _θ_ ( **x** 0 + **z** _, t_ ), using [e.g., Karras et al., 2022]

_L_ ( _S_ _θ_ ) = E **x** 0 E **z** _∼N_ (0 _,σ_ 2 _I_ ) _||S_ _θ_ ( **x** 0 + **z** _, t_ ) _−_ **x** 0 ) _||_ 2 [2] _[.]_ (7)


The trained network is finally used in the backward process (Eq. 2) for generative (prior) modeling.


The original formulations for DMs are the discrete-time DDPM [Ho et al., 2020, Sohl-Dickstein et al., 2015] and its
generalization DDIM [Song et al., 2021a]. Here, _S_ _θ_ ( **x** _t_ _, t_ ) is trained to predict the noise in **x** _t_, _ϵ_ [(] _θ_ _[t]_ [)] [, and the generative]
process for both DDPM and DDIM is defined by



�



**x** _t−_ 1 = _[√]_ ~~_α_~~ _t−_ 1



**x** _t_ _−_ _[√]_ 1 _−_ _α_ ¯ _t_ _ϵ_ [(] _θ_ _[t]_ [)]
~~_√α_~~ ~~¯~~ _t_
�



¯ ˜

+ ~~�~~ 1 _−_ _α_ _t−_ 1 _−_ _σ_ _t_ [2] _[ϵ]_ [(] _θ_ _[t]_ [)] + ˜ _σ_ _t_ _ϵ_ _t_ _,_ (8)



where _β_ _t_ defines the noise schedule of the diffusion process, _α_ _t_ = 1 _−_ _β_ _t_, ˆ _α_ _t_ = [�] _[t]_ _s_ =1 _[α]_ _[s]_ [ and][ ˜] _[σ]_ _t_ [2] [is the conditional]
variance sequence (injected noise during sampling), and _ϵ_ _t_ _∼N_ (0 _, I_ ). DDIM and DDPM are discretizations of
score-based diffusion in continuous-time domain, with _ϵ_ [(] _[t]_ [)] = _∇_ **x** _t_ log _p_ _t_ ( **x** _t_ ) _/_ _[√]_ 1 _−_ _α_ ¯ _t_ [Dhariwal and Nichol, 2021,



Song et al., 2021b]. In DDPM ˜ _σ_ _t_ =
~~�~~



11 _−−αα_ ¯ _tt_ 1 _−_ _αα_ _t−t_ 1 [and the sampling process is associated to a VP SDE [Ho]
~~�~~



et al., 2020, Song et al., 2021b]. In DDIM, ˜ _σ_ _t_ can be arbitrarily changed to 0 so that Eq. 8 becomes a VE ODE for
deterministic sampling [Song et al., 2021a, Karras et al., 2022].


**2.2** **Diffusion Posterior Sampling (DPS)**


In Bayesian inversion, one is interested in characterizing or sampling from the posterior pdf _p_ ( **x** 0 _|_ **d** ) with the data
vector **d** given by
**d** = _F_ ( **x** 0 ) + _ϵ_ **d** _,_ (9)


where _F_ ( _−_ ) corresponds to a forward operator (either linear or nonlinear) and _ϵ_ **d** is the noise in the observed data. The
inversion problem can be formalized by Bayes’ theorem, for a constant model dimension, as _p_ ( **x** 0 _|_ **d** ) _∝_ _p_ ( **d** _|_ **x** 0 ) _p_ ( **x** 0 ),
where _p_ ( **d** _|_ **x** 0 ) is the likelihood function.


Unconditional modeling from _p_ ( **x** 0 ) can be achieved with a trained DM, using Eqs. 2 or 3. Following Bayes’ rule,
sampling from the posterior pdf can be achieved using the score function [Song et al., 2021b]


_∇_ **x** _t_ log _p_ _t_ ( **x** _t_ _|_ **d** ) = _∇_ **x** _t_ log _p_ _t_ ( **x** _t_ ) + _∇_ **x** _t_ log _p_ _t_ ( **d** _|_ **x** _t_ ) _,_ (10)


where the first term can be defined the trained neural network _S_ _θ_ ( **x** _t_ _, t_ ) and _∇_ **x** _t_ log _p_ _t_ ( **d** _|_ **x** _t_ ) is a noise-dependent
likelihood score. In the context of inverse modeling, we refer to _∇_ **x** _t_ log _p_ _t_ ( **x** _t_ ) as the _prior score_ . If _S_ _θ_ ( **x** _t_ _, t_ ) _≃_
_∇_ **x** _t_ log _p_ _t_ ( **x** _t_ ) (Eq. 6), then ˆ **x** [(] 0 _[t]_ [)] is derived using Tweedie’s formula (Eq. 5). Otherwise, it is directly obtained as

ˆ
**x** [(] 0 _[t]_ [)] = _S_ _θ_ ( **x** _t_ _, t_ ) (Eq. 7). The likelihood score is given by an intractable integral


_p_ _t_ ( **d** _|_ **x** _t_ ) = _p_ ( **d** _|_ **x** 0 ) _p_ ( **x** 0 _|_ **x** _t_ ) _d_ **x** 0 _._ (11)
�


4


_Preprint submitted to Computers & Geosciences_


Chung et al. [2023] note that for a given **x** _t_, an estimate of the expected mean for _p_ ( **x** 0 _|_ **x** _t_ ) is given by the denoiser

ˆ ˆ
**x** [(] 0 _[t]_ [)] = E[ **x** 0 _|_ **x** _t_ ] (Eq. 5); therefore, the likelihood can be approximated as _p_ ( **d** _|_ **x** _t_ ) _≃_ _p_ ( **d** _|_ **x** [(] 0 _[t]_ [)] [)][, and]


_∇_ **x** _t_ log _p_ ( **d** _|_ **x** _t_ ) _≃∇_ **x** _t_ log _p_ ( **d** _|_ ˆ **x** [(] 0 _[t]_ [)] [)] _[.]_ (12)


In practical implementations in DM, the score in Eq. 12 is computed by backpropagating the log-likelihood values
evaluated at each time step _t_ .


This approach is referred to as Diffusion Posterior Sampling (DPS), and was proposed by Chung et al. [2023] as a general inference framework for both linear and nonlinear inverse problems. Assuming uncorrelated and homoscedastic
Gaussian data errors, Chung et al. [2023] use the likelihood



_,_ (13)

�



_p_ ( **d** _|_ ˆ **x** [(] 0 _[t]_ [)] [) =] 1 exp
~~�~~ (2 _π_ ) _[n]_ _σ_ **d** ~~_[n]_~~



_−_ _[||][F]_ [(][ˆ] **[x]** [(] 0 _[t]_ [)] [)] _[ −]_ **[d]** _[||]_ 2 [2]

� 2 _σ_ **d** [2]



where _σ_ **d** is the standard deviation of the data noise _ϵ_ **d**, leading to

_∇_ **x** _t_ log _p_ ( **d** _|_ **x** _t_ ) _≃∇_ **x** _t_ log _p_ ( **d** _|_ ˆ **x** [(] 0 _[t]_ [)] [) =] _[ −]_ [1] _∇_ **x** _t_ _||F_ (ˆ **x** [(] 0 _[t]_ [)] [)] _[ −]_ **[d]** _[||]_ 2 [2] _[.]_ (14)

_σ_ **d** [2]


This approximation does not account for errors in the approximated ˆ **x** [(] 0 _[t]_ [)] [. In their implementation, Chung et al. [2023]]
propose a weight to the likelihood score, replacing _σ_ 1 **d** [2] [with an hyperparameter] _[ ρ]_ [, to account for the impact of the]
likelihood approximation used. In their accompanying code, the authors further replace the sum of square error in
Eq. 14 with the root-mean-square error (RMSE), justifying this choice as a solution for increased stability [see open
review in Chung et al., 2023]. Indeed, this arbitrary choice will drastically decrease the magnitude of the likelihood
score and, hence, decrease the weight given to the data in the sampling process.


The conditional score approximation used within DPS results in a too low variance of the generated samples, and the
sampling process has been rather associated to the maximization of a posterior [Xu et al., 2025]. Improvements to this
approximation should include the denoiser’s ( _x_ ˆ [(] 0 _[t]_ [)] [) uncertainty [Hamidi and Yang, 2025, Boys et al., 2024, Yismaw]
et al., 2025, Song et al., 2023, Peng et al., 2024]. Following Efron [2011], the denoiser’s variance can be computed
using Tweedie’s formula for the second moment

Var _{_ ˆ **x** [(] 0 _[t]_ [)] _[|]_ **[x]** _[t]_ _[}]_ [ =] _[ σ]_ **[t]** [(1 +] _[ σ]_ **[t]** _[∇]_ **x** [2] _t_ [log] _[ p]_ _[t]_ [(] **[x]** _[t]_ [))] _[,]_ (15)


where _∇_ [2] **x** _t_ [log] _[ p]_ _[t]_ [(] **[x]** _[t]_ [)][ is the Hessian matrix of the score of the data distribution] **[ x]** _[t]_ [. Using the Hessian directly is compu-]
tationally demanding, as its calculation scales exponentially with the model’s dimensionality [Boys et al., 2024]. The
Covariance-Aware Diffusion Posterior Sampling [Hamidi and Yang, 2025] for linear inversion uses an approximation
of the Hessian, computed as the difference between the scores at consecutive and sufficiently small time-steps (i.e.,
using the finite difference method). The ΠGDM [Song et al., 2023] and CoDPS [Yismaw et al., 2025] methods use
time-step dependent heuristic approximations of the denoiser’s standard deviation for linear inverse problems, representing significantly faster solutions. Peng et al. [2024] derive a formulation for optimal covariance quantification for
specific DM networks able to predict the conditional variance used in DDIM ( _σ_ ˜ _t_ in Eq. 8). For more general applications, they suggest to estimate the denoiser variance directly from the denoising errors of the trained network, using
known test data. In the following, we follow the latter approach, and introduce a general and consistent formulation
for approximate posterior sampling for both linear and nonlinear conditioning.


**2.2.1** **Corrected Diffusion Posterior Sampling (CDPS)**


The proposed corrected DPS (CDPS), in which the likelihood formulation accounts for the denoising-step dependent
error _ϵ_ ˆ [(] _[t]_ [)] ˆ
**x** 0 [of the trained model. The] _[ ϵ]_ **x** [(] _[t]_ 0 [)] [is the largest at early steps of the backward process and decrease with]
decreasing _σ_ **t** . The forward problem at time _t_ can be written as (c.f., Eq. 9).

**d** = _F_ (ˆ **x** [(] 0 _[t]_ [)] + _ϵ_ ˆ [(] **x** _[t]_ 0 [)] [) +] _[ ϵ]_ **[d]** (16)


Assuming an ideal denoiser, we assume _ϵ_ ˆ [(] **x** _[t]_ 0 [)] [to be spatially uncorrelated and Gaussian-distributed [Yismaw et al.,]
2025, Song et al., 2023, Peng et al., 2024]. We evaluate, post-training, the standard deviation of the error for each
data dimension (i.e., cells or pixels), using different test data samples _i_ corrputed with known noise, as _σ_ _x_ [(] ˆ _[t]_ 0 [)] _,i_ _[≃]_
� ~~�~~ _Ni_ =1 [(ˆ] _[x]_ 0 [(] _[t]_ _,i_ [)] _[−]_ _[x]_ [0] _[,i]_ [)] [2] _[/N]_ [. In fact, the error distribution depends on the spatial features of the input data, which is]


5


_Preprint submitted to Computers & Geosciences_


computable only through Hessian estimation [Hamidi and Yang, 2025, Boys et al., 2024, Efron, 2011] or using a
network predicting conditional covariance [Peng et al., 2024, ; see Section 2.2]. We assume error homoscedasticity,
using the mean value of _σ_ _x_ [(] ˆ _[t]_ 0 [)] [from all the pixels.]


In agreement with Linde et al. [2017] and Friedli and Linde [2024] who studied approximations to intractable likelihoods arising in geophysics, we propagate the errors through the forward model using a first-order Taylor expansion
**x** _�→F_ ( **x** ) around ˆ **x** [(] 0 _[t]_ [)] [,]
**d** _≈F_ (ˆ **x** [(] 0 _[t]_ [)] [) +] _[ J]_ ˆ **x** [(] 0 _[t]_ [)] _[ϵ]_ ˆ [(] **x** _[t]_ 0 [)] [+] _[ ϵ]_ **[d]** _[,]_ (17)


where _J_ ˆ **x** (0 _t_ ) is the corresponding Jacobian matrix of the forward operator. Assuming Gaussian distributions _p_ ( _ϵ_ ˆ [(] **x** _[t]_ 0 [)] [) =]

_N_ (0 _, σ_ ˆ **x** [2] 0 [(] _[t]_ [)] **[I]** [) =] _[ N]_ [(0] _[,]_ [ Σ] ˆ **x** [(] _[t]_ 0 [)] [)][ and] _[ p]_ [(] _[ϵ]_ **[d]** [) =] _[ N]_ [(0] _[, σ]_ **d** [2] **[I]** [) =] _[ N]_ [(0] _[,]_ [ Σ] **[d]** [)][, the likelihood] _[ p]_ [(] **[d]** _[|]_ [ˆ] **[x]** 0 [(] _[t]_ [)] [)][ can be approximated as]


_p_ ( **d** _|_ **x** _t_ ) _≃_ _p_ ( **d** _|_ ˆ **x** [(] 0 _[t]_ [)] [) =] _[ N]_ [(0] _[,]_ [ ˜Σ] [(] **d** _[t]_ [)] [)] _[,]_ (18)


where Σ [˜] [(] **d** _[t]_ [)] = _J_ ˆ **x** _[T]_ [(] 0 _[t]_ [)] [Σ] ˆ **x** [(] _[t]_ 0 [)] _[J]_ ˆ **x** [(] 0 _[t]_ [)] + Σ **d** (see Friedli and Linde [2024] for further details). For a linear operator (e.g., local

conditioning), the full covariance matrix in Eq. 18 is obtained without any approximations, using matrix multiplication
with the linear operator. Finally, we re-write the likelihood score (Eq. 14) as


_∇_ **x** _t_ log _p_ ( **d** _|_ **x** _t_ ) _≃∇_ **x** _t_ log _p_ ( **d** _|_ ˆ **x** [(] 0 _[t]_ [)] [) =] _[ −∇]_ **[x]** _t_ ( _F_ (ˆ **x** [(] 0 _[t]_ [)] [)] _[ −]_ **[d]** [)] _[T]_ [ ˜Σ] _[−]_ **d** [1] [(] _[t]_ [)] ( _F_ (ˆ **x** [(] 0 _[t]_ [)] [)] _[ −]_ **[d]** [)] _._ (19)
� �


This is the main adaptation introduced by CDPS. We further observe another source of error in the original DPS
implementation on DDPM [Chung et al., 2023]. At each step, the authors use the likelihood score to obtain the
conditional denoised image as **x** _t−_ 1 = **x** _[′]_ _t−_ 1 _[−]_ _[ρ][∇]_ **[x]** _t_ [log] _[ p]_ [(] **[d]** _[|]_ [ˆ] **[x]** [(] 0 _[t]_ [)] [)][, where] **[ x]** _t_ _[′]_ _−_ 1 [is the denoised image considering the]
prior score, obtained with Eq. 8 as is. However, this is inconsistent with the appliaction of Bayes theorem [e.g., Eq.
10; Song et al., 2021b], according to which Eq. 8 in the conditioning case should be



�



**x** _t−_ 1 = _[√]_ ~~_α_~~ _t−_ 1



**x** _t_ _−_ _[√]_ 1 _−_ _α_ ¯ _t_ ˆ _ϵ_ [(] _θ_ _[t]_ [)]
~~_√α_~~ ~~¯~~ _t_
�



¯

+ ~~�~~ 1 _−_ _α_ _t−_ 1 _−_ _σ_ _t_ [2] _[ϵ]_ [ˆ] [(] _θ_ _[t]_ [)] + _σ_ _t_ _ϵ_ _t_ _._ (20)



¯ ˆ
where ˆ _ϵ_ [(] _θ_ _[t]_ [)] = _ϵ_ [(] _θ_ _[t]_ [)] _−_ _[√]_ 1 _−_ _α_ _t_ _∇_ **x** _t_ log _p_ ( **d** _|_ **x** [(] 0 _[t]_ [)] [)][, following the approach for guided diffusion modeling proposed by]
Dhariwal and Nichol [2021]. In practice, even if one does not consider the error in the approximation ˆ **x** [(] 0 _[t]_ [)] [, the]
likelihood score _∇_ **x** _t_ log _p_ ( **d** _|_ ˆ **x** [(] 0 _[t]_ [)] [)][ should be rescaled to the diffusion noise magnitudes rather than a fixed arbitrary] _[ ρ]_ [.]
We suggest that this error may be an additional cause of instability of the DPS method and provide an implementation
of the CDPS for DDPM and DDIM in the Appendix C, implementing this correction.QUAQUA The CDPS algorithm
for score-based diffusion is given in Algorithm 1, specifically within the EDM diffusion framework [Karras et al.,
2022]. Further details on EDM are provided in Section 2.3.


**2.3** **Multivariate diffusion modeling**


The simultaneous sampling of multivariate subsurface properties is carried out within a single generative process. We
do not consider formulations involving dimensionality reduction with LDMs [Rombach et al., 2022]. We adopt a VE
score-based DM framework, often referred to as EDM [Karras et al., 2022], and use its probability flow ODE for
unconditional modeling [Karras et al., 2022]:


_d_ **x** = _−_ _[σ]_ _[t]_ (21)

_dt_ _[σ]_ _[t]_ _[∇]_ **[x]** _[t]_ [ log] _[ p]_ _[t]_ [(] **[x]** _[t]_ [)] _[.]_


For posterior sampling (Eq. 10), we substitute the score with _∇_ **x** _t_ log _p_ _t_ ( **x** _t_ _|_ **d** ) using our proposed likelihood approximation (Eq. 19). Sampling is performed evaluating the ODE at noise levels following the schedule [Karras et al.,
2022]

_i_ _ρ_
_σ_ _i>_ 0 = � _σ_ _max_ [1] _[/ρ]_ [+] _N −_ 1 [(] _[σ]_ _min_ [1] _[/ρ]_ _[−]_ _[σ]_ _max_ [1] _[/ρ]_ [)] � _and_ _σ_ _N_ = 0 _,_ (22)


where _σ_ _max_ and _σ_ _min_ are the maximum and minimum noise levels in the corrupted data, _i_ is the step number, _i ∈_
_{_ 1 _, ..., N_ _}_, and _ρ_ is an exponential factor. In agreement with the authors original choices, _σ_ _max_ = 80, _σ_ _min_ = 0 _._ 002
and _ρ_ = 7 provided the best performance; the motivation of these parameter choices is explained by Karras et al.


6


_Preprint submitted to Computers & Geosciences_


**Algorithm 1** Corrected DPS (CDPS) algorithm within the EDM diffusion modeling framework by Karras et al. [2022]

1: **Require:** trained network _S_ _θ_ ( _−_ ), conditioning data **d**, forward operator _F_, maximum number of time steps _N_,
noise schedule _σ_ _i_, limits _σ_ _max_ and _σ_ _min_


2: **for** _σ_ _t_ = 0 _, ..., σ_ _max_ **do**
3: **sample x** 0 _∼_ _q_ _data_
4: **x** _t_ _←_ **x** 0 + _σ_ **t** **z** _,_ **z** _∼N_ (0 _, I_ )

5: ˆ **x** [(] 0 _[t]_ [)] _←_ _S_ _θ_ ( **x** _t_ _, t_ )


_N_
6: _σ_ _x_ [(] ˆ _[t]_ 0 [)] _[←]_ [E] � ~~��~~ _i_ =1 [(ˆ] _[x]_ 0 [(] _[t]_ _,i_ [)] _[−]_ _[x]_ [0] _[,i]_ [)] [2] _[/N]_ �

7: **end for**


8: **sample x** _T_ _∼N_ (0 _, I_ )
9: **for** _i = 0,..., N_ **do**
10: _σ_ _t_ _←_ _σ_ _i_
11: ˆ **x** [(] 0 _[t]_ [)] _←_ _S_ _θ_ ( **x** _t_ _, σ_ _t_ )
12: _∇_ _x_ _t_ log _p_ _t_ ( **x** _t_ ) _←_ ( _S_ _θ_ ( **x** _t_ _, σ_ _t_ ) _−_ **x** _t_ ) _/σ_ **t** [2]
13: Σ ˆ [(] **x** _[t]_ 0 [)] _[←]_ _[σ]_ ˆ **x** [2] 0 [(] _[t]_ [)]
14: **if** _F_ is nonlinear **then**
15: _J_ ˆ **x** 0 _←J_ _F_ (ˆ **x** [(] 0 _[t]_ [)] [)]

16: ˜Σ **d** _←J_ ˆ **x** _[T]_ [(] 0 _[t]_ [)] [Σ] ˆ **x** [(] _[t]_ 0 [)] _[J]_ ˆ **x** [(] 0 _[t]_ [)] + Σ **d**

17: **else**
18: ˜Σ **d** _←F_ _[T]_ Σ ˆ [(] **x** _[t]_ 0 [)] _[F]_ [ + Σ] **[d]**
19: **end if**
20: _∇_ _x_ _t_ log _p_ _t_ ( **d** _|_ ˆ **x** [(] 0 _[t]_ [)] [)] _[ ←−∇]_ **[x]** _t_ ( _F_ (ˆ **x** [(] 0 _[t]_ [)] [)] _[ −]_ **[d]** [)] _[T]_ [ ˜Σ] _[−]_ **d** [1] [(] _[t]_ [)] ( _F_ (ˆ **x** [(] 0 _[t]_ [)] [)] _[ −]_ **[d]** [)]
� �

21: _∇_ _x_ _t_ log _p_ _t_ (ˆ **x** [(] 0 _[t]_ [)] _[|]_ **[d]** [)] _[ ←∇]_ _[x]_ _t_ [log] _[ p]_ _[t]_ [(] **[d]** _[|]_ [ˆ] **[x]** [(] 0 _[t]_ [)] [) +] _[ ∇]_ _[x]_ _t_ [log] _[ p]_ _[t]_ [(] **[x]** _[t]_ [)]

22: _x_ _t−_ 1 _←_ _x_ _t_ + ( _σ_ _t_ _−_ _σ_ _t−_ 1 ) _∇_ _x_ _t_ log _p_ _t_ (ˆ **x** [(] 0 _[t]_ [)] _[|]_ **[d]** [)]
23: **if** _σ_ **x** ( _t_ ) _̸_ = 0 **then**

24: _∇_ _x_ _t_ log _p_ _t_ ( **d** _|_ ˆ **x** [(] 0 _[t]_ [)] [)] _[′]_ _[ ←−∇]_ **[x]** _t_ ( _F_ (ˆ **x** [(] 0 _[t]_ [)] [)] _[ −]_ **[d]** [)] _[T]_ [ ˜Σ] _[−]_ **d** [1] [(] _[t]_ [)] ( _F_ (ˆ **x** [(] 0 _[t]_ [)] [)] _[ −]_ **[d]** [)]
� �

25: _∇_ _x_ _t_ log _p_ _t_ (ˆ **x** [(] 0 _[t]_ [)] _[|]_ **[d]** [)] _[′]_ [ =] _[ ∇]_ _[x]_ _t_ [log] _[ p]_ _[t]_ [(] **[d]** _[|]_ [ˆ] **[x]** [(] 0 _[t]_ [)] [) +] _[ ∇]_ _[x]_ _t_ [log] _[ p]_ _[t]_ [(] **[x]** _[t]_ [)]



26: _x_ _t−_ 1 _←_ _x_ _t_ + ( _σ_ _t_ _−_ _σ_ _t−_ 1 )( [1] 2 _[∇]_ _[x]_ _[t]_ [log] _[ p]_ _[t]_ [(ˆ] **[x]** [(] 0 _[t]_ [)] _[|]_ **[d]** [) +] 2 [1] _[∇]_ _[x]_ _[t]_ [log] _[ p]_ _[t]_ [(ˆ] **[x]** [(] 0 _[t]_ [)] _[|]_ **[d]** [)] _[′]_ [)]

27: **end if**

28: **end for**
29: **Output:** ˜ **x** 0 _∼_ _p_ ( **x** 0 _|_ **d** )




[1] 2 _[∇]_ _[x]_ _[t]_ [log] _[ p]_ _[t]_ [(ˆ] **[x]** [(] 0 _[t]_ [)] _[|]_ **[d]** [) +] 2 [1]



26: _x_ _t−_ 1 _←_ _x_ _t_ + ( _σ_ _t_ _−_ _σ_ _t−_ 1 )( [1]




[2022]. The ODE is evaluated using the Heun’s 2 [nd] order method [Ascher and Petzold, 1998] (lines 23-36 in Algorithm
1).


The network _S_ _θ_ ( **x** _t_ _, t_ ) is trained as a denoiser, using an improved training algorithm with noise-specific weights
integrated in Eq. 7 proposed by Karras et al. [2022]. The network architecture relies on the encoder-decoder structure
of a U-Net [Ronneberger et al., 2015], as implemented by Dhariwal and Nichol [2021] for general image generation.
We use a multi-head self-attention mechanism [Vaswani et al., 2017] at each of its three layers to capture global
dependencies. In our implementation, each subsurface property type is represented by a separate channel. Categorical
variables such as geological facies assume a single numerical variable within a single dimension. Similarly to Miele
et al. [2024] and Miele and Azevedo [2024], the multivariate prior distribution _p_ ( **x** 0 ) is represented by a training dataset
where each sample is a joint realization of two or more co-located subsurface properties. The data can be represented
as a single large image (randomly sampled during training) [see, e.g., Laloy et al., 2018] or as separate samples.


**2.4** **Performance assessment**


Prior modeling is assessed by comparing the unconditional subsurface realizations against the TI and realizations
generated with two benchmark models, a GAN and a VAE, proposed by Miele et al. [2024] for an analogous case
study. The statistical distance between the target and modeled distributions (either prior or posterior) were measured
using the root-mean-square-error (RMSE) and the Kullback-Leibler divergence ( _KL_ ) [Kullback and Leibler, 1951].


7


_Preprint submitted to Computers & Geosciences_


The structural similarity of spatial features in the realizations was assessed using the structural similarity index (SSIM)

[Wang et al., 2004] defined as


(2 _µ_ **u** _µ_ **v** + _C_ 1 )(2 _σ_ **uv** + _C_ 2 )
SSIM( **u** _,_ **v** ) = (23)
( _µ_ [2] **u** + _µ_ [2] **v** + _C_ 1 )( _σ_ **u** [2] + _σ_ **v** [2] + _C_ 2 ) _[,]_


where **u** and **v** define sliding windows of size _M × M_, on two different images, _µ_ and _σ_ are their corresponding mean
and standard deviations, and _C_ is a constant. Following the original implementation [Wang et al., 2004] and previous
related work [e.g., Levy et al., 2023, Miele et al., 2024], we set _M_ = 7, _C_ 1 = 0 _._ 01 and _C_ 2 = 0 _._ 03. The SSIM ranges
between -1 and 1, with 1 indicating a perfect match between images. Further metrics concerning the reproduction of
facies volume fractions, two-points statistics (i.e., variograms), and lateral continuity (shape) of geological features,
are used in the Appendix A.


Convergence of the inversions results was assessed using the weighted RMSE (WRMSE), a data misfit weighted by
the data standard deviation _σ_ **d**, defined as



2

_._ (24)
~~�~~



**d** _i_ _−F_ ( **x** 0 ) _i_
� ~~[�]~~ _σ_ **d** _,i_



WRMSE =



�



1

_N_ **d**



A WRMSE _≤_ 1 _._ 1 is chosen to indicate convergence in terms of data misfit as the residuals have a similar statistic as
the observational noise. A WRMSE _<_ 1 may indicate overfitting the noisy data. Moreover, the predictive power of the
inferred pdf is assessed using the logarithmic scoring rule (logS) [Good, 1952], computed as logS(ˆ _p,_ **d** ) = _−_ log ˆ _p_ ( **d** ),
where ˆ _p_ is the inferred pdf and **d** is the real data. A lower value of logS corresponds to a better approximation. The
proposed CDPS is compared to the original DPS [Chung et al., 2023] and to a sampling method based on Markov chain
Monte Carlo (MCMC), to assess whether the inferred solutions are consistent. For MCMC, we use the preconditioned
Crank–Nicolson algorithm (pCN) [Cotter et al., 2013], which is well-suited for high-dimensional problems. In this
method, each new proposal **x** _new_ is defined by **x** _new_ = �1 _−_ _β_ [2] **x** _old_ + _β_ **z**, where **x** _old_ represents the previous state,

_β_ is a constant determining the step size, **z** _∼N_ (0 _,_ **C** ), and **C** is the prior covariance, determining the jump rate.
The acceptance probability _α_ takes the form _α_ ( **x** _new_ _,_ **x** _old_ ) = min(1 _,_ ( _p_ ( **d** _|_ **x** _new_ ) _/p_ ( **d** _|_ **x** _old_ ))). Convergence of the
pCN samples is determined with the Gelman-Rubin diagnostic [Gelman and Rubin, 1992] using _R_ [ˆ] _≤_ 1 _._ 2 for all
model parameters as a criterion. We further assess the exploration of the parameter space by computing within-chain
autocorrelation of samples, that is measuring the covariance between two samples divided by the parameter’s variance.


**2.5** **Code implementation**


The methods were implemented in Python (Python 3.12) using PyTorch (PyTorch 2.5) libraries with CUDA implementation. The training of all the generative models and the corresponding unconditional and conditional modeling
were performed using a single GPU NVIDIA [™] GeForce [®] GTX TITAN X (CUDA 12.2). The pCN sampling was
performed on the same machine, generating DM samples using this GPU. No parallelization on different GPUs was
considered. Training, modeling and sampling times mentioned in Section 3.3 are related to this machine.


**3** **Results**


We evaluate the generative performance of the trained DM by parameterizing a bivariate prior pdf, describing a sequence of lenticular sandy channels in a shale background (facies), and the associated acoustic impedance ( _I_ _P_ ). We
consider an area of 100 _×_ 80 cells, assumed to have sides of 1 m (Fig. 1). The TI consists of 3000 samples (Fig.
1a) of MPS facies realizations, generated using Petrel software (SLB) based on a 3D conceptual geological model.
Shales and sands are categorized as 0 and 1, respectively. The facies distribution ensemble is spatially uniform, with
each cell having average _p_ (sand) = 0.3 (Fig. 1c). The facies-dependent _I_ _P_ is represented using direct sequential cosimulations with multi-local distribution functions [Nunes et al., 2017, Soares, 2001]. We account for facies-dependent
_I_ _P_ marginal distributions (Fig. 1b) and spatial models (2D variograms; detailed in Appendix A). The average spatial
distribution of _I_ _P_ (Fig. 1d) is primarily dependent on facies with an area of relatively lower values in the lower-right
section of the considered area. The DM was trained for a total of 600 epochs (225000 steps). The GAN and VAE
used for benchmarking were trained on the same TI, for a total of 2000 epochs, using the hyperparameters proposed
by Miele et al. [2024].


**3.1** **Unconditonal modeling**


Prior modeling performances are assessed on 1000 unconditional samples generated from the trained DM, GAN and
VAE each. The channel morphologies and _I_ _P_ spatial patterns reproduced by DM and GAN ( _Samples_ in Fig. 2a and


8


_Preprint submitted to Computers & Geosciences_


Figure 1: Prior distribution as represented in the training dataset. a) Facies and _I_ _P_ realizations; b) _I_ _P_ distribution per
facies; c) point-wise average facies distribution; d) point-wise average and standard deviation of _I_ _P_ .


c) are visually indistinguishable from those of the TI (Fig. 1a). For DM, the _p_ (sand) and average _I_ _P_ match the prior
accurately; Table 1 shows the average relative error for the two property types. Of the two benchmarking models, the
GAN’s _p_ (sand) and average _I_ _P_ distributions (Fig. 2c) show significant misfit with the prior. In fact, the overestimation
of _p_ (sand) in the central area may indicate mode collapse. Despite the smooth spatial features in the generated samples
(Fig. 2e), the VAE performs better than GAN at capturing both properties average distributions, and displays even a
larger accuracy than DM for facies (Table 1).


The joint distributions of facies and co-located _I_ _P_ obtained from the sampled realizations (Fig. 2b, d, and f) show
significantly superior modeling ability of the DM (Fig. 2b). The facies classes are modeled with negligible errors (i.e.,
facies values different from 0 and 1); the corresponding _I_ _P_ marginal distributions (plotted along the y-axes) match
accurately the prior’s mean and standard deviation values, fitting the data with a _KL_ divergence approximately five
times lower than for GAN and VAE (Table 1). Further analysis of the networks unconditional modeling performances
focusing on geostatistical metrics, are provided in Appendix A.


**3.2** **Inverse modeling using CDPS**


We evaluated the proposed CDPS by considering both linear and nonlinear inverse problems using a test realization
from the prior distribution as our "True" facies and _I_ _P_ (Fig. 3a). The linear test case consists in generating samples
that are locally conditioned on direct observations. The data, **d** **obs**, are obtained from two wells ("Well 1" and "Well 2"
in Figure 3a). This task is analogous to _image inpainting_ in computer vision. The nonlinear example is a geophysical
inversion problem with fullstack seismic data (Fig. 3c), assumed to be derived from the true _I_ _P_, and modeled as
follows. First, vertical sequences of impedance contrasts ( _R_ ) are computed following _R_ _i_ = ( _I_ _P_ _i_ +1 _−_ _I_ _P_ _i_ ) _/_ ( _I_ _P_ _i_ +1 +
_I_ _P_ _i_ ), where the subscript _i_ indicates an _I_ _P_ sample. Then, each trace is convolved with a known seismic source


9


_Preprint submitted to Computers & Geosciences_


Figure 2: Summary of unconditional modeling performances of DM (a, b), GAN (c, d), and VAE (e, f). Subplots (a),
(c), (e): realizations and average distribution of facies and _I_ _P_ for DM (a), GAN (c) and VAE (e); subplots (b), (d), (f):
joint and _I_ _P_ distributions for DM (b), GAN (d) and VAE (f).


10


_Preprint submitted to Computers & Geosciences_


Table 1: Metrics for unconditional prior modeling using the trained DM and benchmarking GAN and VAE. The
_p_ (sand) and Avg. _I_ _P_ refer to the average models obtained from the realizations ensembles, shown in Figure 2a, c, e.
_KL_ : Kullback–Leibler divergence; _Avg._ : average; _St. dev._ : standard deviation.

|Col1|Col2|p(sand) —Avg. IP|IP marginal distributions|
|---|---|---|---|
|||_p_(sand) —Avg._ IP_|_IP_ marginal distributions|
||Lithology|Relative error|Avg. ±St. dev.<br>m/s g/cm3<br>_KLIP_|
|Prior|_Sand_<br>_Shale_|-|6660 ±730<br>8540 ±660<br>-|
|DM|_Sand_<br>_Shale_|8.3%<br>0.8%|6650 ±730<br>8530 ±680<br>0.25e-3<br>0.42e-3|
|GAN|_Sand_<br>_Shale_|21.0%<br>2.1%|6750 ±770<br>8700 ±720<br>2.1e-3<br>1.1e-3|
|VAE|_Sand_<br>_Shale_|7.0%<br>1.2%|6870 ±760<br>8620 ±690<br>1.3e-3<br>1.5e-3|



Table 2: Noise contamination levels of the conditioning data ( **d** **obs** ) for the linear and nonlinear inversion examples.

|Col1|Linear<br>σ, σ ( m g )<br>F ac. IP s cm3|Nonlinear<br>r, σ (Amplitude)<br>c|
|---|---|---|
|Data noise 1|0.05, 120|0.025, 0.5|
|Data noise 2|0.1, 230|0.05, 1|
|Data noise 3|0.2, 120|0.1, 2|



wavelet; in our case, a Ricker wavelet with central frequency of 25Hz and resolution of 3 ms. For both case studies,
we evaluated the CDPS under different Gaussian data noise conditions (Table 2).


We compare the results to those obtained with DPS [Chung et al., 2023]. For fair comparison of the two theoretical
approaches, we implemented the DPS within the EDM framework used in this work: following DPS theory, we
compute the actual Gaussian log-likelihood considering **d** **obs** noise only (e.g., Eq. 14 for absolute _σ_ **d** ) and sum it to
the prior score for denoising (i.e., Algorithm 1 without lines 13-19).

Using 300 validation examples from the TI we evaluated the modeling error _σ_ ˆ **x** [(] _[t]_ 0 [)] [for 1000 denoising steps. Examples]
of pixel-dependent uncertainty are shown in Fig. 4a, the full homoscedastic noise sequence used is shown in Fig. 4b.
Here, even if the facies classes are categorical, they are modeled as a continuous variable by DM and treated as such
in the likelihood score computation. In the Appendix B we applications of CDPS using both well and seismic data,
simultaneously.


**3.2.1** **Linear conditioning results**


For the linear conditioning case, we considered 3 study cases contaminating **d** **obs** (Figure 3b) with different uncorrelated Gaussian noise of absolute standard deviations ( _σ_ _F ac_ and _σ_ _I_ _P_ in Table 2). Figure 5a shows the score magnitudes,
expressed as L2-norms, for CDPS and DPS as a function of denoising/generative steps. For all settings, the scores of
the data distribution have very similar values and decreasing trends; an average for the three noise levels is plotted as
dashed lines. The likelihood scores show generally an inverse correlation between their magnitude and the data noise
level, highlighting a stronger constraining effect of low-noise **d** **obs** . This correlation is negligible for the CDPS after
300 diffusion steps, with _∇_ **x** _t_ log _p_ _t_ ( **d** _|_ **x** _t_ ) being approximately 30 times lower than _∇_ **x** _t_ log _p_ _t_ ( **x** _t_ ). A similar trend is
observed in DPS only for the the largest data noise case ( _Data noise 3_ ), while the likelihood score values generally
increase with decreasing data error.


For each scenario, we generated 100 conditional samples. Their convergence to **d** **obs** is shown in Figure 5d as WRMSE

ˆ
between the predicted ˆ **x** [(] 0 _[t]_ [)] ( **x** [(] 0 _[t]_ [)] = **x** 0 at _t_ = 0) and the conditional noisy **d** **obs** . Between 93 and 95% of the generated
CDPS samples converged to a WRMSE _<_ 1 _._ 1. For DPS, none of the samples converged when considering the lowest
noise levels, and 99% of the samples converged to a WRMSE <1.1 for the two larger-error cases. Overall, the WRMSE
of the DPS samples approach lower values faster than CDPS during the denoising process; for _Data noise 1_, the DPS
samples (ˆ **x** [(] 0 _[t]_ [)] [) initially overfit the data noise (WRMSE < 1) and then divergence (WRMSE] _[ ≈]_ [1] _[.]_ [5][). CDPS WRMSE]
values also show comparatively larger variance (i.e., DPS samples follow similar paths).


11


_Preprint submitted to Computers & Geosciences_


Figure 3: Test model used in inverse modeling; a) real subsurface distribution ("True") and location of the two wells
used for the linear inversion; b) conditioning well log data for the linear inversion; c) conditioning seismic data for the
nonlinear inversion.


Figure 6a and b show the ensembles of realizations sampled by CDPS and DPS, respectively, for _Data noise 2_ case.
The realizations’ generally converge correctly at well locations, and show increasingly larger variability with distance
from the wells, towards the prior spatial uniformity. Considering, as example, the unconditioned leftmost area between
_x_ = 0 m and _x_ = 20 m, CDPS better represents this uniformity (with _p_ (sand) = 0.36 and _I_ _P_ = 7400 ±1080 m/s g/cm [3] ),
while DPS shows non-uniform channels distributions ( _p_ (sand) = 0.50±0.44 and _I_ _P_ = 7120 ±880 m/s g/cm [3] ). We
further attempted to sample the same posterior using pCN MCMC, considering _β_ = 0.03 and three independent chains,
for 50000 iterations. After a burn-in time of 25000 iterations, only 60 parameters of the 120 conditioned parameters
satisfied the _R_ [ˆ] statistics threshold of 1.2. The sampled solutions’ ensembles are shown in Fig. 6c. Here, local
conditioning is effective, but large variance (particularly evident from the facies distribution) highlight the points of
non-convergence of the pCN. The unconditioned area between _x_ = 0 m and _x_ = 20 m, has _p_ (sand) = 0.28 and _I_ _P_ =
7900 ±1030 m/s g/cm [3] and shows facies and _I_ _P_ spatial patterns closer to those of the CDPS. However, the samples
show poor mixing: on average, the converged parameters show autocorrelation values lower than 0.2 only after 400
samples (i.e., each chain has approximately 62 independent samples).


Figure 7 shows the realizations of _I_ _P_ sampled by the three methods at the well locations, while their agreement with
the target data ( **d** **obs** without noise) is summarized in Table 3. The DPS results show a comparatively poorer fit to the
data than CDPS (Fig. 7b), showing larger WRMSE and logS values than for CDPS. While the pCN results fit the data
within the noise assumptions (Fig. 7c), it shows larger logS and RMSE values than DPS and CDPS to Well 2 data.
These discrepancies may be due to the lack of sufficient posterior samples as indicated by the limited convergence of
the MCMC chains.


**3.2.2** **Nonlinear inversion results**


We ran the nonlinear inversion example for three noise-levels in **d** **obs** (Fig. 3c), considering absolute and relative
components of the standard deviations ( _σ_ _c_ and _r_, respectively, in Table 2). Similar to the linear case, the prior scores
(Fig. 5b) display a decreasing trend that does not change significantly for different noise levels. The _∇_ **x** _t_ log _p_ _t_ ( **d** _|_ **x** _t_ )


12


_Preprint submitted to Computers & Geosciences_


Figure 4: Modeling error; a) examples of errors at specific noise levels; b) standard deviation of the homoscedastic
error considered.


Table 3: RMSE and logS measures of the predicted _I_ _P_ for the linear inversion, considering the _Data noise 2_ case.

|Col1|Well 1|Col3|Col4|Well 2|Col6|Col7|
|---|---|---|---|---|---|---|
||CDPS|DPS|pCN|CDPS|DPS|pCN|
|WRMSE|0.70|0.74|0.69|0.79|0.78|0.86|
|LogS|6.3|7.1|6.3|6.5|6.7|6.4|
|LogS (Prior)|9.02|9.02|9.02|8.79|8.79|8.79|



values of CDPS show a similar decreasing trend for the three noise levels, ranging from one-tenth to those of the
_∇_ **x** _t_ log _p_ _t_ ( **x** _t_ ). Unlike the linear case, these scores show permanent differences between noise scales, with a maximum difference of one order of magnitude between the minimum and maximum data noise used. For DPS, the
_∇_ **x** _t_ log _p_ _t_ ( **d** _|_ **x** _t_ ) values are comparatively larger than CDPS and rapidly exceed the prior ones, becoming up to ten
times larger ( _Data noise 1_ ).


All samples obtained with CDPS converged to a WRMSE _<_ 1 _._ 1 (Fig. 5e). On the other hand, convergence of DPS
samples occurred only for the larger data noise case, while none of the samples converging for _Data noise 1_ (WRMSE
= 1 _._ 23 _±_ 0 _._ 03) and _2_ (WRMSE = 1 _._ 13 _±_ 0 _._ 02). The results for _Data noise 3_ case are visualized in Fig. 8 and
corresponding metrics are shown in Table 4. When comparing the distances between the noiseless observed data and
those from the predicted _I_ _P_, the CDPS solutions are comparatively more accurate (WRMSE **d** in Table 4 and error
distributions (Fig. 8e). However, both methods’ predictions have a forward response that is largely within the data
error distributions (WRMSE **d** _<_ 1). This is also shown by the trace plot examples of Figure 8f comparing predicted
and conditioning data distributions for one vertical trace at x = 25 m.


Overall, the CDPS shows large predictive power; the average facies and _I_ _P_ generated by CDPS (Fig. 8a and c)
retrieve the target with large accuracy (lower RMSE F and RMSE _I_ _P_ ; Table 4) and larger variability than DPS (Fig. 8b
and d). The generated CDPS facies realizations predict the target facies with a precision of 98% and 95% of accuracy,


13


_Preprint submitted to Computers & Geosciences_


Figure 5: Evolution of prior (dashed) and likelihood (continuous) score functions generative steps, expressed as L2norm, for (a) linear conditioning and (b) nonlinear conditioning case studies; and WRMSE of 100 samples generated
as solutions of the linear (c) and nonlinear (d) inverse problems. The values of magnitude of the three data noise
assumed are indicated in 2.


Table 4: Metrics of the posterior distributions sampled by CDPS and DPS for the non-linear inversion case, measured
for data, facies and _I_ _P_ (subscripts **d**, F, and _I_ _P_, respectively) for the _Data noise 3_ case.

|F, and IP, respec|ctively) for the|e Data noise 3|
|---|---|---|
||CDPS|DPS|
|WRMSE**d**|0.34±0.01|0.39±0.01|
|RMSEF<br>SSIMF|0.16<br>0.73±0.03|0.19<br>0.67±0.01|
|RMSE_IP_<br>LogS_IP_<br>_KLIP_ (10~~_−_3~~)|334<br>7.3<br>0.5±0.2|561<br>29.9<br>1.3±0.2|



showing a relative improvement compared to the DPS (97% and 94% respectively), and having a large structural
similarity (SSIM F values in Table 4).


The CDPS shows superior prediction accuracy. For the whole study area, the generated solutions’ ensemble capture
the true _I_ _P_ much better than DPS, as shown by the lower values of logS in Table 4 and in the trace example in Figure
8h ( _x_ =25 m). Moreover, each realization reproduces the target _I_ _P_ values distribution with low _KL_ divergence (Table
4) and presents low-scale spatial variability patterns, which appear smoothed in DPS realizations (Fig. 8g).


In an attempt to compare the CDPS results with a high-quality posterior estimate, we ran the pCN method with
_β_ -values of 0.1, 0.05, 0.03, and 0.01, using three parallel chains. None of the runs converged towards a posterior
distribution after 50000 iterations, indicating that the inversion problem is hard, and implicitly suggesting that the
CDPS method performs well in practice.


**3.3** **Computational performances**


In this study, we did not consider any parallelization (see section 2.5). Moreover, we adopt a 2 [nd] Heun solver when
generating DM samples, and do not evaluate the costs/benefits of 1 [st] solvers (e.g., Euler). The U-Net adopted in the
DM (1 _._ 31 _×_ 10 [8] trainable parameters) required a total training time of 116 hours (600 epochs); the GAN (1 _×_ 10 [7]


14


_Preprint submitted to Computers & Geosciences_


Figure 6: Point-wise average and standard distribution of Facies and _I_ _P_ posterior samples from a) CDPS, b) DPS, c)
pCN MCMC.


parameters) and VAE (3 _._ 2 _×_ 10 [7] parameters) used for comparison took 21 and 10 hours, respectively, (2000 epochs).
The DM exceeded, however, the modeling accuracy of the fully trained GAN and VAE already at epoch 100, that is,
after 19.5 hours of training (Appendix A).


With EDM, accurate sampling could be obtained using at least 18 diffusion steps, with no significant improvements
when considering additional steps. The average generation time of the DM is 3 seconds multivariate realization (5
to 7 times longer than for GAN and VAE). For the linear conditioning case study (Section 3.2.1), the CDPS and
DPS have similar computational time, of 0.6 seconds per time step in our tests. The relative time increase, compared
to unconditional modeling, is mostly due to the backpropagation computation for the likelihood scores. For the nonlinear case (Section 3.2.2), the CDPS had a larger computational cost, requiring 1.5 seconds per denoising step, mostly
due to the additional Jacobian matrix computations. After testing multiple numbers of time-steps (from 18 to 2000
steps), we observed that both CDPS and DPS achieved their best modeling performances using 32 denoising steps.
On the other hand we observed significant differences in inversion accuracy between CDPS and DPS, for nonlinear
conditioning, as summarized in Table 5. Here, the CDPS had a near-optimal performance with 250 steps, while the
DPS required a minimum of 1000 steps, which, in our tests, resulted in a significant reduction of the sampling time
(approximately half), despite the increased computational costs of CDPS.


The time required by pCN MCMC for the linear case was substantially larger: the burn-in was reached after more
than three days of computation, while the poor mixing of the sampled solutions show that much longer sampling
would be necessary to recover the full posterior. We do not exclude that better hyperparameters would improve these
performances. The application of pCN on the nonlinear case did not converge in any of the considered case.


15


_Preprint submitted to Computers & Geosciences_


Figure 7: _I_ _P_ sampled distribution from (a) CDPS, (b) DPS, and (c) pCN MCMC, compared to the expected conditional
data distribution **d** **obs** _±σ_ _I_ _P_ .


16


_Preprint submitted to Computers & Geosciences_


Figure 8: Sampled posterior distribution represented as (a) average facies; (b) facies standard deviation; (c) average
_I_ _P_ ; (d) _I_ _P_ standard deviation; (e) average data error; (f) predicted data versus conditional data distribution ( **d** obs _±_ _σ_ **d** )
for a vertical trace at x = 25 m; (g) _I_ _P_ realizations; (h) _I_ _P_ samples for a vertical trace at x = 25 m compared to the
unknown target distribution.


17


_Preprint submitted to Computers & Geosciences_


Table 5: WRMSE **d** for the three data noise cases considered in the nonlinear inverse modeling (Section 3.2.2); "Div."
(Diverged) indicates generative processes diverging to "Not a Number" (NaN) values.

|Denoising steps|CDPS<br>Data noise 1 Data noise 2 Data noise 3|Col3|Col4|DPS<br>Data noise 1 Data noise 2 Data noise 3|Col6|Col7|
|---|---|---|---|---|---|---|
|250<br>500<br>1000|1.06±0.03<br>1.05±0.02<br>1.05±0.02|1.01±0.01<br>0.99±0.01<br>0.99±0.01|0.998±0.005<br>0.999±0.005<br>0.997±0.005|Div.<br>Div.<br>1.30±0.03|1.30±0.02<br>1.18±0.02<br>1.17±0.02|1.10±0.02<br>1.17±0.02<br>1.054±0.004|



**4** **Discussion**


The trained DM effectively samples from complex bivariate geological priors. To evaluate performance, we mainly
focused on first- and second- order statistics (higher-order aspects is considered in Appendix A). The generated images
match the TI features with a fidelity that, to our knowledge, is unprecedented for multivariate geological modeling with
deep generative models. For the considered metrics, the DM outperformed the VAE and GAN implementations by
Miele et al. [2024] (e.g., Fig. 2 and Table 1), except for a general overestimation of sand volume fraction that was
slightly less pronounced in the VAE results (Table 1 and Figure S1 in Appendix A). A practical advantage of DMs for
multivariate modeling is the possibility to associate individual channels of the underlying U-Net to specific subsurface
property types without the need to modify the network architecture. In contrast, VAE and GAN modeling required
independent convolutional layers for each property type. This suggests that pre-designed DM architectures require
none or limited adjustments for modeling different geological scenarios.


The stability of the training process and the high accuracy of DM-based generative modeling come at the cost of
high dimensionality and comparatively slow generation times. For instance, the benchmark VAE and GAN perform a
compression from 16000 to 60 variables and the corresponding generation times are much faster [Miele et al., 2024].
Modeling biases and errors occur, however, even when considering much lower compression rates, such as in LDMs

[Ovanger et al., 2025], highlighting inherent trade-offs between speed, accuracy and dimensionality.


Using the proposed CDPS, pre-trained DMs can be flexibly modified to achieve noise-robust posterior sampling in
both linear and non-linear inversion settings. The framework can be used for a wide range of inversion tasks without
retraining, and deal with the simultaneous use of multiple conditioning data types (Appendix B). The method includes
modifications to the DPS by Chung et al. [2023], mainly an improved likelihood score approximation. By accounting
for the error in ˆ **x** [(] 0 _[t]_ [)] [on the simulated conditioning data (Eq. 19), the likelihood scores become more accurate and scaled]
to diffusion noise (Fig. 5). With this modification, we allow a much more stable generative process compared to DPS,
balancing the two scores contributions for posterior sampling (Eq. 10) automatically (i.e., without arbitrary weighting),
and making the likelihood score more independent of the data noise. However, a small noise-dependency remains for
the nonlinear case with CDPS (Fig. 5b). This might be due to the overly simple assumption of homoscedastic noise in
the ˆ **x** [(] 0 _[t]_ [)] [estimates (Fig. 4), which may not hold if the trained network presents any form of localized bias (due to wrong]
training procedure, network’s architecture and/or biased TI distributions). Improved error estimates may be needed in
future applications, considering locally dependent variances or using approximations of Eq. 15.


Our proposed CDPS algorithm (Algorithm 1) enables efficient and more accurate local conditioning than DPS (see,
e.g., Fig. 6a and b). We did not manage to properly benchmark the results against MCMC with pCN proposals (Fig.
6c). Indeed, even in the linear inversion case there is a relatively poor mixing and limited convergence of pCN; we did
not investigate more advanced MCMC approaches such as parallel tempering formulations [Xu et al., 2020]. However,
there is an overall good agreement between CDPS and pCN samples at the well locations (Fig. 7 and Table 3). This
suggests that CDPS appropriately approximate the posterior. Compared with DPS (Fig. 7b), CDPS (Fig. 7a) has
wider uncertainty bounds of magnitudes similar to those obtained by MCMC. We postulate that the underestimation
by DPS is primarily a consequence of the likelihood score magnitudes being overestimated (Fig. 5a) when using the
approximation in Eq. 12.


For the non-linear inversion case, the CDPS performed well for all considered noise levels while DPS failed when
considering low data noise levels (e.g., Fig. 6 and Table 5). The DPS used herein is an implementation of the theory
by Chung et al. [2023] using the score-based EDM framework. We also considered arbitrarily-weighted Gaussian
likelihood functions or the RMSE for the log-score approximation (as in the original DDIM implementation); this led
to no convergence and unstable behavior in all the considered cases. We reproduce similar results using CDPS with
DDIM (Appendix C) highlighting that the method also works well for this formulation.


Even if the CDPS method performs very well in practice, it is an approximate method because the logarithmic score
estimate (Eq. 19) relies on an approximation of the intractable likelihood (Eq. 11), the prior score estimates provided


18


_Preprint submitted to Computers & Geosciences_


by the neural network will inevitably have some errors and the number of diffusion time steps are finite. One computational challenge of the CDPS method is the need to estimate the Jacobian of the forward operator at each time step.
When this calculation is very expensive, we anticipate that the CDPS would still work well even if the Jacobian is only
updated occasionally during the generative process.


**5** **Conclusions**


We investigate diffusion models (DM) for sampling multivariate geological priors and propose a modified method
(CDPS) for approximate probabilistic Bayesian inversion. itsperformance is evaluated on a bivariate distribution of
lenticular sand deposits and corresponding acoustic impedances ( _I_ _P_ ). The unconditional DM simulations effectively
captures first- and second-order statistics, as well as higher-order facies structures represented in training images (TI),
consistently outperforming benchmark generative models based on VAEs and GANs. In contrast to these methods, the
DM does not require architectural modifications for modeling bivariate geological properties.


The inversion method leverages the iterative nature of diffusion modeling; posterior samples are obtained by considering, at each iteration of the denoising process, a likelihood score approximation that modify the update direction
during the generative process. There is no need to train the DM when considering conditioning data and the inversion
is performed without any form of data dimensionality reduction. We demonstrate that the method enables efficient
Bayesian posterior sampling for both linear (borehole data) and nonlinear (fullstack seismic) data scenarios, or combinations thereof. Our implementation makes corrections to the likelihood approximation of the original Diffusion
Posterior Sampling (DPS) method [DPS; Chung et al., 2023]. This leads to stabilized inversion across noise levels, as well as more accurate posterior estimates indicated by lower bias and better exploration of the posterior pdf.
For the considered test examples, CDPS also shows significant advantages over an MCMC formulation in terms of
convergence and vastly decreased computing times, particularly for nonlinear and high-dimensional data settings.


This work highlights the strengths of diffusion-based generative models for complex subsurface modeling and inference tasks. The training is much more robust than for GANs and the data generation quality is beyond the capabilities
of GANs and VAEs. The CDPS approach enables flexible and scalable probabilistic inversion with minimal architectural tuning, resulting in an approximate Bayesian inversion method with broad applications across complex geological
scenarios and data types.


**6** **Code availability**


The implementation of CDPS for EDM is available at `[https://github.com/romiele/CDPS_EDM](https://github.com/romiele/CDPS_EDM)` . The corresponding CDPS inversion using DDPM and DDIM frameworks are available at `[https://github.com/romiele/CDPS_](https://github.com/romiele/CDPS_DDIM)`
`[DDIM](https://github.com/romiele/CDPS_DDIM)` .


**References**


Dario Grana, Tapan Mukerji, and Philippe Doyen. _Seismic Reservoir Modeling: Theory, Examples, and Algorithms_ .
Wiley, 1 edition, April 2021. ISBN 978-1-119-08618-5 978-1-119-08621-5. doi: 10.1002/9781119086215.

Yoram Rubin and Susan Sharpless Hubbard. _Hydrogeophysics_ . Number v. 50 in Water Science and Technology
Library. Springer, Dordrecht [Netherlands] New York, 2005. ISBN 978-1-4020-3102-1.

Albert Tarantola. _Inverse Problem Theory and Methods for Model Parameter Estimation_ . Society for Industrial and Applied Mathematics, January 2005. ISBN 978-0-89871-572-9 978-0-89871-792-1. doi: 10.1137/1.
9780898717921.

Niklas Linde, David Ginsbourger, James Irving, Fabio Nobile, and Arnaud Doucet. On uncertainty quantification in
hydrogeology and hydrogeophysics. _Advances in Water Resources_, 110:166–181, December 2017. ISSN 03091708.
doi: 10.1016/j.advwatres.2017.10.014.

J.Jaime Gómez-Hernández and Xian-Huan Wen. To be or not to be multi-Gaussian? A reflection on stochastic
hydrogeology. _Advances in Water Resources_, 21(1):47–61, February 1998. ISSN 03091708. doi: 10.1016/
S0309-1708(96)00031-0.

Insa Neuweiler, Alexandros Papafotiou, Holger Class, and Rainer Helmig. Estimation of effective parameters for a
two-phase flow problem in non-Gaussian heterogeneous porous media. _Journal of Contaminant Hydrology_, 120–
121:141–156, March 2011. ISSN 01697722. doi: 10.1016/j.jconhyd.2010.08.001.

Andre G. Journel and Clayton V. Deutsch. Entropy and spatial disorder. _Mathematical Geology_, 25(3):329–355, April
1993. ISSN 0882-8121, 1573-8868. doi: 10.1007/BF00901422.


19


_Preprint submitted to Computers & Geosciences_


Fengde Zhou, Daren Shields, Stephen Tyson, and Joan Esterle. Comparison of sequential indicator simulation, object
modelling and multiple-point statistics in reproducing channel geometries and continuity in 2D with two different spaced conditional datasets. _Journal of Petroleum Science and Engineering_, 166:718–730, July 2018. ISSN
09204105. doi: 10.1016/j.petrol.2018.03.043.


Felipe B. Guardiano and R. Mohan Srivastava. Multivariate Geostatistics: Beyond Bivariate Moments. In F. M.
Gradstein and Amilcar Soares, editors, _Geostatistics Tróia ’92_, volume 5, pages 133–144. Springer Netherlands,
Dordrecht, 1993. ISBN 978-0-7923-2157-6 978-94-011-1739-5. doi: 10.1007/978-94-011-1739-5_12.


Sebastien Strebelle. Conditional Simulation of Complex Geological Structures Using Multiple-Point Statistics. _Math-_
_ematical Geology_, 34(1):1–21, January 2002. ISSN 1573-8868. doi: 10.1023/A:1014009426274.


Gregoire Mariethoz, Philippe Renard, and Julien Straubhaar. The Direct Sampling method to perform multiple-point
geostatistical simulations. _Water Resources Research_, 46(11):2008WR007621, November 2010a. ISSN 0043-1397,
1944-7973. doi: 10.1029/2008WR007621.


Gregoire Mariethoz and Jef Caers. _Multiple-Point Geostatistics: Stochastic Modeling with Training Images_ . Wiley, 1
edition, October 2014. ISBN 978-1-118-66275-5 978-1-118-66295-3. doi: 10.1002/9781118662953.


Anne-Sophie Høyer, Giulio Vignoli, Thomas Mejer Hansen, Le Thanh Vu, Donald A. Keefer, and Flemming Jørgensen. Multiple-point statistical simulation for hydrogeological models: 3-D training image development and
conditioning strategies. _Hydrology and Earth System Sciences_, 21(12):6069–6089, December 2017. ISSN 16077938. doi: 10.5194/hess-21-6069-2017.


Mathieu Le Coz, Jacques Bodin, and Philippe Renard. On the use of multiple-point statistics to improve groundwater
flow modeling in karst aquifers: A case study from the Hydrogeological Experimental Site of Poitiers, France.
_Journal of Hydrology_, 545:109–119, February 2017. ISSN 00221694. doi: 10.1016/j.jhydrol.2016.12.010.


Julien Straubhaar, Philippe Renard, and Tatiana Chugunova. Multiple-point statistics using multi-resolution images.
_Stochastic Environmental Research and Risk Assessment_, 34(2):251–273, February 2020. ISSN 1436-3240, 14363259. doi: 10.1007/s00477-020-01770-8.


Yulia Melnikova, Andrea Zunino, Katrine Lange, Knud Skou Cordua, and Klaus Mosegaard. History Matching
Through a Smooth Formulation of Multiple-Point Statistics. _Mathematical Geosciences_, 47(4):397–416, May 2015.
ISSN 1874-8961, 1874-8953. doi: 10.1007/s11004-014-9537-y.


Jef Caers and Tuanfeng Zhang. Multiple-point Geostatistics: A Quantitative Vehicle for Integrating Geologic Analogs
into Multiple Reservoir Models. In G. Michael Grammer, Paul M. “Mitch” Harris, and Gregor P. Eberli, editors, _Integration of Outcrop and Modern Analogs in Reservoir Modeling_, pages 383–394. American Association of
Petroleum Geologists, 2004. ISBN 978-0-89181-361-3 978-1-62981-047-8. doi: 10.1306/M80924C18.


Julien Straubhaar, Philippe Renard, and Grégoire Mariethoz. Conditioning multiple-point statistics simulations to
block data. _Spatial Statistics_, 16:53–71, May 2016. ISSN 22116753. doi: 10.1016/j.spasta.2016.02.005.


Thomas Mejer Hansen, Le Thanh Vu, Klaus Mosegaard, and Knud Skou Cordua. Multiple point statistical simulation
using uncertain (soft) conditional data. _Computers & Geosciences_, 114:1–10, May 2018. ISSN 00983004. doi:
10.1016/j.cageo.2018.01.017.


Julien Straubhaar and Philippe Renard. Conditioning Multiple-Point Statistics Simulation to Inequality Data. _Earth_
_and Space Science_, 8(5):e2020EA001515, May 2021. ISSN 2333-5084, 2333-5084. doi: 10.1029/2020EA001515.


Tobias Zahner, Tobias Lochbühler, Grégoire Mariethoz, and Niklas Linde. Image synthesis with graph cuts: A
fast model proposal mechanism in probabilistic inversion. _Geophysical Journal International_, 204(2):1179–1190,
February 2016. ISSN 0956-540X, 1365-246X. doi: 10.1093/gji/ggv517.


Shiran Levy, Lea Friedli, Grégoire Mariéthoz, and Niklas Linde. Conditioning of multiple-point statistics simulations
to indirect geophysical data. _Computers & Geosciences_, 187:105581, May 2024. ISSN 00983004. doi: 10.1016/j.
cageo.2024.105581.


Eric Laloy, Niklas Linde, Diederik Jacques, and Grégoire Mariethoz. Merging parallel tempering with sequential
geostatistical resampling for improved posterior exploration of high-dimensional subsurface categorical fields. _Ad-_
_vances in Water Resources_, 90:57–69, April 2016. ISSN 03091708. doi: 10.1016/j.advwatres.2016.02.008.


Thomas Mejer Hansen, Knud Skou Cordua, and Klaus Mosegaard. Inverse problems with non-trivial priors: Efficient
solution through sequential Gibbs sampling. _Computational Geosciences_, 16(3):593–611, June 2012. ISSN 14200597, 1573-1499. doi: 10.1007/s10596-011-9271-1.


Grégoire Mariethoz, Philippe Renard, and Jef Caers. Bayesian inverse problem and optimization with iterative spatial
resampling. _Water Resources Research_, 46(11):2010WR009274, November 2010b. ISSN 0043-1397, 1944-7973.
doi: 10.1029/2010WR009274.


20


_Preprint submitted to Computers & Geosciences_


Ian Goodfellow, Yoshua Bengio, and Aaron Courville. _Deep Learning_ . Adaptive Computation and Machine Learning.
The MIT press, Cambridge, Mass, 2016. ISBN 978-0-262-03561-3.


Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In _International Conference on Learning_
_Representations_, December 2013.


Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,
and Yoshua Bengio. Generative Adversarial Networks, June 2014.


Eric Laloy, Romain Hérault, John Lee, Diederik Jacques, and Niklas Linde. Inversion using a new low-dimensional
representation of complex binary geological media based on a deep neural network. _Advances in Water Resources_,
110:387–405, December 2017. ISSN 03091708. doi: 10.1016/j.advwatres.2017.09.029.


Eric Laloy, Romain Hérault, Diederik Jacques, and Niklas Linde. Training-Image Based Geostatistical Inversion
Using a Spatial Generative Adversarial Neural Network. _Water Resources Research_, 54(1):381–406, January 2018.
ISSN 0043-1397, 1944-7973. doi: 10.1002/2017WR022148.


Alain Oliviero-Durmus, Yazid Janati, and Marcelo Pereyra. Generative modelling meets Bayesian inference: A new
paradigm for inverse problems. _Philosophical transactions of the Royal Society A_, 383(2299), June 2025.


Shiran Levy, Jürg Hunziker, Eric Laloy, James Irving, and Niklas Linde. Using deep generative neural networks
to account for model errors in Markov chain Monte Carlo inversion. _Geophysical Journal International_, 228(2):
1098–1118, October 2021. ISSN 0956-540X, 1365-246X. doi: 10.1093/gji/ggab391.


Lukas Mosser, Olivier Dubrule, and Martin J. Blunt. Stochastic Seismic Waveform Inversion Using Generative Adversarial Networks as a Geological Prior. _Mathematical Geosciences_, 52(1):53–79, January 2020. ISSN 1874-8961,
1874-8953. doi: 10.1007/s11004-019-09832-6.


Mingliang Liu, Dario Grana, and Leandro Passos De Figueiredo. Uncertainty quantification in stochastic inversion
with dimensionality reduction using variational autoencoder. _GEOPHYSICS_, 87(2):M43–M58, March 2022. ISSN
0016-8033, 1942-2156. doi: 10.1190/geo2021-0138.1.


Arnaud Doucet, Nando Freitas, and Neil Gordon, editors. _Sequential Monte Carlo Methods in Practice_ . Springer New
York, New York, NY, 2001. ISBN 978-1-4419-2887-0 978-1-4757-3437-9. doi: 10.1007/978-1-4757-3437-9.


M Amaya, N Linde, and E Laloy. Adaptive sequential Monte Carlo for posterior inference and model selection among
complex geological priors. _Geophysical Journal International_, 226(2):1220–1238, May 2021. ISSN 0956-540X,
1365-246X. doi: 10.1093/gji/ggab170.


Smith W.A. Canchumuni, Alexandre A. Emerick, and Marco Aurélio C. Pacheco. Towards a robust parameterization for conditioning facies models using deep variational autoencoders and ensemble smoother. _Computers &_
_Geosciences_, 128:87–102, July 2019. ISSN 00983004. doi: 10.1016/j.cageo.2019.04.006.


Mingliang Liu and Dario Grana. Petrophysical characterization of deep saline aquifers for CO2 storage using ensemble
smoother and deep convolutional autoencoder. _Advances in Water Resources_, 142:103634, August 2020. ISSN
03091708. doi: 10.1016/j.advwatres.2020.103634.


Shaoxing Mo, Nicholas Zabaras, Xiaoqing Shi, and Jichun Wu. Integration of Adversarial Autoencoders With Residual Dense Convolutional Networks for Estimation of Non-Gaussian Hydraulic Conductivities. _Water Resources_
_Research_, 56(2):e2019WR026082, February 2020. ISSN 0043-1397, 1944-7973. doi: 10.1029/2019WR026082.


Eric Laloy, Niklas Linde, Cyprien Ruffino, Romain Hérault, Gilles Gasso, and Diederik Jacques. Gradient-based
deterministic inversion of geophysical data with generative adversarial networks: Is it feasible? _Computers &_
_Geosciences_, 133:104333, December 2019. ISSN 00983004. doi: 10.1016/j.cageo.2019.104333.


Emilien Dupont, Tuanfeng Zhang, Peter Tilke, Lin Liang, and William Bailey. Generating Realistic Geology Conditioned on Physical Measurements with Generative Adversarial Networks, July 2018.


Shing Chan and Ahmed H. Elsheikh. Parametric generation of conditional geological realizations using generative
neural networks. _Computational Geosciences_, 23(5):925–952, October 2019. ISSN 1420-0597, 1573-1499. doi:
10.1007/s10596-019-09850-7.


Shiran Levy, Eric Laloy, and Niklas Linde. Variational Bayesian inference with complex geostatistical priors using
inverse autoregressive flows. _Computers & Geosciences_, 171:105263, February 2023. ISSN 00983004. doi:
10.1016/j.cageo.2022.105263.


Roberto Miele, Shiran Levy, Niklas Linde, Amilcar Soares, and Leonardo Azevedo. Deep generative networks for
multivariate fullstack seismic data inversion using inverse autoregressive flows. _Computers & Geosciences_, 188:
105622, June 2024. ISSN 00983004. doi: 10.1016/j.cageo.2024.105622.


21


_Preprint submitted to Computers & Geosciences_


L. Mosser, O. Dubrule, and M.J. Blunt. Conditioning of Generative Adversarial Networks for Pore and Reservoir Scale
Models. In _Proceedings_, Copenhagen, Denmark, June 2018. EAGE Publications BV. doi: 10.3997/2214-4609.
201800774.

Chengkai Zhang, Xianzhi Song, and Leonardo Azevedo. U-net generative adversarial network for subsurface facies
modeling. _Computational Geosciences_, 25(1):553–573, February 2021. ISSN 1420-0597, 1573-1499. doi: 10.
1007/s10596-020-10027-w.

Eric Laloy, Niklas Linde, and Diederik Jacques. Approaching geoscientific inverse problems with vector-to-image
domain transfer networks, November 2020.

Runhai Feng, Klaus Mosegaard, Dario Grana, Tapan Mukerji, and Thomas Mejer Hansen. Stochastic Facies Inversion
with Prior Sampling by Conditional Generative Adversarial Networks Based on Training Image. _Mathematical_
_Geosciences_, 56(4):665–690, May 2024. ISSN 1874-8961, 1874-8953. doi: 10.1007/s11004-023-10119-0.

Roberto Miele and Leonardo Azevedo. Physics-informed W-Net GAN for the direct stochastic inversion of fullstack
seismic data into facies models. _Scientific Reports_, 14(1):5122, March 2024. ISSN 2045-2322. doi: 10.1038/
s41598-024-55683-5.

Mario Lucic, Karol Kurach, Marcin Michalski, Olivier Bousquet, and Sylvain Gelly. Are GANs created equal? a
large-scale study. In _Proceedings of the 32nd International Conference on Neural Information Processing Systems_,
NIPS’18, pages 698–707, Red Hook, NY, USA, December 2018. Curran Associates Inc.

Jorge Lopez-Alvis, Eric Laloy, Frédéric Nguyen, and Thomas Hermans. Deep generative models in inversion: A
review and development of a new approach based on a variational autoencoder. _Computers & Geosciences_, 152:
104762, July 2021. ISSN 00983004. doi: 10.1016/j.cageo.2021.104762.

Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _Proceedings of the 34th_
_International Conference on Neural Information Processing Systems_, NIPS ’20, pages 6840–6851, Red Hook, NY,
USA, December 2020. Curran Associates Inc. ISBN 978-1-7138-2954-6.

Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsupervised Learning using
Nonequilibrium Thermodynamics. In _Proceedings of the 32nd International Conference on Machine Learning_,
November 2015.

Prafulla Dhariwal and Alex Nichol. Diffusion Models Beat GANs on Image Synthesis, June 2021.

Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models. In _Proceedings of the_
_International Conference on Learning Representations_, 2021a.

Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. ScoreBased Generative Modeling through Stochastic Differential Equations. In _Thirty-Seventh Conference on Neural_
_Information Processing Systems_, 2021b. doi: 10.48550/arXiv.2011.13456.

Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the Design Space of Diffusion-Based Generative
Models. In _36th Conference on Neural Information Processing Systems_, October 2022.

L. Mosser. Deep Diffusion Models for Facies Modeling. In _84th EAGE Annual Conference & Exhibition Workshop_
_Programme_, pages 1–5, Vienna, Austria„ 2023. European Association of Geoscientists & Engineers. doi: 10.3997/
2214-4609.2023101627.

Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans.
Autoregressive Diffusion Models. In _International Conference on Learning Representations_, February 2022. doi:
10.48550/arXiv.2110.02037.

Minghui Xu, Suihong Song, and Tapan Mukerji. DiffSim: Denoising diffusion probabilistic models for generative
facies geomodeling. In _Fourth International Meeting for Applied Geoscience & Energy_, pages 1660–1664, Houston,
Texas, July 2024. Society of Exploration Geophysicists and American Association of Petroleum Geologists. doi:
10.1190/image2024-4081304.1.

Ali Aouf, Eric Laloy, Bart Rogiers, and Christophe De Vleeschouwer. 3D clay microstructure synthesis using Denoising Diffusion Probabilistic Models. _Applied Computing and Geosciences_, 26:100248, June 2025. ISSN 25901974.
doi: 10.1016/j.acags.2025.100248.

Daesoo Lee, Oscar Ovanger, Jo Eidsvik, Erlend Aune, Jacob Skauvold, and Ragnar Hauge. Latent Diffusion Model for
Conditional Reservoir Facies Generation. _Computers & Geosciences_, 194:105750, January 2025. ISSN 00983004.
doi: 10.1016/j.cageo.2024.105750.

Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition_
_(CVPR)_, pages 10674–10685, New Orleans, LA, USA, June 2022. IEEE. doi: 10.1109/cvpr52688.2022.01042.


22


_Preprint submitted to Computers & Geosciences_


Oscar Ovanger, Daesoo Lee, Jo Eidsvik, Ragnar Hauge, Jacob Skauvold, and Erlend Aune. A Statistical Study of
Latent Diffusion Models for Geological Facies Modeling. _Mathematical Geosciences_, February 2025. ISSN 18748961, 1874-8953. doi: 10.1007/s11004-025-10178-5.


G. Matheron, H. Beucher, C. De Fouquet, A. Galli, D. Guerillot, and C. Ravenne. Conditional Simulation of the
Geometry of Fluvio-Deltaic Reservoirs. In _SPE Annual Technical Conference and Exhibition_, pages SPE–16753–
MS, Dallas, Texas, September 1987. SPE. doi: 10.2118/16753-MS.


Trond Mannseth. Relation Between Level Set and Truncated Pluri-Gaussian Methodologies for Facies Representation. _Mathematical Geosciences_, 46(6):711–731, August 2014. ISSN 1874-8961, 1874-8953. doi:
10.1007/s11004-013-9507-9.


Guido Di Federico and Louis J. Durlofsky. Latent diffusion models for parameterization of facies-based geomodels
and their use in data assimilation. _Computers & Geosciences_, 194:105755, January 2025. ISSN 00983004. doi:
10.1016/j.cageo.2024.105755.


Giannis Daras, Hyungjin Chung, Chieh-Hsin Lai, Yuki Mitsufuji, Jong Chul Ye, Peyman Milanfar, Alexandros G.
Dimakis, and Mauricio Delbracio. A Survey on Diffusion Models for Inverse Problems, September 2024.


Zheng Zhao, Ziwei Luo, Jens Sjölund, and Thomas Schön. Conditional sampling within generative diffusion models.
_Philosophical transactions of the Royal Society A_, 383(2299), June 2025. doi: 10.1098/rsta.2024.0329.


Jonathan Ho and Tim Salimans. Classifier-Free Diffusion Guidance. In _NeurIPS 2021 Workshop on Deep Generative_
_Models and Downstream Applications_, December 2021.


Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros G. Dimakis, and Peyman Milanfar.
Deblurring via Stochastic Refinement. In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition_
_(CVPR)_, pages 16272–16282, New Orleans, LA, USA, June 2022. IEEE. ISBN 978-1-6654-6946-3. doi: 10.1109/
CVPR52688.2022.01581.


Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A. Theodorou, Weili Nie, and Anima Anandkumar.
I$^2$SB: Image-to-Image Schrödinger Bridge. In _Proceedings of the 40th International Conference on Machine_
_Learning_, May 2023. doi: 10.48550/arXiv.2302.05872.


Morteza Mardani, Jiaming Song, Jan Kautz, and Arash Vahdat. A Variational Perspective on Solving Inverse Problems
with Diffusion Models. In _The Twelfth International Conference on Learning Representations_, October 2023.


Cagan Alkan, Julio Oscanoa, Daniel Abraham, Mengze Gao, Aizada Nurdinova, Kawin Setsompop, John M. Pauly,
Morteza Mardani, and Shreyas Vasanawala. Variational Diffusion Models for Blind MRI Inverse Problems. In
_NeurIPS 2023 Workshop on Deep Learning and Inverse Problems_, November 2023.


Berthy Feng and Katherine Bouman. Variational Bayesian Imaging with an Efficient Surrogate Score-based Prior. In
_Transactions on Machine Learning Research_, March 2024.


Brian L. Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay, and Tommi S. Jaakkola.
Diffusion Probabilistic Modeling of Protein Backbones in 3D for the motif-scaffolding problem. In _The Eleventh_
_International Conference on Learning Representations_, September 2022.


Gabriel Cardoso, Yazid Janati el Idrissi, Sylvain Le Corff, and Eric Moulines. Monte Carlo guided Denoising Diffusion
models for Bayesian linear inverse problems. In _The Twelfth International Conference on Learning Representations_,
October 2023.


Luhuan Wu, Brian L. Trippe, Christian A. Naesseth, David M. Blei, and John P. Cunningham. Practical and Asymptotically Exact Conditional Sampling in Diffusion Models. In _Neural Information Processing Systems_, 2024.


Zehao Dou and Yang Song. Diffusion posterior sampling for linear inverse problem solving — a filtering perspective.
In _The Twelfth International Conference on Learning Representations_, 2024.


Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion
posterior sampling for general noisy inverse problems. In _The 11th International Conference on Learning Repre-_
_sentations_, 2023.


Shayan Mohajer Hamidi and En-Hui Yang. Enhancing Diffusion Models for Inverse Problems with Covariance-Aware
Posterior Sampling. In _The 13th International Conference on Learning Representations_, 2025.


Benjamin Boys, Mark Girolami, Jakiw Pidstrigach, Sebastian Reich, Alan Mosca, and O. Deniz Akyildiz. Tweedie
Moment Projected Diffusions For Inverse Problems. _Transactions on Machine Learning Research_, September 2024.


Nebiyou Yismaw, Ulugbek S. Kamilov, and M. Salman Asif. Gaussian is All You Need: A Unified Framework
for Solving Inverse Problems via Diffusion Posterior Sampling. In _The Thirteenth International Conference on_
_Learning Representations_, 2025.


23


_Preprint submitted to Computers & Geosciences_


Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse
problems. In _International Conference on Learning Representations_, 2023.

Lea Friedli and Niklas Linde. Solving Geophysical Inversion Problems with Intractable Likelihoods: Linearized
Gaussian Approximations Versus the Correlated Pseudo-marginal Method. _Mathematical Geosciences_, 56(1):55–
75, January 2024. ISSN 1874-8961, 1874-8953. doi: 10.1007/s11004-023-10064-y.

S. L. Cotter, G. O. Roberts, A. M. Stuart, and D. White. MCMC Methods for Functions: Modifying Old Algorithms
to Make Them Faster. _Statistical Science_, 28(3), August 2013. ISSN 0883-4237. doi: 10.1214/13-STS421.

Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information_
_Processing Systems_, volume 32. Curran Associates, Inc., 2019.

Brian D.O. Anderson. Reverse-time diffusion equation models. _Stochastic Processes and their Applications_, 12(3):
313–326, May 1982. ISSN 03044149. doi: 10.1016/0304-4149(82)90051-5.

Herbert Robbins. An empirical Bayes approach to statistics. In _Berkeley Symposium on Mathematical Statistics and_
_Probability_, pages 157–163. University of California Press, 1956.

Bradley Efron. Tweedie’s Formula and Selection Bias. _Journal of the American Statistical Association_, 106(496):
1602–1614, December 2011. ISSN 0162-1459, 1537-274X. doi: 10.1198/jasa.2011.tm11181.

Tongda Xu, Xiyan Cai, Xinjie Zhang, Xingtong Ge, Dailan He, Ming Sun, Jingjing Liu, Ya-Qin Zhang, Jian Li, and
Yan Wang. Rethinking Diffusion Posterior Sampling: From Conditional Score Estimator to Maximizing a Posterior.
In _The Thirteenth International Conference on Learning Representations_, 2025.

Xinyu Peng, Ziyang Zheng, Wenrui Dai, Nuoqian Xiao, Chenglin Li, Junni Zou, and Hongkai Xiong. Improving
Diffusion Models for Inverse Problems Using Optimal Posterior Covariance. In _The 12th International Conference_
_on Learning Representations_, June 2024.

Uri M. Ascher and Linda R. Petzold. _Computer Methods for Ordinary Differential Equations and Differential-_
_Algebraic Equations_ . Society for Industrial and Applied Mathematics, Philadelphia, PA, January 1998. ISBN
978-0-89871-412-8 978-1-61197-139-2. doi: 10.1137/1.9781611971392.

Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomedical Image Segmentation, May 2015.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is All you Need. _31st Conference on Neural Information Processing Systems (NIPS 2017)_,
2017.

S. Kullback and R. A. Leibler. On Information and Sufficiency. _The Annals of Mathematical Statistics_, 22(1):79–86,
March 1951. ISSN 0003-4851. doi: 10.1214/aoms/1177729694.

Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: From error visibility to structural similarity. _IEEE Transactions on Image Processing_, 13(4):600–612, April 2004. ISSN 1057-7149, 1941-0042.
doi: 10.1109/TIP.2003.819861.

I. J. Good. Rational Decisions. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 14(1):
107–114, January 1952. ISSN 1369-7412, 1467-9868. doi: 10.1111/j.2517-6161.1952.tb00104.x.

Andrew Gelman and Donald B. Rubin. Inference from Iterative Simulation Using Multiple Sequences. _Statistical_
_Science_, 7(4):457–472, 1992. ISSN 0883-4237.

Rúben Nunes, Amílcar Soares, Leonardo Azevedo, and Pedro Pereira. Geostatistical Seismic Inversion with Direct
Sequential Simulation and Co-simulation with Multi-local Distribution Functions. _Mathematical Geosciences_, 49
(5):583–601, July 2017. ISSN 1874-8961, 1874-8953. doi: 10.1007/s11004-016-9651-0.

Amilcar Soares. Direct Sequential Simulation and Cosimulation. _Mathematical Geology_, 33(8):911–926, November
2001. ISSN 0882-8121, 1573-8868. doi: 10.1023/a:1012246006212.

Teng Xu, Sebastian Reuschen, Wolfgang Nowak, and Harrie-Jan Hendricks Franssen. Preconditioned Crank-Nicolson
Markov Chain Monte Carlo Coupled With Parallel Tempering: An Efficient Method for Bayesian Inversion of
Multi-Gaussian Log-Hydraulic Conductivity Fields. _Water Resources Research_, 56(8), August 2020. ISSN 00431397, 1944-7973. doi: 10.1029/2020wr027110.

Clayton V Deutsch and Andre G Journel. Geostatistical software library and user’s guide. _New York_, 119(147):578,
1992.


24


_Preprint submitted to Computers & Geosciences_

## **Appendix**


**A** **Additional assessment of prior modeling performance**


**A.1** **Geostatistical metrics**


We further assessed the performance of unconditional modeling of the DM based on geological and geostatistical
criteria. To evaluate the ability of the generative method to reproduce the channels’ morphologies, Table 6 summarizes
the average and standard deviations of length, thickness and area of the sandy channels, as they are represented in
the prior TI and reproduced by the DM, GAN and VAE. Here, the DM shows superior modeling ability, especially
compared to the VAE where the unconditional modeling is expected to output unrealistic realizations [see, e.g., Levy
et al., 2023, Miele et al., 2024, Lopez-Alvis et al., 2021]. We also observed that the realizations generated by the DM
slightly overestimate the proportion of sands (0.29±0.04) compared to the targeted prior (0.27±0.05); VAE and GAN
perform better (0.28±0.03 and 0.28±0.04, respectively) (Fig. 9a).


Table 6: Measures for facies morphology computed from 500 prior/unconditional facies realizations.

|Col1|Length (m)|Thickness (m)|Area (m2)|
|---|---|---|---|
|Prior|17.5 ±4.3|5.9 ±1.3|113 ±33|
|DM|17.4 ±4.4|5.9 ±1.4|118 ±36|
|GAN|16.2 ±3.6|5.6 ±1.2|105 ±30|
|VAE|8.9 ±1.6|3.3 ±0.6|53 ±13|



Figure 9: Volume fraction of sands in the generated realizations, compared to TI (Prior), for the fully trained models
(a), and for early training stages at epoch 100 (b).


We further computed the spatial uncertainty patterns of the generated _I_ _P_ distributions as experimental variograms
along the horizontal and vertical directions, from 60 realizations. The _I_ _P_ distributions in the TI were generated
considering facies-dependent spatial patterns. For sands we used an exponential model, with maximum distance of
significant spatial correlation [ranges; e.g., Deutsch and Journel, 1992] of 50 m and 25 m along horizontal and
vertical directions, respectively. For shales, we use a spherical variogram model with ranges of 65 m (horizontal


25


_Preprint submitted to Computers & Geosciences_


direction) and 25 m (vertical direction). Due to the channels heterogeneity, the facies-dependent _I_ _P_ spatial patterns are
affected by truncations at large ranges and their corresponding experimental variograms show disagreements with the
theoretical models. However, we qualitatively estimate if spatial patterns are reproduced, by obtaining the experimental
variograms from 100 TI samples and _I_ _P_ realizations generated with DM, GAN and VAE. The results are shown in
Figure 10, where the experimental points are linearly interpolated for visual aid. Overall, we see a good match between
the DM and the prior, with noticeable improvements for the _I_ _P_ distribution in the shale.


Figure 10: Experimental variograms, per facies and direction, computed from 60 _I_ _P_ prior and models realizations.


**A.2** **Training efficiency**


Diffusion models generally have a larger computational costs than GANs and VAEs. This is reflected, for example in
the total training time of the proposed DM (4 days and 15h for 600 epochs) compared to the other generative models
(respectively, 21h for the GAN and 12h for the VAE for 2000 epochs each). While we adopted a long training time
for the three model to maximize their accuracy (Fig. 11), we observe that the training of the DM is significantly more
efficient. Already at epoch 100 (i.e., after 19h of training), the DM approximates the prior first- and second- order
statistics with sufficient accuracy (Table 7, Fig. 12 and Fig. 9b) and it is generally more accurate than the fully trained
GAN which required a similar training time.


Table 7: Metrics for unconditional prior modeling using the trained DM and benchmarking GAN and VAE at epoch
100. _p_ (sand) and Avg. _I_ _P_ refer to the average models obtained from the realizations ensembles. _KL_ : Kull
|ce; Avg.:|: average; St.|dev.: standard deviatio|on.|
|---|---|---|---|
|||_p_(sand) —Avg._ IP_|_IP_ marginal distributions|
||Lithology|Relative error|Avg.±St. dev.<br>m/s g/cm3<br>_KLIP_|
|Prior|_Sand_<br>_Shale_|-|6660 ±730<br>8540 ±660<br>-|
|DM|_Sand_<br>_Shale_|15.0%<br>0.82%|6650 ±730<br>8530 ±670<br>0.8e-3<br>0.9e-3|
|GAN|_Sand_<br>_Shale_|58.0%<br>3.8%|6620 ±650<br>8530 ±630<br>2.3e-3<br>1.4e-3|
|VAE|_Sand_<br>_Shale_|12.3%<br>1.2%|6860 ±700<br>8620 ±680<br>1.4e-3<br>1.8e-3|



26


_Preprint submitted to Computers & Geosciences_


Figure 11: Loss values for the three trained networks. DM and VAE have comparable loss metrics (data mean squared
error), GAN has an adversarial loss.


27


_Preprint submitted to Computers & Geosciences_


Figure 12: Summary of unconditional modeling performances of DM (a, b), GAN (c, d), and VAE (e, f) at training
epoch 100. Subplots (a), (c), (e): realizations and average distribution of facies and _I_ _P_ for DM (a), GAN (c) and VAE
(e); subplots (b), (d), (f): joint and _I_ _P_ distributions for DM (b), GAN (d) and VAE (f).


28


_Preprint submitted to Computers & Geosciences_


Table 8: Metrics of the posterior distributions sampled by CDPS, measured for data, facies and _I_ _P_ (subscripts **d**, F,
and _I_ _P_, respectively), for the three studied cases assuming _Data noise 2_ .

|Col1|Case 1|Case 2 Case 3|
|---|---|---|
|WRMSE**d**|0.204±0.005|0.199±0.006<br>0.201±0.06|
|RMSEF<br>SSIMF|0.19<br>0.73±0.05|0.20<br>0.21<br>0.72±0.02<br>0.69±0.02|
|RMSE_IP_<br>LogS_IP_<br>_KLIP_ (10~~_−_4~~)|450<br>7.3<br>6.3±1.5|358<br>324<br>9.1<br>7.0<br>3.5±0.5<br>3.8±0.8|



**B** **Conditioning on multiple data types**


We applied the CDPS algorithm on three different test cases ( _Target_ in Fig. 13a, b, c) using both direct observations
(well logs) and seismic reflection data; the location of the well is shown in Fig. 13 as red dashed line. The conditioning
data has Gaussian uncorrelated noise with amplitude corresponding to the _Data noise 2_ (Table 2 in Section 3.2).
Conditioning on two data types required computing the corresponding likelihood scores, individually, and summing
them to the prior score for the denoising at each time-step. For these applications we considered 300 steps. The target
used as _Case 1_ (Fig. 13a) represents a scenario not included in our TI, having only a single channel body in the facies
distribution (2% in sand volume fraction). This can be seen as an example of inversion when the prior knowledge
is biased; _Case 2_ and _Case 3_ (Fig. 13b and c, respectively) are test example extracted from the TI (not used during
training).


Figures 13 and 14 summarize the solutions obtained, showing the results from 50 generated samples; the accuracy
of the inversion solutions is described in Table 8 using the same metrics shown in Section 3.2.2. In all the cases, we
observe a good convergence of the solutions, with all the samples having WRMSE (when compared to the noiseless
data) _<_ 1 (WRMSE **d** in Table 8). The solutions retrieved honor the conditioning data within the uncertainty ranges,
in both well data and seismic data domains (Fig. 14): at the well location (x = 50 m in Fig. 14), the _I_ _P_ values fit
the uncertainty range, while the corresponding seismic data shows lower variability than the considered error standard
deviation, especially at larger seismic amplitudes. A similar pattern is observed at locations where only seismic data
conditioning is used (e.g., x = 25 m in Fig. 14). Given the close match between the predicted and noiseless seismic
data, this behavior suggests that the relative component of the seismic uncertainty does not significantly impact the
inversion results.


Both facies and _I_ _P_ distributions match the target with good accuracy in the studied area (Fig. 14). Overall, the facies
realizations generated match the target with an accuracy of 94% for _Case 1_ and 96% for _Case 2_ and _Case 3_, with a
large structural similarity to the target (SSIM F in Table 8). The p(sand) and average _I_ _P_ computed from the realizations
ensembles (Fig. 13) match the targets, and their corresponding standard deviation shows larger variability mostly at
facies interfaces. The standard deviation of _I_ _P_ realizations ( _Std. dev._ in Fig. 13) also highlights the lateral conditioning
of the well log data. For _Case 1_, few solutions predict the presence of sands in the upper left area (an area of lower _I_ _P_
values), significantly mismatching the target; here, we also see larger variability of the solutions (Fig. 13 and, e.g., _I_ _P_
trace at x = 25 m in Fig. 14a). This can be interpreted as an effect of the bias of the trained DM compared to this case
study.


Overall, the predictive power of the CDPS is large, with good results for the biased _Case 1_, as shown by the comparably
close values of all the metrics in Table 8. The _Case 2_ shows relatively poorer performances of the CDPS in _I_ _P_
distribution prediction. The realizations present lower variability (Figures 13b and 14b), and their ensemble not always
match well to the local true _I_ _P_, hence the higher logS measured values (Table 8).


29


_Preprint submitted to Computers & Geosciences_


Figure 13: Target facies and _I_ _P_ for three case studies (a, b, c), sampled posterior distribution shown as mean and
standard deviation ( _Std. dev._ ) of realizations’ ensembles, and corresponding average seismic data misfit.


30


_Preprint submitted to Computers & Geosciences_


Figure 14: Predicted _I_ _P_ and corresponding seismic data vertical traces at well location (x = 50 m) and at x = 25 m,
compared with conditioning data **d** **obs** and unknown true _I_ _P_ ( _Target_ ).


31


_Preprint submitted to Computers & Geosciences_


Table 9: Metrics of the posterior distributions sampled by CDPS and DPS, measured for data, facies and _I_ _P_ (subscripts
**d**, F, and _I_ _P_, respectively).

|Col1|CDPS|DPS|
|---|---|---|
|WRMSE**d**|0.70±0.01|7.1±0.6|
|RMSEF<br>SSIMF|0.19<br>0.60±0.04|0.24<br>0.51±0.15|
|RMSE_IP_<br>LogS_IP_<br>_KLIP_ (10~~_−_3~~)|447<br>7.1<br>0.4±0.1|419<br>9.1<br>3.1±1.1|



**C** **CDPS implementation with DDPM and DDIM**


We implement the CDPS in the DDPM and DDIM frameworks, shown in Algorithm 2. Differently from score-based
diffusion, the trained network predicts the noise added to a sample, denoted as _ϵ_ _θ_ . This can be related to the score
function using the following correlation [Dhariwal and Nichol, 2021]


1
_∇_ _x_ _t_ log _p_ _t_ ( **x** _t_ ) = _−_ ~~_√_~~ 1 _−_ _α_ ¯ _t_ _ϵ_ [(] _θ_ _[t]_ [)] _[,]_ (25)


where ¯ _α_ _t_ is inversely correlated to the diffusion noise (See Section 2.1 and Ho et al. [2020]). Therefore, to integrate


¯

the likelihood score within the DDPM and DDIM frameworks, we rescale it to �(1 _−_ _α_ _t_ ) _∇_ _x_ _t_ log _p_ _t_ ( **d** _|_ **x** _t_ ) (line 19 of

Algorithm 2). We then sum the two components as


ˆ ¯
_ϵ_ [(] _[t]_ [)] = _ϵ_ [(] _θ_ _[t]_ [)] _−_ ~~�~~ (1 _−_ _α_ _t_ ) _∇_ _x_ _t_ log _p_ _t_ ( **d** _|_ **x** _t_ ) _,_ (26)


and use this "posterior-related" drift to obtain the denoised **x** _t−_ 1 . This approach was first proposed by Dhariwal and
Nichol [2021] in their guided diffusion modeling. For the application of the CDPS with DDIM, we use the same
diffusion UNet described in Section 2.3 and retrain it using the same TI for noise prediction; the training parameters
and process are the same described in Dhariwal and Nichol [2021].


Using one well as conditioning data, contaminated with Gaussian noise of magnitude equal to the case _Data noise_
_2_ (Table 2 in Section 3.2), we sampled 100 samples using CDPS (Algorithm 2) and the DPS method in its original
implementation, with weight _ρ_ = 1 as defined for the inpainting tasks. Using CDPS with either DDIM (deterministic)
or DDPM (stochastic) sampling did not change the results; we used the DPS in its original implementation with
DDPM. For both CDPS and DPS, we sampled with 1000 denoising steps. The results are summarized in Fig. 15. All
the realizations using CDPS match the conditioning facies with 100% of accuracy and have WRMSE = 0.7 ±0.2; in
the unconditioned area between x = 0 m and x = 20 m, the p(sand) = 0.28. These values are closer to the prior than our
linear conditioning case with score-based diffusion (Section 3.2.1); we attribute this difference to the absence of the
second conditioning well. For DPS, we observe that the data is perfectly matching the noisy data (overfitting), with
standard deviation approaching 0. This does not occur in our implementation of the DPS with score-based diffusion, as
the likelihood score magnitude is rescaled proportionally to the diffusion noise content, hence is less _influential_ on the
conditioning diffusion. The lack of such rescaling is, in fact, the inconsistency between conditional diffusion theory
and the original DPS implementation on DDPM we highlight in Section 2.2.1. Our CDPS (Algorithm 2) integrates
this correction.


We further applied the methods for fullstack seismic data inverse modeling. We show here the results for a nonlinear
inverse problem equal to that use in Section 3.2.2, considering the _data noise 2_ . For DPS, we show the results
considering a likelihood score weight _ρ_ = 0 _._ 4, corresponding to that suggested by Chung et al. [2023] for nonlinear
phase retrieval. For both CDPS and DPS, we sampled with 1000 denoising steps. The results are summarized in Fig.
16 and integrated with the metrics of Table 9. The DPS showed high instability of the denoising process; the CDPS
underperformed compared to the equivalent score-based inverse problem. Although we did not investigate the possible
causes for such difference, we believe that assumptions of an isotropic uncertainty for the denoiser may play a role in
the inversion accuracy for the nonlinear case.


32


_Preprint submitted to Computers & Geosciences_


Figure 15: Conditioned realizations sampled using (a) CDPS and (b) DPS, represented as mean and standard deviation
( _Std. dev._ ) of facies and _I_ _P_ distributions, and sampled _I_ _P_ at the conditioning well location (x=50m).


33


_Preprint submitted to Computers & Geosciences_


**Algorithm 2** Corrected DPS algorithm for DDPM (Gaussian error)

1: **Require:** trained network _S_ _θ_ ( **x** _t_ _, t_ ), conditioning data **d**, forward operator _F_, noise schedule _{σ}_ _[N]_ _t_ =1 [and] _[ {][β][}]_ _[N]_ _t_ =1
(with _α_ = 1 _−_ _β_ )


2: **for** _t = T,..., 1_ **do**
3: **sample x** 0 _∼_ _q_ _data_
4: **x** _t_ _←_ _α_ _t_ **x** 0 + (1 _−_ _α_ _t_ ) **z** _,_ **z** _∼N_ (0 _, I_ )

5: _ϵ_ [(] _θ_ _[t]_ [)] _←_ _S_ _θ_ ( **x** _t_ _, t_ )

6: ˆ **x** [(] 0 _[t]_ [)] _←_ **x** _t_ _−_ _[√]_ 1 _−_ _α_ ¯ _t_ _ϵ_ [(] _θ_ _[t]_ [)] _/_ _[√]_ ~~_α_~~ ~~¯~~ _t_
� �


_N_
7: _σ_ _x_ [(] ˆ _[t]_ 0 [)] _[←]_ [E] � ~~��~~ _i_ =1 [(ˆ] _[x]_ 0 [(] _[t]_ _,i_ [)] _[−]_ _[x]_ [0] _[,i]_ [)] [2] _[/N]_ �

8: **end for**


9: **sample x** _T_ _∼N_ (0 _, I_ )
10: **for** _t = T-1,..., 0_ **do**
11: _ϵ_ [(] _θ_ _[t]_ [)] _←_ _S_ _θ_ ( **x** _t_ _, t_ )

12: ˆ **x** [(] 0 _[t]_ [)] _←_ � **x** _t_ _−_ _[√]_ 1 _−_ _α_ ¯ _t_ _ϵ_ _θ_ � _/_ _[√]_ ~~_α_~~ ~~¯~~ _t_
13: **if** _F_ is nonlinear **then**
14: _J_ ˆ **x** 0 _←J_ _F_ (ˆ **x** [(] 0 _[t]_ [)] [)]

15: ˜Σ **d** _←J_ ˆ **x** _[T]_ [(] 0 _[t]_ [)] [Σ] ˆ **x** [(] _[t]_ 0 [)] _[J]_ ˆ **x** [(] 0 _[t]_ [)] + Σ **d**

16: **else**
17: ˜Σ **d** _←F_ _[T]_ Σ ˆ [(] **x** _[t]_ 0 [)] _[F]_ [ + Σ] **[d]**
18: **end if**
19: _∇_ _x_ _t_ log _p_ _t_ ( **d** _|_ ˆ **x** [(] 0 _[t]_ [)] [)] _[ ←−∇]_ **[x]** _t_ ( _F_ (ˆ **x** [(] 0 _[t]_ [)] [)] _[ −]_ **[d]** [)] _[T]_ [ ˜Σ] _[−]_ **d** [1] [(] _[F]_ [(ˆ] **[x]** [(] 0 _[t]_ [)] [)] _[ −]_ **[d]** [)]
� �


ˆ ¯
20: _ϵ_ [(] _[t]_ [)] _←_ _ϵ_ [(] _θ_ _[t]_ [)] + ~~�~~ (1 _−_ _α_ _t_ ) _∇_ _x_ _t_ log _p_ _t_ ( **x** _t_ )

21: **z** _∼N_ (0 _, I_ )



1 _t_ _−_ ~~(~~ 1 _α_ ¯ _−_ _t_ _α_ _−_ ¯ _t_ 1 _−_ 1 ) **x** _t_ + ~~_√_~~ 1 _−_ ~~_α_~~ _t_ _α_ ¯ _−t_ 1 _−_ ~~_β_~~ 1 _t_



22: _x_ _t−_ 1 _←_ ~~_√α_~~ _t_ ~~(~~ 1¯ _−α_ ¯ _t−_ 1 )



1 _−α_ ¯ _t−_ 1



**x** _t_ _−_ _[√]_ 1 _−_ _α_ ¯ _t_ _∇_ _x_ _t_ log _p_ _t_ (ˆ **x** [(] 0 _[t]_ [)] _[|]_ **[d]** [)] _/_ _[√]_ ~~_α_~~ ~~¯~~ _t_ + _σ_ _t_ **z**
� �


34



23: **end for**
24: **Output:** ˜ **x** 0 _∼_ _p_ ( **x** 0 _|_ **d** )


_Preprint submitted to Computers & Geosciences_


Figure 16: Sampled posterior distribution represented as (a) average facies; (b) facies standard deviation; (c) average
_I_ _P_ ; (d) _I_ _P_ standard deviation; (e) average data error; (f) predicted data versus conditional data distribution ( **d** obs _±_ _σ_ **d** )
for a vertical trace at x = 50m; (g) _I_ _P_ realizations; (h) _I_ _P_ samples for a vertical trace at x = 25m compared to the
unknown target distribution.


35


